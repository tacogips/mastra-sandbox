{
    "id": "https://mastra.ai/docs/voice/overview",
    "title": "Voice in Mastra | Mastra Docs",
    "url": "https://mastra.ai/docs/voice/overview",
    "author": "",
    "text": "\n Mastra’s Voice system provides a unified interface for voice interactions, enabling text-to-speech (TTS), speech-to-text (STT), and real-time speech-to-speech (STS) capabilities in your applications. \n Adding Voice to Agents \n To learn how to integrate voice capabilities into your agents, check out the Adding Voice to Agents documentation. This section covers how to use both single and multiple voice providers, as well as real-time interactions. \n import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n \n // Initialize OpenAI voice for TTS \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n You can then use the following voice capabilities: \n Text to Speech (TTS) \n Turn your agent’s responses into natural-sounding speech using Mastra’s TTS capabilities.\nChoose from multiple providers like OpenAI, ElevenLabs, and more. \n For detailed configuration options and advanced features, check out our Text-to-Speech guide. \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { playAudio } from \" @mastra/node-audio \"; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n \n const { text } = await voiceAgent. generate ( ' What color is the sky? '); \n \n // Convert text to speech to an Audio Stream \n const audioStream = await voiceAgent.voice. speak (text, { \n speaker: \" default \", // Optional: specify a speaker \n responseFormat: \" wav \", // Optional: specify a response format \n}); \n \n playAudio (audioStream); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Speech to Text (STT) \n Transcribe spoken content using various providers like OpenAI, ElevenLabs, and more. For detailed configuration options and more, check out Speech to Text. \n You can download a sample audio file from here . \n \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { createReadStream } from ' fs '; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n \n // Use an audio file from a URL \n const audioStream = await createReadStream ( \"./how_can_i_help_you.mp3 \"); \n \n // Convert audio to text \n const transcript = await voiceAgent.voice. listen (audioStream); \n console. log ( ` User said: ${ transcript}`); \n \n // Generate a response based on the transcript \n const { text } = await voiceAgent. generate (transcript); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Speech to Speech (STS) \n Create conversational experiences with speech-to-speech capabilities. The unified API enables real-time voice interactions between users and AI agents.\nFor detailed configuration options and advanced features, check out Speech to Speech. \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { playAudio, getMicrophoneStream } from ' @mastra/node-audio '; \n import { OpenAIRealtimeVoice } from \" @mastra/voice-openai-realtime \"; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIRealtimeVoice (), \n}); \n \n // Listen for agent audio responses \n voiceAgent.voice. on ( ' speaker ', ({ audio }) =&gt; { \n playAudio (audio); \n}); \n \n // Initiate the conversation \n await voiceAgent.voice. speak ( ' How can I help you today? '); \n \n // Send continuous audio from the microphone \n const micStream = getMicrophoneStream (); \n await voiceAgent.voice. send (micStream); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Voice Configuration \n Each voice provider can be configured with different models and options. Below are the detailed configuration options for all supported providers: \n OpenAI // OpenAI Voice Configuration \n const voice = new OpenAIVoice ({ \n speechModel: { \n name: \" gpt-3.5-turbo \", // Example model name \n apiKey: process.env.OPENAI_API_KEY, \n language: \" en-US \", // Language code \n voiceType: \" neural \", // Type of voice model \n }, \n listeningModel: { \n name: \" whisper-1 \", // Example model name \n apiKey: process.env.OPENAI_API_KEY, \n language: \" en-US \", // Language code \n format: \" wav \", // Audio format \n }, \n speaker: \" alloy \", // Example speaker name \n}); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Using Multiple Voice Providers \n This example demonstrates how to create and use two different voice providers in Mastra: OpenAI for speech-to-text (STT) and PlayAI for text-to-speech (TTS). \n Start by creating instances of the voice providers with any necessary configuration. \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { PlayAIVoice } from \" @mastra/voice-playai \"; \n import { CompositeVoice } from \" @mastra/core/voice \"; \n import { playAudio, getMicrophoneStream } from \" @mastra/node-audio \"; \n \n // Initialize OpenAI voice for STT \n const input = new OpenAIVoice ({ \n listeningModel: { \n name: \" whisper-1 \", \n apiKey: process.env.OPENAI_API_KEY, \n }, \n}); \n \n // Initialize PlayAI voice for TTS \n const output = new PlayAIVoice ({ \n speechModel: { \n name: \" playai-voice \", \n apiKey: process.env.PLAYAI_API_KEY, \n }, \n}); \n \n // Combine the providers using CompositeVoice \n const voice = new CompositeVoice ({ \n input, \n output, \n}); \n \n // Implement voice interactions using the combined voice provider \n const audioStream = getMicrophoneStream (); // Assume this function gets audio input \n const transcript = await voice. listen (audioStream); \n \n // Log the transcribed text \n console. log ( \" Transcribed text: \", transcript); \n \n // Convert text to speech \n const responseAudio = await voice. speak ( ` You said: ${ transcript}`, { \n speaker: \" default \", // Optional: specify a speaker, \n responseFormat: \" wav \", // Optional: specify a response format \n}); \n \n // Play the audio response \n playAudio (responseAudio); \n For more information on the CompositeVoice, refer to the CompositeVoice Reference. \n More Resources \n \n CompositeVoice \n MastraVoice \n OpenAI Voice \n Azure Voice \n Google Voice \n Deepgram Voice \n PlayAI Voice \n Voice Examples \n",
    "image": "https://mastra.ai/api/og/docs?title=Voice%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20voice%20capabilities%20in%20Mastra,%20including%20text-to-speech,%20speech-to-text,%20and%20real-time%20speech-to-speech%20interactions.",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/voice/overview",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}