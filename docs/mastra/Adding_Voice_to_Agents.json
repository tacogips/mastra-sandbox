{
    "id": "https://mastra.ai/docs/agents/adding-voice",
    "title": "Adding Voice to Agents",
    "url": "https://mastra.ai/docs/agents/adding-voice",
    "author": "",
    "text": "\n Mastra agents can be enhanced with voice capabilities, allowing them to speak responses and listen to user input. You can configure an agent to use either a single voice provider or combine multiple providers for different operations. \n Using a Single Provider \n The simplest way to add voice to an agent is to use a single provider for both speaking and listening: \n import { createReadStream } from \" fs \"; \n import path from \" path \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n // Initialize the voice provider with default settings \n const voice = new OpenAIVoice (); \n \n // Create an agent with voice capabilities \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with both STT and TTS capabilities. `, \n model: openai ( \" gpt-4o \"), \n voice, \n}); \n \n // The agent can now use voice for interaction \n const audioStream = await agent.voice. speak ( \" Hello, I'm your AI assistant! \", { \n filetype: \" m4a \", \n}); \n \n playAudio (audioStream!); \n \n try { \n const transcription = await agent.voice. listen (audioStream); \n console. log (transcription) \n} catch (error) { \n console. error ( \" Error transcribing audio: \", error); \n} \n Using Multiple Providers \n For more flexibility, you can use different providers for speaking and listening using the CompositeVoice class: \n import { Agent } from \" @mastra/core/agent \"; \n import { CompositeVoice } from \" @mastra/core/voice \"; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { PlayAIVoice } from \" @mastra/voice-playai \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with both STT and TTS capabilities. `, \n model: openai ( \" gpt-4o \"), \n \n // Create a composite voice using OpenAI for listening and PlayAI for speaking \n voice: new CompositeVoice ({ \n input: new OpenAIVoice (), \n output: new PlayAIVoice (), \n }), \n}); \n Working with Audio Streams \n The speak() and listen() methods work with Node.js streams. Hereâ€™s how to save and load audio files: \n Saving Speech Output \n The speak method returns a stream that you can pipe to a file or speaker. \n import { createWriteStream } from \" fs \"; \n import path from \" path \"; \n \n // Generate speech and save to file \n const audio = await agent.voice. speak ( \" Hello, World! \"); \n const filePath = path. join (process. cwd (), \" agent.mp3 \"); \n const writer = createWriteStream (filePath); \n \n audio. pipe (writer); \n \n await new Promise &lt; void &gt;((resolve, reject) =&gt; { \n writer. on ( \" finish \", () =&gt; resolve ()); \n writer. on ( \" error \", reject); \n}); \n Transcribing Audio Input \n The listen method expects a stream of audio data from a microphone or file. \n import { createReadStream } from \" fs \"; \n import path from \" path \"; \n \n // Read audio file and transcribe \n const audioFilePath = path. join (process. cwd (), \" /agent.m4a \"); \n const audioStream = createReadStream (audioFilePath); \n \n try { \n console. log ( \" Transcribing audio file... \"); \n const transcription = await agent.voice. listen (audioStream, { \n filetype: \" m4a \", \n }); \n console. log ( \" Transcription: \", transcription); \n} catch (error) { \n console. error ( \" Error transcribing audio: \", error); \n} \n Speech-to-Speech Voice Interactions \n For more dynamic and interactive voice experiences, you can use real-time voice providers that support speech-to-speech capabilities: \n import { Agent } from \" @mastra/core/agent \"; \n import { getMicrophoneStream } from \" @mastra/node-audio \"; \n import { OpenAIRealtimeVoice } from \" @mastra/voice-openai-realtime \"; \n import { search, calculate } from \"../tools \"; \n \n // Initialize the realtime voice provider \n const voice = new OpenAIRealtimeVoice ({ \n chatModel: { \n apiKey: process.env.OPENAI_API_KEY, \n model: \" gpt-4o-mini-realtime \", \n }, \n speaker: \" alloy \", \n}); \n \n // Create an agent with speech-to-speech voice capabilities \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with speech-to-speech capabilities. `, \n model: openai ( \" gpt-4o \"), \n tools: { \n // Tools configured on Agent are passed to voice provider \n search, \n calculate, \n }, \n voice, \n}); \n \n // Establish a WebSocket connection \n await agent.voice. connect (); \n \n // Start a conversation \n agent.voice. speak ( \" Hello, I'm your AI assistant! \"); \n \n // Stream audio from a microphone \n const microphoneStream = getMicrophoneStream (); \n agent.voice. send (microphoneStream); \n \n // When done with the conversation \n agent.voice. close (); \n Event System \n The realtime voice provider emits several events you can listen for: \n // Listen for speech audio data sent from voice provider \n agent.voice. on ( \" speaking \", ({ audio }) =&gt; { \n // audio contains ReadableStream or Int16Array audio data \n}); \n \n // Listen for transcribed text sent from both voice provider and user \n agent.voice. on ( \" writing \", ({ text, role }) =&gt; { \n console. log ( `${ role} said: ${ text}`); \n}); \n \n // Listen for errors \n agent.voice. on ( \" error \", (error) =&gt; { \n console. error ( \" Voice error: \", error); \n}); \n Supported Voice Providers \n Mastra supports multiple voice providers for text-to-speech (TTS) and speech-to-text (STT) capabilities: \n Provider Package Features Reference OpenAI @mastra/voice-openai TTS, STT Documentation OpenAI Realtime @mastra/voice-openai-realtime Realtime speech-to-speech Documentation ElevenLabs @mastra/voice-elevenlabs High-quality TTS Documentation PlayAI @mastra/voice-playai TTS Documentation Google @mastra/voice-google TTS, STT Documentation Deepgram @mastra/voice-deepgram STT Documentation Murf @mastra/voice-murf TTS Documentation Speechify @mastra/voice-speechify TTS Documentation Sarvam @mastra/voice-sarvam TTS, STT Documentation Azure @mastra/voice-azure TTS, STT Documentation Cloudflare @mastra/voice-cloudflare TTS Documentation \n For more details on voice capabilities, see the Voice API Reference.",
    "image": "https://mastra.ai/api/og/docs?title=Adding%20Voice%20to%20Agents&description=undefined",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/agents/adding-voice",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}