{
  "data": {
    "results": [
      {
        "id": "https://mastra.ai/en/docs",
        "title": "Introduction | Mastra Docs",
        "url": "https://mastra.ai/en/docs",
        "author": "",
        "text": "About Mastra \n Mastra is an open-source TypeScript agent framework. \n It’s designed to give you the primitives you need to build AI applications and features. \n You can use Mastra to build AI agents that have memory and can execute functions, or chain LLM calls in deterministic workflows. You can chat with your agents in Mastra’s local dev environment, feed them application-specific knowledge with RAG, and score their outputs with Mastra’s evals. \n The main features include: \n \n Model routing : Mastra uses the Vercel AI SDK  for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini. \n Agent memory and tool calling: With Mastra, you can give your agent tools (functions) that it can call. You can persist agent memory and retrieve it based on recency, semantic similarity, or conversation thread. \n Workflow graphs: When you want to execute LLM calls in a deterministic way, Mastra gives you a graph-based workflow engine. You can define discrete steps, log inputs and outputs at each step of each run, and pipe them into an observability tool. Mastra workflows have a simple syntax for control flow ( step(),.then(),.after()) that allows branching and chaining. \n Agent development environment: When you’re developing an agent locally, you can chat with it and see its state and memory in Mastra’s agent development environment. \n Retrieval-augmented generation (RAG): Mastra gives you APIs to process documents (text, HTML, Markdown, JSON) into chunks, create embeddings, and store them in a vector database. At query time, it retrieves relevant chunks to ground LLM responses in your data, with a unified API on top of multiple vector stores (Pinecone, pgvector, etc) and embedding providers (OpenAI, Cohere, etc). \n Deployment: Mastra supports bundling your agents and workflows within an existing React, Next.js, or Node.js application, or into standalone endpoints. The Mastra deploy helper lets you easily bundle agents and workflows into a Node.js server using Hono, or deploy it onto a serverless platform like Vercel, Cloudflare Workers, or Netlify. \n Evals: Mastra provides automated evaluation metrics that use model-graded, rule-based, and statistical methods to assess LLM outputs, with built-in metrics for toxicity, bias, relevance, and factual accuracy. You can also define your own evals. \n Installation",
        "image": "https://mastra.ai/api/og/docs?title=Introduction%20|%20Mastra%20Docs&description=Mastra%20is%20a%20TypeScript%20agent%20framework.%20It%20helps%20you%20build%20AI%20applications%20and%20features%20quickly.%20It%20gives%20you%20the%20set%20of%20primitives%20you%20need:%20workflows,%20agents,%20RAG,%20integrations,%20syncs%20and%20evals.",
        "favicon": "https://mastra.ai/favicon.ico",
        "extras": {
          "links": [
            "https://mastra.ai/en/docs",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
          ]
        },
        "subpages": [
          {
            "id": "https://mastra.ai/docs",
            "title": "Introduction | Mastra Docs",
            "url": "https://mastra.ai/docs",
            "author": "",
            "text": "About Mastra \n Mastra is an open-source TypeScript agent framework. \n It’s designed to give you the primitives you need to build AI applications and features. \n You can use Mastra to build AI agents that have memory and can execute functions, or chain LLM calls in deterministic workflows. You can chat with your agents in Mastra’s local dev environment, feed them application-specific knowledge with RAG, and score their outputs with Mastra’s evals. \n The main features include: \n \n Model routing : Mastra uses the Vercel AI SDK  for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini. \n Agent memory and tool calling: With Mastra, you can give your agent tools (functions) that it can call. You can persist agent memory and retrieve it based on recency, semantic similarity, or conversation thread. \n Workflow graphs: When you want to execute LLM calls in a deterministic way, Mastra gives you a graph-based workflow engine. You can define discrete steps, log inputs and outputs at each step of each run, and pipe them into an observability tool. Mastra workflows have a simple syntax for control flow ( step(),.then(),.after()) that allows branching and chaining. \n Agent development environment: When you’re developing an agent locally, you can chat with it and see its state and memory in Mastra’s agent development environment. \n Retrieval-augmented generation (RAG): Mastra gives you APIs to process documents (text, HTML, Markdown, JSON) into chunks, create embeddings, and store them in a vector database. At query time, it retrieves relevant chunks to ground LLM responses in your data, with a unified API on top of multiple vector stores (Pinecone, pgvector, etc) and embedding providers (OpenAI, Cohere, etc). \n Deployment: Mastra supports bundling your agents and workflows within an existing React, Next.js, or Node.js application, or into standalone endpoints. The Mastra deploy helper lets you easily bundle agents and workflows into a Node.js server using Hono, or deploy it onto a serverless platform like Vercel, Cloudflare Workers, or Netlify. \n Evals: Mastra provides automated evaluation metrics that use model-graded, rule-based, and statistical methods to assess LLM outputs, with built-in metrics for toxicity, bias, relevance, and factual accuracy. You can also define your own evals. \n Installation",
            "image": "https://mastra.ai/api/og/docs?title=Introduction%20|%20Mastra%20Docs&description=Mastra%20is%20a%20TypeScript%20agent%20framework.%20It%20helps%20you%20build%20AI%20applications%20and%20features%20quickly.%20It%20gives%20you%20the%20set%20of%20primitives%20you%20need:%20workflows,%20agents,%20RAG,%20integrations,%20syncs%20and%20evals.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples",
            "title": "Examples List: Workflows, Agents, RAG | Mastra Docs",
            "url": "https://mastra.ai/examples",
            "author": "",
            "text": "Examples Overview \n The Examples section is a short list of example projects demonstrating basic AI engineering with Mastra, including text generation, structured output, streaming responses, and retrieval‐augmented generation (RAG). \n Agent with System Prompt Agentic Workflows Using a Tool Hierarchical Multi-Agent System Multi-Agent Workflow Bird Checker Giving a System Prompt",
            "image": "https://mastra.ai/api/og/docs?title=Examples%20List%3A%20Workflows%2C%20Agents%2C%20RAG%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/guides",
            "title": "Overview",
            "url": "https://mastra.ai/guides",
            "author": "",
            "text": "Guides \n While examples show quick implementations and docs explain specific features, these guides are a bit longer and designed to demonstrate core Mastra concepts: \n AI Recruiter \n Create a workflow that processes candidate resumes and conducts interviews, demonstrating branching logic and LLM integration in Mastra workflows. \n Chef Assistant \n Build an AI chef agent that helps users cook meals with available ingredients, showing how to create interactive agents with custom tools. \n Research Paper Assistant \n Develop an AI research assistant that analyzes academic papers using Retrieval Augmented Generation (RAG), demonstrating document processing and question answering. \n Stock Agent \n Implement a simple agent that fetches stock prices, illustrating the basics of creating tools and integrating them with Mastra agents. Agents: Chef Michel",
            "image": "https://mastra.ai/api/og/docs?title=Overview&description=Guides%20on%20building%20with%20Mastra",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/guides",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/reference",
            "title": "API Reference",
            "url": "https://mastra.ai/reference",
            "author": "",
            "text": "\n The Reference section provides documentation of Mastra’s API, including parameters, types and usage examples. \n Mastra Class",
            "image": "https://mastra.ai/api/og/docs?title=API%20Reference&description=Mastra%20API%20Reference",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/reference",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/showcase",
            "title": "Showcase",
            "url": "https://mastra.ai/showcase",
            "author": "",
            "text": "Check out these applications built with Mastra. Audiofeed Audiofeed repurposes your content into audio and video. Bird Checker Bird Checker is a bird identification app. OpenAPI Spec Writer Generate an open api spec from your documentation url. Crypto Chatbot You can ask about current crypto prices and trends in the cryptocurrency market.",
            "image": "https://mastra.ai/api/og/docs?title=Showcase",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://github.com/mastra-ai/mastra",
            "title": "GitHub - mastra-ai/mastra: The TypeScript AI agent framework. ⚡ Assistants, RAG, observability. Supports any LLM: GPT-4, Claude, Gemini, Llama.",
            "url": "https://github.com/mastra-ai/mastra",
            "publishedDate": "2024-08-06T20:44:31.000Z",
            "author": "mastra-ai",
            "text": "[Skip to content](https://github.com/mastra-ai/mastra#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/mastra-ai/mastra) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/mastra-ai/mastra) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/mastra-ai/mastra) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[mastra-ai](https://github.com/mastra-ai)/ **[mastra](https://github.com/mastra-ai/mastra)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmastra-ai%2Fmastra) You must be signed in to change notification settings\n- [Fork\\\n73](https://github.com/login?return_to=%2Fmastra-ai%2Fmastra)\n- [Star\\\n1.6k](https://github.com/login?return_to=%2Fmastra-ai%2Fmastra)\n\n\nthe TypeScript AI agent framework\n\n[mastra.ai](https://mastra.ai)\n\n### License\n\n[View license](https://github.com/mastra-ai/mastra/blob/main/LICENSE)\n\n[1.6k\\\nstars](https://github.com/mastra-ai/mastra/stargazers) [73\\\nforks](https://github.com/mastra-ai/mastra/forks) [Branches](https://github.com/mastra-ai/mastra/branches) [Tags](https://github.com/mastra-ai/mastra/tags) [Activity](https://github.com/mastra-ai/mastra/activity)\n\n[Star](https://github.com/login?return_to=%2Fmastra-ai%2Fmastra)\n\n[Notifications](https://github.com/login?return_to=%2Fmastra-ai%2Fmastra) You must be signed in to change notification settings\n\n# mastra-ai/mastra\n\nmain\n\n[Branches](https://github.com/mastra-ai/mastra/branches) [Tags](https://github.com/mastra-ai/mastra/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [6,593 Commits](https://github.com/mastra-ai/mastra/commits/main/) |\n| [.changeset](https://github.com/mastra-ai/mastra/tree/main/.changeset) | [.changeset](https://github.com/mastra-ai/mastra/tree/main/.changeset) | | |\n| [.dev](https://github.com/mastra-ai/mastra/tree/main/.dev) | [.dev](https://github.com/mastra-ai/mastra/tree/main/.dev) | | |\n| [.github/workflows](https://github.com/mastra-ai/mastra/tree/main/.github/workflows) | [.github/workflows](https://github.com/mastra-ai/mastra/tree/main/.github/workflows) | | |\n| [.husky](https://github.com/mastra-ai/mastra/tree/main/.husky) | [.husky](https://github.com/mastra-ai/mastra/tree/main/.husky) | | |\n| [deployers](https://github.com/mastra-ai/mastra/tree/main/deployers) | [deployers](https://github.com/mastra-ai/mastra/tree/main/deployers) | | |\n| [docs](https://github.com/mastra-ai/mastra/tree/main/docs) | [docs](https://github.com/mastra-ai/mastra/tree/main/docs) | | |\n| [examples](https://github.com/mastra-ai/mastra/tree/main/examples) | [examples](https://github.com/mastra-ai/mastra/tree/main/examples) | | |\n| [explorations/prompt](https://github.com/mastra-ai/mastra/tree/main/explorations/prompt) | [explorations/prompt](https://github.com/mastra-ai/mastra/tree/main/explorations/prompt) | | |\n| [generated-changelogs](https://github.com/mastra-ai/mastra/tree/main/generated-changelogs) | [generated-changelogs](https://github.com/mastra-ai/mastra/tree/main/generated-changelogs) | | |\n| [integrations](https://github.com/mastra-ai/mastra/tree/main/integrations) | [integrations](https://github.com/mastra-ai/mastra/tree/main/integrations) | | |\n| [packages](https://github.com/mastra-ai/mastra/tree/main/packages) | [packages](https://github.com/mastra-ai/mastra/tree/main/packages) | | |\n| [speech](https://github.com/mastra-ai/mastra/tree/main/speech) | [speech](https://github.com/mastra-ai/mastra/tree/main/speech) | | |\n| [storage](https://github.com/mastra-ai/mastra/tree/main/storage) | [storage](https://github.com/mastra-ai/mastra/tree/main/storage) | | |\n| [stores](https://github.com/mastra-ai/mastra/tree/main/stores) | [stores](https://github.com/mastra-ai/mastra/tree/main/stores) | | |\n| [vector-stores](https://github.com/mastra-ai/mastra/tree/main/vector-stores) | [vector-stores](https://github.com/mastra-ai/mastra/tree/main/vector-stores) | | |\n| [.env.example](https://github.com/mastra-ai/mastra/blob/main/.env.example) | [.env.example](https://github.com/mastra-ai/mastra/blob/main/.env.example) | | |\n| [.gitignore](https://github.com/mastra-ai/mastra/blob/main/.gitignore) | [.gitignore](https://github.com/mastra-ai/mastra/blob/main/.gitignore) | | |\n| [.prettierignore](https://github.com/mastra-ai/mastra/blob/main/.prettierignore) | [.prettierignore](https://github.com/mastra-ai/mastra/blob/main/.prettierignore) | | |\n| [.prettierrc](https://github.com/mastra-ai/mastra/blob/main/.prettierrc) | [.prettierrc](https://github.com/mastra-ai/mastra/blob/main/.prettierrc) | | |\n| [.vercelignore](https://github.com/mastra-ai/mastra/blob/main/.vercelignore) | [.vercelignore](https://github.com/mastra-ai/mastra/blob/main/.vercelignore) | | |\n| [CODE\\_OF\\_CONDUCT.md](https://github.com/mastra-ai/mastra/blob/main/CODE_OF_CONDUCT.md) | [CODE\\_OF\\_CONDUCT.md](https://github.com/mastra-ai/mastra/blob/main/CODE_OF_CONDUCT.md) | | |\n| [CONTRIBUTING.md](https://github.com/mastra-ai/mastra/blob/main/CONTRIBUTING.md) | [CONTRIBUTING.md](https://github.com/mastra-ai/mastra/blob/main/CONTRIBUTING.md) | | |\n| [DEVELOPMENT.md](https://github.com/mastra-ai/mastra/blob/main/DEVELOPMENT.md) | [DEVELOPMENT.md](https://github.com/mastra-ai/mastra/blob/main/DEVELOPMENT.md) | | |\n| [LICENSE](https://github.com/mastra-ai/mastra/blob/main/LICENSE) | [LICENSE](https://github.com/mastra-ai/mastra/blob/main/LICENSE) | | |\n| [README.md](https://github.com/mastra-ai/mastra/blob/main/README.md) | [README.md](https://github.com/mastra-ai/mastra/blob/main/README.md) | | |\n| [mastra-homepage.png](https://github.com/mastra-ai/mastra/blob/main/mastra-homepage.png) | [mastra-homepage.png](https://github.com/mastra-ai/mastra/blob/main/mastra-homepage.png) | | |\n| [package.json](https://github.com/mastra-ai/mastra/blob/main/package.json) | [package.json](https://github.com/mastra-ai/mastra/blob/main/package.json) | | |\n| [pnpm-lock.yaml](https://github.com/mastra-ai/mastra/blob/main/pnpm-lock.yaml) | [pnpm-lock.yaml](https://github.com/mastra-ai/mastra/blob/main/pnpm-lock.yaml) | | |\n| [pnpm-workspace.yaml](https://github.com/mastra-ai/mastra/blob/main/pnpm-workspace.yaml) | [pnpm-workspace.yaml](https://github.com/mastra-ai/mastra/blob/main/pnpm-workspace.yaml) | | |\n| [tsconfig.json](https://github.com/mastra-ai/mastra/blob/main/tsconfig.json) | [tsconfig.json](https://github.com/mastra-ai/mastra/blob/main/tsconfig.json) | | |\n| [tsconfig.node.json](https://github.com/mastra-ai/mastra/blob/main/tsconfig.node.json) | [tsconfig.node.json](https://github.com/mastra-ai/mastra/blob/main/tsconfig.node.json) | | |\n| [turbo.json](https://github.com/mastra-ai/mastra/blob/main/turbo.json) | [turbo.json](https://github.com/mastra-ai/mastra/blob/main/turbo.json) | | |\n| [vitest.config.ts](https://github.com/mastra-ai/mastra/blob/main/vitest.config.ts) | [vitest.config.ts](https://github.com/mastra-ai/mastra/blob/main/vitest.config.ts) | | |\n| View all files |\n\n## Repository files navigation\n\n# Mastra [![Project Status: Alpha](https://camo.githubusercontent.com/6854b02efa256be43fff1222f5699190a0f3f1cd830590ccadfabed6b0c2cc1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374617475732d416c7068612d726564)](https://camo.githubusercontent.com/6854b02efa256be43fff1222f5699190a0f3f1cd830590ccadfabed6b0c2cc1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374617475732d416c7068612d726564)[![Project Status: Alpha](https://camo.githubusercontent.com/eba86d8b38cba382489c0de325f7b9d4dac9848aa2793dc187d05a8cc9ba609b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732352d6f72616e67653f7374796c653d666c61742d737175617265)](https://camo.githubusercontent.com/eba86d8b38cba382489c0de325f7b9d4dac9848aa2793dc187d05a8cc9ba609b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732352d6f72616e67653f7374796c653d666c61742d737175617265)\n\nMastra is an opinionated Typescript framework that helps you build AI applications and features quickly. It gives you the set of primitives you need: workflows, agents, RAG, integrations and evals. You can run Mastra on your local machine, or deploy to a serverless cloud.\n\nThe main Mastra features are:\n\n| Features | Description |\n| --- | --- |\n| LLM Models | Mastra uses the [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini. You can choose the specific model and provider, and decide whether to stream the response. |\n| [Agents](https://mastra.ai/docs/agents/00-overview) | Agents are systems where the language model chooses a sequence of actions. In Mastra, agents provide LLM models with tools, workflows, and synced data. Agents can call your own functions or APIs of third-party integrations and access knowledge bases you build. |\n| [Tools](https://mastra.ai/docs/agents/02-adding-tools) | Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation. Each tool has a schema that defines its inputs, an executor function that implements its logic, and access to configured integrations. |\n| [Workflows](https://mastra.ai/docs/workflows/00-overview) | Workflows are durable graph-based state machines. They have loops, branching, wait for human input, embed other workflows, do error handling, retries, parsing and so on. They can be built in code or with a visual editor. Each step in a workflow has built-in OpenTelemetry tracing. |\n| [RAG](https://mastra.ai/docs/rag/overview) | Retrieval-augemented generation (RAG) lets you construct a knowledge base for agents. RAG is an ETL pipeline with specific querying techniques, including chunking, embedding, and vector search. |\n| [Integrations](https://mastra.ai/docs/local-dev/integrations) | In Mastra, integrations are auto-generated, type-safe API clients for third-party services that can be used as tools for agents or steps in workflows. |\n| [Evals](https://mastra.ai/docs/08-running-evals) | Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions. |\n\n## Quick Start\n\n### Prerequisites\n\n- Node.js (v20.0+)\n\n## Get an LLM provider API key\n\nIf you don't have an API key for an LLM provider, you can get one from the following services:\n\n- [OpenAI](https://platform.openai.com/)\n- [Anthropic](https://console.anthropic.com/settings/keys)\n- [Google Gemini](https://ai.google.dev/gemini-api/docs)\n\nIf you don't have an account with these providers, you can sign up and get an API key. OpenAI and Anthropic require a credit card to get an API key. Gemini does not and has a generous free tier for its API.\n\n## Create a new project\n\nAs a first step, create a project directory and navigate into it:\n\n```\nmkdir hello-mastra\ncd hello-mastra\n```\n\nNext, initialize a TypeScript project using npm:\n\n```\nnpm init -y\nnpm install typescript tsx @types/node --save-dev\nnpm install @mastra/core@alpha zod\n```\n\n### Add an index.ts file\n\n```\nmkdir src\ntouch src/index.ts\n```\n\nThen, add this code to `src/index.ts`:\n\n```\nimport { openai } from '@ai-sdk/openai';\nimport { Agent } from '@mastra/core/agent';\n\nasync function main() {\n const agent = new Agent({\n name: 'story-writer',\n model: openai('gpt-4o-mini'),\n instructions: `You are a helpful assistant who writes creative stories.`,\n tools: {},\n });\n\n const result = await agent.generate('Write a short story about a robot learning to paint.');\n\n console.log('Agent response:', result.text);\n}\n\nmain();\n```\n\n### Run the script\n\nFinally, run the script:\n\n```\nOPENAI_API_KEY= npx tsx src/index.ts\n```\n\nIf you're using Anthropic, set the `ANTHROPIC_API_KEY`. If you're using Gemini, set the `GOOGLE_GENERATIVE_AI_API_KEY`.\n\n## Contributing\n\nLooking to contribute? All types of help are appreciated, from coding to testing and feature specification.\n\nIf you are a developer and would like to contribute with code, please open an issue to discuss before opening a Pull Request.\n\nInformation about the project setup can be found in the [development documentation](https://github.com/mastra-ai/mastra/blob/main/DEVELOPMENT.md)\n\n## Support\n\nWe have an [open community Discord](https://discord.gg/BTYqqHKUrf). Come and say hello and let us know if you have any questions or need any help getting things running.\n\nIt's also super helpful if you leave the project a star here at the [top of the page](https://github.com/mastra-ai/mastra)\n\n## About\n\nthe TypeScript AI agent framework\n\n[mastra.ai](https://mastra.ai)\n\n### Topics\n\n[nodejs](https://github.com/topics/nodejs) [javascript](https://github.com/topics/javascript) [typescript](https://github.com/topics/typescript) [ai](https://github.com/topics/ai) [reactjs](https://github.com/topics/reactjs) [mcp](https://github.com/topics/mcp) [nextjs](https://github.com/topics/nextjs) [tts](https://github.com/topics/tts) [chatbots](https://github.com/topics/chatbots) [workflows](https://github.com/topics/workflows) [agents](https://github.com/topics/agents) [llm](https://github.com/topics/llm) [evals](https://github.com/topics/evals)\n\n### Resources\n\n[Readme](https://github.com/mastra-ai/mastra#readme-ov-file)\n\n### License\n\n[View license](https://github.com/mastra-ai/mastra#License-1-ov-file)\n\n### Code of conduct\n\n[Code of conduct](https://github.com/mastra-ai/mastra#coc-ov-file)\n\n[Activity](https://github.com/mastra-ai/mastra/activity)\n\n[Custom properties](https://github.com/mastra-ai/mastra/custom-properties)\n\n### Stars\n\n[**1.6k**\\\nstars](https://github.com/mastra-ai/mastra/stargazers)\n\n### Watchers\n\n[**11**\\\nwatching](https://github.com/mastra-ai/mastra/watchers)\n\n### Forks\n\n[**73**\\\nforks](https://github.com/mastra-ai/mastra/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmastra-ai%2Fmastra&report=mastra-ai+%28user%29)\n\n## [Releases\\ 9](https://github.com/mastra-ai/mastra/releases)\n\n[v0.1.10Latest\\\n\\\nOct 14, 2024](https://github.com/mastra-ai/mastra/releases/tag/v0.1.10)\n\n[\\+ 8 releases](https://github.com/mastra-ai/mastra/releases)\n\n## [Contributors\\ 33](https://github.com/mastra-ai/mastra/graphs/contributors)\n\n[\\+ 19 contributors](https://github.com/mastra-ai/mastra/graphs/contributors)\n\n## Languages\n\n- [TypeScript99.8%](https://github.com/mastra-ai/mastra/search?l=typescript)\n- Other0.2%\n\nYou can’t perform that action at this time.",
            "image": "https://opengraph.githubassets.com/d89a7c0a0bf89bd4be5b793b82a7eb3d5fb39fd8b20f65a7102090e67399f12e/mastra-ai/mastra",
            "favicon": "https://github.com/fluidicon.png",
            "extras": {
              "links": [
                "https://github.com/mastra-ai/mastra",
                "https://github.com/",
                "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmastra-ai%2Fmastra",
                "https://github.com/features/copilot"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/getting-started/installation",
            "title": "Installing Mastra Locally | Getting Started | Mastra Docs",
            "url": "https://mastra.ai/docs/getting-started/installation",
            "author": "",
            "text": "\n To run Mastra, you need access to an LLM. Typically, you’ll want to get an API key from an LLM provider such as OpenAI, Anthropic, or Google Gemini. You can also run Mastra with a local LLM using Ollama. \n Prerequisites \n \n Node.js or higher \n Access to a supported large language model (LLM) \n \n Automatic Installation \n \n Note: If you prefer to run the command with flags (non-interactive mode) and include the example code, you can use: \n This allows you to specify your preferences upfront without being prompted. \n Manual Installation \n \n Start the Mastra Server \n Mastra provides commands to serve your agents via REST endpoints \n Development Server \n Run the following command to start the Mastra server: \n If you have the mastra CLI installed, run: \n This command creates REST API endpoints for your agents. \n Test the Endpoint \n You can test the agent’s endpoint using or : \n Run from the command line \n If you’d like to directly call agents from the command line, you can create a script to get an agent and call it: \n Then, run the script to test that everything is set up correctly: \n This should output the agent’s response to your console. \n Introduction Project Structure",
            "image": "https://mastra.ai/api/og/docs?title=Installing%20Mastra%20Locally%20%7C%20Getting%20Started%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/getting-started/project-structure",
            "title": "Local Project Structure | Getting Started | Mastra Docs",
            "url": "https://mastra.ai/docs/getting-started/project-structure",
            "author": "",
            "text": "\n This page provides a guide for organizing folders and files in Mastra. Mastra is a modular framework, and you can use any of the modules separately or together. \n You could write everything in a single file (as we showed in the quick start), or separate each agent, tool, and workflow into their own files. \n We don’t enforce a specific folder structure, but we do recommend some best practices, and the CLI will scaffold a project with a sensible structure. \n Using the CLI \n is an interactive CLI that allows you to: \n \n Choose a directory for Mastra files: Specify where you want the Mastra files to be placed (default is ). \n Select components to install: Choose which components you want to include in your project:\n \n Agents \n Tools \n Workflows \n \n \n Select a default LLM provider: Choose from supported providers like OpenAI, Anthropic, or Groq. \n Include example code: Decide whether to include example code to help you get started. \n \n Example Project Structure \n Assuming you select all components and include example code, your project structure will look like this: \n Top-level Folders \n Top-level Files \n Installation Agents: Chef Michel",
            "image": "https://mastra.ai/api/og/docs?title=Local%20Project%20Structure%20%7C%20Getting%20Started%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/getting-started/mcp-docs-server",
            "title": "Using with Cursor/Windsurf | Getting Started | Mastra Docs",
            "url": "https://mastra.ai/docs/getting-started/mcp-docs-server",
            "author": "",
            "text": "Mastra Tools for your agentic IDE \n @mastra/mcp-docs-server provides direct access to Mastra’s complete knowledge base in Cursor, Windsurf, Cline, or any other IDE that supports MCP. \n It has access to documentation, code examples, technical blog posts / feature announcements, and package changelogs which your IDE can read to help you build with Mastra. \n \n The MCP server tools have been designed to allow an agent to query the specific information it needs to complete a Mastra related task - for example: adding a Mastra feature to an agent, scaffolding a new project, or helping you understand how something works. \n How it works \n Once it’s installed in your IDE you can write prompts and assume the agent will understand everything about Mastra. \n Add features \n \n “Add evals to my agent and write tests” \n “Write me a workflow that does the following [task] ” \n “Make a new tool that allows my agent to access [3rd party API] ” \n \n Ask about integrations \n \n “Does Mastra work with the AI SDK?\nHow can I use it in my [React/Svelte/etc] project?” \n “What’s the latest Mastra news around MCP?” \n “Does Mastra support [provider] speech and voice APIs? Show me an example in my code of how I can use it.” \n \n Debug or update existing code \n \n “I’m running into a bug with agent memory, have there been any related changes or bug fixes recently?” \n “How does working memory behave in Mastra and how can I use it to do [task]? It doesn’t seem to work the way I expect.” \n “I saw there are new workflow features, explain them to me and then update [workflow] to use them.” \n \n And more - if you have a question, try asking your IDE and let it look it up for you. \n Automatic Installation \n Run pnpm create mastra@latest and select Cursor or Windsurf when prompted to install the MCP server. For other IDEs, or if you already have a Mastra project, install the MCP server by following the instructions below. \n Manual Installation \n \n Cursor: Edit.cursor/mcp.json in your project root, or ~/.cursor/mcp.json for global configuration \n Windsurf: Edit ~/.codeium/windsurf/mcp_config.json (only supports global configuration) \n \n Add the following configuration: \n MacOS/Linux \n { \n \" mcpServers \": { \n \" mastra \": { \n \" command \": \" npx \", \n \" args \": [ \" -y \", \" @mastra/mcp-docs-server@latest \"] \n } \n } \n} \n Windows \n { \n \" mcpServers \": { \n \" mastra \": { \n \" command \": \" cmd \", \n \" args \": [ \" /c \", \" npx \", \" -y \", \" @mastra/mcp-docs-server@latest \"] \n } \n } \n} \n After Configuration \n Cursor \n If you followed the automatic installation, you’ll see a popup when you open cursor in the bottom left corner to prompt you to enable the Mastra Docs MCP Server. \n Otherwise, for manual installation, do the following. \n \n Open Cursor settings \n Navigate to MCP settings \n Click “enable” on the Mastra MCP server \n If you have an agent chat open, you’ll need to re-open it or start a new chat to use the MCP server \n \n Windsurf \n \n Fully quit and re-open Windsurf \n If tool calls start failing, go to Windsurfs MCP settings and re-start the MCP server. This is a common Windsurf MCP issue and isn’t related to Mastra. Right now Cursor’s MCP implementation is more stable than Windsurfs is. \n \n In both IDEs it may take a minute for the MCP server to start the first time as it needs to download the package from npm. \n Available Agent Tools \n Documentation \n Access Mastra’s complete documentation: \n \n Getting started / installation \n Guides and tutorials \n API references \n \n Examples \n Browse code examples: \n \n Complete project structures \n Implementation patterns \n Best practices \n \n Blog Posts \n Search the blog for: \n \n Technical posts \n Changelog and feature announcements \n AI news and updates \n \n Package Changes \n Track updates for Mastra and @mastra/* packages: \n \n Bug fixes \n New features \n Breaking changes \n \n Common Issues \n \n \n Server Not Starting \n \n Ensure npx is installed and working \n Check for conflicting MCP servers \n Verify your configuration file syntax \n On Windows, make sure to use the Windows-specific configuration \n \n \n \n Tool Calls Failing \n \n Restart the MCP server and/or your IDE \n Update to the latest version of your IDE \n \n \n Project Structure Model Capabilities",
            "image": "https://mastra.ai/api/og/docs?title=Using%20with%20Cursor/Windsurf%20|%20Getting%20Started%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20the%20Mastra%20MCP%20documentation%20server%20in%20your%20IDE%20to%20turn%20it%20into%20an%20agentic%20Mastra%20expert.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/getting-started/mcp-docs-server",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/getting-started/model-capability",
            "title": "Model Capability",
            "url": "https://mastra.ai/docs/getting-started/model-capability",
            "publishedDate": "2025-02-19T00:00:00.000Z",
            "author": "",
            "text": "Model Capabilities \n The AI providers support different language models with various capabilities. Not all models support structured output, image input, object generation, tool usage, or tool streaming. \n Here are the capabilities of popular models: \n AI Model Capabilities by Provider Provider Model Image Input Object Generation Tool Usage Tool Streaming xAI Grok grok-3 xAI Grok grok-3-fast xAI Grok grok-3-mini xAI Grok grok-3-mini-fast xAI Grok grok-2-1212 xAI Grok grok-2-vision-1212 xAI Grok grok-beta xAI Grok grok-vision-beta OpenAI gpt-4.1 OpenAI gpt-4.1-mini OpenAI gpt-4.1-nano OpenAI gpt-4o OpenAI gpt-4o-mini OpenAI gpt-4-turbo OpenAI gpt-4 OpenAI o3-mini OpenAI o1 OpenAI o1-mini OpenAI o1-preview Anthropic claude-3-7-sonnet-20250219 Anthropic claude-3-5-sonnet-20241022 Anthropic claude-3-5-sonnet-20240620 Anthropic claude-3-5-haiku-20241022 Mistral pixtral-large-latest Mistral mistral-large-latest Mistral mistral-small-latest Mistral pixtral-12b-2409 Google Generative AI gemini-2.0-flash-exp Google Generative AI gemini-1.5-flash Google Generative AI gemini-1.5-pro Google Vertex gemini-2.0-flash-exp Google Vertex gemini-1.5-flash Google Vertex gemini-1.5-pro DeepSeek deepseek-chat DeepSeek deepseek-reasoner Cerebras llama3.1-8b Cerebras llama3.1-70b Cerebras llama3.3-70b Groq meta-llama/llama-4-scout-17b-16e-instruct Groq llama-3.3-70b-versatile Groq llama-3.1-8b-instant Groq mixtral-8x7b-32768 Groq gemma2-9b-it \n Source: https://sdk.vercel.ai/docs/foundations/providers-and-models#model-capabilities ",
            "image": "https://mastra.ai/api/og/docs?title=Model%20Capability&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/getting-started/model-capability",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/agents/overview",
            "title": "Creating and Calling Agents | Agent Documentation | Mastra",
            "url": "https://mastra.ai/docs/agents/overview",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "\n Agents in Mastra are systems where the language model can autonomously decide on a sequence of actions to perform tasks. They have access to tools, workflows, and synced data, enabling them to perform complex tasks and interact with external systems. Agents can invoke your custom functions, utilize third-party APIs through integrations, and access knowledge bases you have built. \n Agents are like employees who can be used for ongoing projects. They have names, persistent memory, consistent model configurations, and instructions across calls, as well as a set of enabled tools. \n 1. Creating an Agent \n To create an agent in Mastra, you use the Agent class and define its properties: \n src/mastra/agents/index.ts import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n export const myAgent = new Agent ({ \n name: \" My Agent \", \n instructions: \" You are a helpful assistant. \", \n model: openai ( \" gpt-4o-mini \"), \n}); \n Note: Ensure that you have set the necessary environment variables, such as your OpenAI API key, in your.env file: \n.env OPENAI_API_KEY=your_openai_api_key \n Also, make sure you have the @mastra/core package installed: \n Registering the Agent \n Register your agent with Mastra to enable logging and access to configured tools and integrations: \n import { Mastra } from \" @mastra/core \"; \n import { myAgent } from \"./agents \"; \n \n export const mastra = new Mastra ({ \n agents: { myAgent }, \n}); \n 2. Generating and streaming text \n Generating text \n Use the.generate() method to have your agent produce text responses: \n const response = await myAgent. generate ([ \n { role: \" user \", content: \" Hello, how can you assist me today? \" }, \n]); \n \n console. log ( \" Agent: \", response.text); \n For more details about the generate method and its options, see the generate reference documentation. \n Streaming responses \n For more real-time responses, you can stream the agent’s response: \n const stream = await myAgent. stream ([ \n { role: \" user \", content: \" Tell me a story. \" }, \n]); \n \n console. log ( \" Agent: \"); \n \n for await ( const chunk of stream.textStream) { \n process.stdout. write (chunk); \n} \n For more details about streaming responses, see the stream reference documentation. \n 3. Structured Output \n Agents can return structured data by providing a JSON Schema or using a Zod schema. \n Using JSON Schema \n const schema = { \n type: \" object \", \n properties: { \n summary: { type: \" string \" }, \n keywords: { type: \" array \", items: { type: \" string \" } }, \n }, \n additionalProperties: false, \n required: [ \" summary \", \" keywords \"], \n}; \n \n const response = await myAgent. generate ( \n [ \n { \n role: \" user \", \n content: \n \" Please provide a summary and keywords for the following text: ... \", \n }, \n ], \n { \n output: schema, \n }, \n); \n \n console. log ( \" Structured Output: \", response.object); \n Using Zod \n You can also use Zod schemas for type-safe structured outputs. \n First, install Zod: \n Then, define a Zod schema and use it with the agent: \n import { z } from \" zod \"; \n \n // Define the Zod schema \n const schema = z. object ({ \n summary: z. string (), \n keywords: z. array (z. string ()), \n}); \n \n // Use the schema with the agent \n const response = await myAgent. generate ( \n [ \n { \n role: \" user \", \n content: \n \" Please provide a summary and keywords for the following text: ... \", \n }, \n ], \n { \n output: schema, \n }, \n); \n \n console. log ( \" Structured Output: \", response.object); \n Using Tools \n If you need to generate structured output alongside tool calls, you’ll need to use the experimental_output property instead of output. Here’s how: \n const schema = z. object ({ \n summary: z. string (), \n keywords: z. array (z. string ()), \n}); \n \n const response = await myAgent. generate ( \n [ \n { \n role: \" user \", \n content: \n \" Please analyze this repository and provide a summary and keywords... \", \n }, \n ], \n { \n // Use experimental_output to enable both structured output and tool calls \n experimental_output: schema, \n }, \n); \n \n console. log ( \" Structured Output: \", response.object); \n This allows you to have strong typing and validation for the structured data returned by the agent. \n 4. Multi-step Tool use Agents \n Agents can be enhanced with tools - functions that extend their capabilities beyond text generation. Tools allow agents to perform calculations, access external systems, and process data. For details on creating and configuring tools, see the Adding Tools documentation. \n Using maxSteps \n The maxSteps parameter controls the maximum number of sequential LLM calls an agent can make, particularly important when using tool calls. By default, it is set to 1 to prevent infinite loops in case of misconfigured tools. You can increase this limit based on your use case: \n src/mastra/agents/index.ts import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n import * as mathjs from \" mathjs \"; \n import { z } from \" zod \"; \n \n export const myAgent = new Agent ({ \n name: \" My Agent \", \n instructions: \" You are a helpful assistant that can solve math problems. \", \n model: openai ( \" gpt-4o-mini \"), \n tools: { \n calculate: { \n description: \" Calculator for mathematical expressions \", \n schema: z. object ({ expression: z. string () }), \n execute: async ({ expression }) =&gt; mathjs. evaluate (expression), \n }, \n }, \n}); \n \n const response = await myAgent. generate ( \n [ \n { \n role: \" user \", \n content: \n \" If a taxi driver earns $9461 per hour and works 12 hours a day, how much does they earn in one day? \", \n }, \n ], \n { \n maxSteps: 5, // Allow up to 5 tool usage steps \n }, \n); \n Using onStepFinish \n You can monitor the progress of multi-step operations using the onStepFinish callback. This is useful for debugging or providing progress updates to users.\n onStepFinish is only available when streaming or generating text without structured output. \n src/mastra/agents/index.ts const response = await myAgent. generate ( \n [{ role: \" user \", content: \" Calculate the taxi driver's daily earnings. \" }], \n { \n maxSteps: 5, \n onStepFinish: ({ text, toolCalls, toolResults }) =&gt; { \n console. log ( \" Step completed: \", { text, toolCalls, toolResults }); \n }, \n }, \n); \n Using onFinish \n The onFinish callback is available when streaming responses and provides detailed information about the completed interaction. It is called after the LLM has finished generating its response and all tool executions have completed.\nThis callback receives the final response text, execution steps, token usage statistics, and other metadata that can be useful for monitoring and logging: \n src/mastra/agents/index.ts const stream = await myAgent. stream ( \n [{ role: \" user \", content: \" Calculate the taxi driver's daily earnings. \" }], \n { \n maxSteps: 5, \n onFinish: ({ \n steps, \n text, \n finishReason, // 'complete', 'length', 'tool', etc. \n usage, // token usage statistics \n reasoningDetails, // additional context about the agent's decisions \n }) =&gt; { \n console. log ( \" Stream complete: \", { \n totalSteps: steps.length, \n finishReason, \n usage, \n }); \n }, \n }, \n); \n 5. Running Agents \n Mastra provides a CLI command mastra dev to run your agents behind an API. By default, this looks for exported agents in files in the src/mastra/agents directory. \n Starting the Server \n This will start the server and make your agent available at http://localhost:4111/api/agents/myAgent/generate. \n Interacting with the Agent \n You can interact with the agent using curl from the command line: \n curl -X POST http://localhost:4111/api/agents/myAgent/generate \\ \n -H \" Content-Type: application/json \" \\ \n -d ' { \n \"messages\": [ \n { \"role\": \"user\", \"content\": \"Hello, how can you assist me today?\" } \n ] \n } ' \n Next Steps \n \n Learn about Agent Memory in the Agent Memory guide. \n Learn about Agent Tools in the Agent Tools guide. \n See an example agent in the Chef Michel example. \n",
            "image": "https://mastra.ai/api/og/docs?title=Creating%20and%20Calling%20Agents%20|%20Agent%20Documentation%20|%20Mastra&description=Overview%20of%20agents%20in%20Mastra,%20detailing%20their%20capabilities%20and%20how%20they%20interact%20with%20tools,%20workflows,%20and%20external%20systems.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/docs/agents/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/agents/agent-memory",
            "title": "Using Agent Memory | Agents | Mastra Docs",
            "url": "https://mastra.ai/docs/agents/agent-memory",
            "author": "",
            "text": "\n Agents in Mastra can leverage a powerful memory system to store conversation history, recall relevant information, and maintain persistent context across interactions. This allows agents to have more natural, stateful conversations. \n Enabling Memory for an Agent \n To enable memory, simply instantiate the Memory class and pass it to your agent’s configuration. You also need to install the memory package: \n npm install @mastra/memory@latest \n import { Agent } from \" @mastra/core/agent \"; \n import { Memory } from \" @mastra/memory \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n // Basic memory setup \n const memory = new Memory (); \n \n const agent = new Agent ({ \n name: \" MyMemoryAgent \", \n instructions: \" You are a helpful assistant with memory. \", \n model: openai ( \" gpt-4o \"), \n memory: memory, // Attach the memory instance \n}); \n This basic setup uses default settings, including LibSQL for storage and FastEmbed for embeddings. For detailed setup instructions, see Memory. \n Using Memory in Agent Calls \n To utilize memory during interactions, you must provide resourceId and threadId when calling the agent’s stream() or generate() methods. \n \n resourceId: Typically identifies the user or entity (e.g., user_123). \n threadId: Identifies a specific conversation thread (e.g., support_chat_456). \n \n // Example agent call using memory \n await agent. stream ( \" Remember my favorite color is blue. \", { \n resourceId: \" user_alice \", \n threadId: \" preferences_thread \", \n}); \n \n // Later in the same thread... \n const response = await agent. stream ( \" What's my favorite color? \", { \n resourceId: \" user_alice \", \n threadId: \" preferences_thread \", \n}); \n // Agent will use memory to recall the favorite color. \n These IDs ensure that conversation history and context are correctly stored and retrieved for the appropriate user and conversation. \n Next Steps \n Keep exploring Mastra’s memory capabilities like threads, conversation history, semantic recall, and working memory. Overview Tools and MCP",
            "image": "https://mastra.ai/api/og/docs?title=Using%20Agent%20Memory%20|%20Agents%20|%20Mastra%20Docs&description=Documentation%20on%20how%20agents%20in%20Mastra%20use%20memory%20to%20store%20conversation%20history%20and%20contextual%20information.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/agents/agent-memory",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/agents/using-tools-and-mcp",
            "title": "Using Tools with Agents | Agents | Mastra Docs",
            "url": "https://mastra.ai/docs/agents/using-tools-and-mcp",
            "author": "",
            "text": "\n Tools are typed functions that can be executed by agents or workflows. Each tool has a schema defining its inputs, an executor function implementing its logic, and optional access to configured integrations. \n Creating Tools \n Here’s a basic example of creating a tool: \n src/mastra/tools/weatherInfo.ts import { createTool } from \" @mastra/core/tools \"; \n import { z } from \" zod \"; \n \n export const weatherInfo = createTool ({ \n id: \" Get Weather Information \", \n inputSchema: z. object ({ \n city: z. string (), \n }), \n description: ` Fetches the current weather information for a given city `, \n execute: async ({ context: { city } }) =&gt; { \n // Tool logic here (e.g., API call) \n console. log ( \" Using tool to fetch weather information for \", city); \n return { temperature: 20, conditions: \" Sunny \" }; // Example return \n }, \n}); \n For details on creating and designing tools, see the Tools Overview. \n Adding Tools to an Agent \n To make a tool available to an agent, add it to the tools property in the agent’s configuration. \n src/mastra/agents/weatherAgent.ts import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n import { weatherInfo } from \"../tools/weatherInfo \"; \n \n export const weatherAgent = new Agent ({ \n name: \" Weather Agent \", \n instructions: \n \" You are a helpful assistant that provides current weather information. When asked about the weather, use the weather information tool to fetch the data. \", \n model: openai ( \" gpt-4o-mini \"), \n tools: { \n weatherInfo, \n }, \n}); \n When you call the agent, it can now decide to use the configured tool based on its instructions and the user’s prompt. \n Adding MCP Tools to an Agent \n Model Context Protocol (MCP)  provides a standardized way for AI models to discover and interact with external tools and resources. You can connect your Mastra agent to MCP servers to use tools provided by third parties. \n For more details on MCP concepts and how to set up MCP clients and servers, see the MCP Overview. \n Installation \n First, install the Mastra MCP package: \n npm install @mastra/mcp@latest \n Using MCP Tools \n Because there are so many MCP server registries to choose from, we’ve created an MCP Registry Registry  to help you find MCP servers. \n Once you have a server you want to use with your agent, import the Mastra MCPClient and add the server configuration. \n import { MCPClient } from \" @mastra/mcp \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n // Configure MCPClient to connect to your server(s) \n export const mcp = new MCPClient ({ \n servers: { \n filesystem: { \n command: \" npx \", \n args: [ \n \" -y \", \n \" @modelcontextprotocol/server-filesystem \", \n \" /Users/username/Downloads \", \n ], \n }, \n }, \n}); \n Then connect your agent to the server tools: \n src/mastra/agents/mcpAgent.ts import { mcp } from \"../mcp \"; \n // Create an agent and add tools from the MCP client \n const agent = new Agent ({ \n name: \" Agent with MCP Tools \", \n instructions: \" You can use tools from connected MCP servers. \", \n model: openai ( \" gpt-4o-mini \"), \n tools: await mcp. getTools (), \n}); \n For more details on configuring MCPClient and the difference between static and dynamic MCP server configurations, see the MCP Overview.",
            "image": "https://mastra.ai/api/og/docs?title=Using%20Tools%20with%20Agents%20|%20Agents%20|%20Mastra%20Docs&description=Learn%20how%20to%20create%20tools,%20add%20them%20to%20Mastra%20agents,%20and%20integrate%20tools%20from%20MCP%20servers.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/agents/using-tools-and-mcp",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/agents/adding-voice",
            "title": "Adding Voice to Agents",
            "url": "https://mastra.ai/docs/agents/adding-voice",
            "author": "",
            "text": "\n Mastra agents can be enhanced with voice capabilities, allowing them to speak responses and listen to user input. You can configure an agent to use either a single voice provider or combine multiple providers for different operations. \n Using a Single Provider \n The simplest way to add voice to an agent is to use a single provider for both speaking and listening: \n import { createReadStream } from \" fs \"; \n import path from \" path \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n // Initialize the voice provider with default settings \n const voice = new OpenAIVoice (); \n \n // Create an agent with voice capabilities \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with both STT and TTS capabilities. `, \n model: openai ( \" gpt-4o \"), \n voice, \n}); \n \n // The agent can now use voice for interaction \n const audioStream = await agent.voice. speak ( \" Hello, I'm your AI assistant! \", { \n filetype: \" m4a \", \n}); \n \n playAudio (audioStream!); \n \n try { \n const transcription = await agent.voice. listen (audioStream); \n console. log (transcription) \n} catch (error) { \n console. error ( \" Error transcribing audio: \", error); \n} \n Using Multiple Providers \n For more flexibility, you can use different providers for speaking and listening using the CompositeVoice class: \n import { Agent } from \" @mastra/core/agent \"; \n import { CompositeVoice } from \" @mastra/core/voice \"; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { PlayAIVoice } from \" @mastra/voice-playai \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with both STT and TTS capabilities. `, \n model: openai ( \" gpt-4o \"), \n \n // Create a composite voice using OpenAI for listening and PlayAI for speaking \n voice: new CompositeVoice ({ \n input: new OpenAIVoice (), \n output: new PlayAIVoice (), \n }), \n}); \n Working with Audio Streams \n The speak() and listen() methods work with Node.js streams. Here’s how to save and load audio files: \n Saving Speech Output \n The speak method returns a stream that you can pipe to a file or speaker. \n import { createWriteStream } from \" fs \"; \n import path from \" path \"; \n \n // Generate speech and save to file \n const audio = await agent.voice. speak ( \" Hello, World! \"); \n const filePath = path. join (process. cwd (), \" agent.mp3 \"); \n const writer = createWriteStream (filePath); \n \n audio. pipe (writer); \n \n await new Promise &lt; void &gt;((resolve, reject) =&gt; { \n writer. on ( \" finish \", () =&gt; resolve ()); \n writer. on ( \" error \", reject); \n}); \n Transcribing Audio Input \n The listen method expects a stream of audio data from a microphone or file. \n import { createReadStream } from \" fs \"; \n import path from \" path \"; \n \n // Read audio file and transcribe \n const audioFilePath = path. join (process. cwd (), \" /agent.m4a \"); \n const audioStream = createReadStream (audioFilePath); \n \n try { \n console. log ( \" Transcribing audio file... \"); \n const transcription = await agent.voice. listen (audioStream, { \n filetype: \" m4a \", \n }); \n console. log ( \" Transcription: \", transcription); \n} catch (error) { \n console. error ( \" Error transcribing audio: \", error); \n} \n Speech-to-Speech Voice Interactions \n For more dynamic and interactive voice experiences, you can use real-time voice providers that support speech-to-speech capabilities: \n import { Agent } from \" @mastra/core/agent \"; \n import { getMicrophoneStream } from \" @mastra/node-audio \"; \n import { OpenAIRealtimeVoice } from \" @mastra/voice-openai-realtime \"; \n import { search, calculate } from \"../tools \"; \n \n // Initialize the realtime voice provider \n const voice = new OpenAIRealtimeVoice ({ \n chatModel: { \n apiKey: process.env.OPENAI_API_KEY, \n model: \" gpt-4o-mini-realtime \", \n }, \n speaker: \" alloy \", \n}); \n \n // Create an agent with speech-to-speech voice capabilities \n export const agent = new Agent ({ \n name: \" Agent \", \n instructions: ` You are a helpful assistant with speech-to-speech capabilities. `, \n model: openai ( \" gpt-4o \"), \n tools: { \n // Tools configured on Agent are passed to voice provider \n search, \n calculate, \n }, \n voice, \n}); \n \n // Establish a WebSocket connection \n await agent.voice. connect (); \n \n // Start a conversation \n agent.voice. speak ( \" Hello, I'm your AI assistant! \"); \n \n // Stream audio from a microphone \n const microphoneStream = getMicrophoneStream (); \n agent.voice. send (microphoneStream); \n \n // When done with the conversation \n agent.voice. close (); \n Event System \n The realtime voice provider emits several events you can listen for: \n // Listen for speech audio data sent from voice provider \n agent.voice. on ( \" speaking \", ({ audio }) =&gt; { \n // audio contains ReadableStream or Int16Array audio data \n}); \n \n // Listen for transcribed text sent from both voice provider and user \n agent.voice. on ( \" writing \", ({ text, role }) =&gt; { \n console. log ( `${ role} said: ${ text}`); \n}); \n \n // Listen for errors \n agent.voice. on ( \" error \", (error) =&gt; { \n console. error ( \" Voice error: \", error); \n}); \n Supported Voice Providers \n Mastra supports multiple voice providers for text-to-speech (TTS) and speech-to-text (STT) capabilities: \n Provider Package Features Reference OpenAI @mastra/voice-openai TTS, STT Documentation OpenAI Realtime @mastra/voice-openai-realtime Realtime speech-to-speech Documentation ElevenLabs @mastra/voice-elevenlabs High-quality TTS Documentation PlayAI @mastra/voice-playai TTS Documentation Google @mastra/voice-google TTS, STT Documentation Deepgram @mastra/voice-deepgram STT Documentation Murf @mastra/voice-murf TTS Documentation Speechify @mastra/voice-speechify TTS Documentation Sarvam @mastra/voice-sarvam TTS, STT Documentation Azure @mastra/voice-azure TTS, STT Documentation Cloudflare @mastra/voice-cloudflare TTS Documentation \n For more details on voice capabilities, see the Voice API Reference.",
            "image": "https://mastra.ai/api/og/docs?title=Adding%20Voice%20to%20Agents&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/agents/adding-voice",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/agents/runtime-variables",
            "title": "Runtime context | Agents | Mastra Docs",
            "url": "https://mastra.ai/docs/agents/runtime-variables",
            "author": "",
            "text": "\n Mastra provides runtime context, which is a system based on dependency injection that enables you to configure your agents and tools with runtime variables. If you find yourself creating several different agents that do very similar things, runtime context allows you to combine them into one agent. \n Overview \n The dependency injection system allows you to: \n \n Pass runtime configuration variables to agents through a type-safe runtimeContext \n Access these variables within tool execution contexts \n Modify agent behavior without changing the underlying code \n Share configuration across multiple tools within the same agent \n \n Basic Usage \n const agent = mastra. getAgent ( \" weatherAgent \"); \n \n // Define your runtimeContext's type structure \n type WeatherRuntimeContext = { \n \" temperature-scale \": \" celsius \" | \" fahrenheit \"; // Fixed typo in \"fahrenheit\" \n}; \n \n const runtimeContext = new RuntimeContext &lt; WeatherRuntimeContext &gt;(); \n runtimeContext. set ( \" temperature-scale \", \" celsius \"); \n \n const response = await agent. generate ( \" What's the weather like today? \", { \n runtimeContext, \n}); \n \n console. log (response.text); \n Using with REST API \n Here’s how to dynamically set temperature units based on a user’s location using the Cloudflare CF-IPCountry header: \n import { Mastra } from \" @mastra/core \"; \n import { RuntimeContext } from \" @mastra/core/di \"; \n import { agent as weatherAgent } from \"./agents/weather \"; \n \n // Define RuntimeContext type with clear, descriptive types \n type WeatherRuntimeContext = { \n \" temperature-scale \": \" celsius \" | \" fahrenheit \"; \n}; \n \n export const mastra = new Mastra ({ \n agents: { \n weather: weatherAgent, \n }, \n server: { \n middleware: [ \n async (c, next) =&gt; { \n const country = c.req. header ( \" CF-IPCountry \"); \n const runtimeContext = c. get &lt; WeatherRuntimeContext &gt;( \" runtimeContext \"); \n \n // Set temperature scale based on country \n runtimeContext. set ( \n \" temperature-scale \", \n country === \" US \" ? \" fahrenheit \" : \" celsius \", \n ); \n \n await next (); // Don't forget to call next() \n }, \n ], \n }, \n}); \n Creating Tools with Variables \n Tools can access runtimeContext variables and must conform to the agent’s runtimeContext type: \n import { createTool } from \" @mastra/core/tools \"; \n import { z } from \" zod \"; \n \n export const weatherTool = createTool ({ \n id: \" getWeather \", \n description: \" Get the current weather for a location \", \n inputSchema: z. object ({ \n location: z. string (). describe ( \" The location to get weather for \"), \n }), \n execute: async ({ context, runtimeContext }) =&gt; { \n // Type-safe access to runtimeContext variables \n const temperatureUnit = runtimeContext. get ( \" temperature-scale \"); \n \n const weather = await fetchWeather (context.location, { \n temperatureUnit, \n }); \n \n return { result: weather }; \n }, \n}); \n \n async function fetchWeather ( \n location: string, \n { temperatureUnit }: { temperatureUnit: \" celsius \" | \" fahrenheit \" }, \n): Promise &lt; WeatherResponse &gt; { \n // Implementation of weather API call \n const response = await weatherApi. fetch (location, temperatureUnit); \n \n return { \n location, \n temperature: \" 72°F \", \n conditions: \" Sunny \", \n unit: temperatureUnit, \n }; \n} Adding Voice Overview",
            "image": "https://mastra.ai/api/og/docs?title=Runtime%20context%20|%20Agents%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20Mastra%27s%20dependency%20injection%20system%20to%20provide%20runtime%20configuration%20to%20agents%20and%20tools.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/agents/runtime-variables",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/tools-mcp/overview",
            "title": "Tools Overview | Tools & MCP | Mastra Docs",
            "url": "https://mastra.ai/docs/tools-mcp/overview",
            "author": "",
            "text": "\n Tools are functions that agents can execute to perform specific tasks or access external information. They extend an agent’s capabilities beyond simple text generation, allowing interaction with APIs, databases, or other systems. \n Each tool typically defines: \n \n Inputs: What information the tool needs to run (defined with an inputSchema, often using Zod). \n Outputs: The structure of the data the tool returns (defined with an outputSchema). \n Execution Logic: The code that performs the tool’s action. \n Description: Text that helps the agent understand what the tool does and when to use it. \n \n Creating Tools \n In Mastra, you create tools using the createTool function from the @mastra/core/tools package. \n src/mastra/tools/weatherInfo.ts import { createTool } from \" @mastra/core/tools \"; \n import { z } from \" zod \"; \n \n const getWeatherInfo = async (city: string) =&gt; { \n // Replace with an actual API call to a weather service \n console. log ( ` Fetching weather for ${ city}... `); \n // Example data structure \n return { temperature: 20, conditions: \" Sunny \" }; \n}; \n \n export const weatherTool = createTool ({ \n id: \" Get Weather Information \", \n description: ` Fetches the current weather information for a given city `, \n inputSchema: z. object ({ \n city: z. string (). describe ( \" City name \"), \n }), \n outputSchema: z. object ({ \n temperature: z. number (), \n conditions: z. string (), \n }), \n execute: async ({ context: { city } }) =&gt; { \n console. log ( \" Using tool to fetch weather information for \", city); \n return await getWeatherInfo (city); \n }, \n}); \n This example defines a weatherTool with an input schema for the city, an output schema for the weather data, and an execute function that contains the tool’s logic. \n When creating tools, keep tool descriptions simple and focused on what the tool does and when to use it, emphasizing its primary use case. Technical details belong in the parameter schemas, guiding the agent on how to use the tool correctly with descriptive names, clear descriptions, and explanations of default values. \n Adding Tools to an Agent \n To make tools available to an agent, you configure them in the agent’s definition. Mentioning available tools and their general purpose in the agent’s system prompt can also improve tool usage. For detailed steps and examples, see the guide on Using Tools and MCP with Agents. \n Compatibility Layer for Tool Schemas \n Different models interpret schemas differely. Some error when certain schema properties are passed and some ignore certain schema properties but don’t throw an error. Mastra adds a compatibility layer for tool schemas, ensuring tools work consistently across different model providers and that the schema constraints are respected. \n Some providers that we include this layer for: \n \n Google Gemini &amp; Anthropic: Remove unsupported schema properties and append relevant constraints to the tool description. \n OpenAI (including reasoning models): Strip or adapt schema fields that are ignored or unsupported, and add instructions to the description for agent guidance. \n DeepSeek &amp; Meta: Apply similar compatibility logic to ensure schema alignment and tool usability. \n \n This approach makes tool usage more reliable and model-agnostic for both custom and MCP tools. Runtime Context MCP Overview",
            "image": "https://mastra.ai/api/og/docs?title=Tools%20Overview%20|%20Tools%20&%20MCP%20|%20Mastra%20Docs&description=Understand%20what%20tools%20are%20in%20Mastra,%20how%20to%20add%20them%20to%20agents,%20and%20best%20practices%20for%20designing%20effective%20tools.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/tools-mcp/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/tools-mcp/mcp-overview",
            "title": "MCP Overview | Tools & MCP | Mastra Docs",
            "url": "https://mastra.ai/docs/tools-mcp/mcp-overview",
            "publishedDate": "2000-01-01T00:00:00.000Z",
            "author": "",
            "text": "\n Model Context Protocol (MCP)  is an open standard designed to let AI models discover and interact with external tools and resources. Think of it as a universal plugin system for AI agents, allowing them to use tools regardless of the language they were written in or where they are hosted. \n Mastra uses MCP to connect agents to external tool servers. \n Use third-party tools with an MCP Client \n Mastra provides the MCPClient class to manage connections to one or more MCP servers and access their tools. \n Installation \n If you haven’t already, install the Mastra MCP package: \n npm install @mastra/mcp@latest \n Configuring MCPClient \n You configure MCPClient with a map of servers you want to connect to. It supports connections via subprocess (Stdio) or HTTP (Streamable HTTP with SSE fallback). \n import { MCPClient } from \" @mastra/mcp \"; \n \n const mcp = new MCPClient ({ \n servers: { \n // Stdio example \n sequential: { \n command: \" npx \", \n args: [ \" -y \", \" @modelcontextprotocol/server-sequential-thinking \"], \n }, \n // HTTP example \n weather: { \n url: new URL ( \" http://localhost:8080/mcp \"), \n requestInit: { \n headers: { \n Authorization: \" Bearer your-token \", \n }, \n }, \n }, \n }, \n}); \n For detailed configuration options, see the MCPClient reference documentation. \n Static vs Dynamic Tool Configurations \n MCPClient offers two approaches to retrieving tools from connected servers, suitable for different application architectures: \n Feature Static Configuration ( await mcp.getTools()) Dynamic Configuration ( await mcp.getToolsets()) Use Case Single-user, static config (e.g., CLI tool) Multi-user, dynamic config (e.g., SaaS app) Configuration Fixed at agent initialization Per-request, dynamic Credentials Shared across all uses Can vary per user/request Agent Setup Tools added in Agent constructor Tools passed in generate() or stream() options \n \n \n Static Configuration ( getTools()): Fetches all tools from all configured servers. Best when the tool configuration (like API keys) is static and shared across all users or requests. You typically call this once and pass the result to the tools property when defining your Agent.\n Reference: getTools() \n import { Agent } from \" @mastra/core/agent \"; \n // ... mcp client setup \n \n const agent = new Agent ({ \n // ... other agent config \n tools: await mcp. getTools (), \n}); \n \n \n Dynamic Configuration ( getToolsets()): Designed for scenarios where configuration might change per request or per user (e.g., different API keys for different tenants in a multi-user application). You pass the result of getToolsets() to the toolsets option in the agent’s generate() or stream() method.\n Reference: getToolsets() \n import { Agent } from \" @mastra/core/agent \"; \n // ... agent setup without tools initially \n \n async function handleRequest (userPrompt: string, userApiKey: string) { \n const userMcp = new MCPClient ({ /* config with userApiKey */ }); \n const toolsets = await userMcp. getToolsets (); \n \n const response = await agent. stream (userPrompt, { \n toolsets, // Pass dynamic toolsets \n }); \n // ... handle response \n await userMcp. disconnect (); \n} \n \n \n Connecting to an MCP registry \n MCP servers can be discovered through registries. Here’s how to connect to some popular ones using MCPClient: \n mcp.run mcp.run  provides pre-authenticated, managed MCP servers. Tools are grouped into Profiles, each with a unique, signed URL. import { MCPClient } from \" @mastra/mcp \"; \n \n const mcp = new MCPClient ({ \n servers: { \n marketing: { // Example profile name \n url: new URL (process.env.MCP_RUN_SSE_URL!), // Get URL from mcp.run profile \n }, \n }, \n}); \n Important: Treat the mcp.run SSE URL like a password. Store it securely, for example, in an environment variable. \n MCP_RUN_SSE_URL = https://www.mcp.run/api/mcp/sse? nonce =... \n \n If you have created your own Mastra tools, you can expose them to any MCP-compatible client using Mastra’s MCPServer class. \n This allows others to use your tools without needing direct access to your codebase. \n Using MCPServer \n You initialize MCPServer with a name, version, and the Mastra tools you want to share. \n import { MCPServer } from \" @mastra/mcp \"; \n import { weatherTool } from \"./tools \"; // Your Mastra tool \n \n const server = new MCPServer ({ \n name: \" My Weather Server \", \n version: \" 1.0.0 \", \n tools: { weatherTool }, // Provide your tool(s) here \n}); \n \n // Start the server (e.g., using stdio for a CLI tool) \n // await server.startStdio(); \n \n // Or integrate with an HTTP server using startSSE() \n // See MCPServer reference for details \n For detailed usage and examples, see the MCPServer reference documentation. Overview Dynamic Tool Context",
            "image": "https://mastra.ai/api/og/docs?title=MCP%20Overview%20|%20Tools%20&%20MCP%20|%20Mastra%20Docs&description=Learn%20about%20the%20Model%20Context%20Protocol%20(MCP),%20how%20to%20use%20third-party%20tools%20via%20MCPClient,%20connect%20to%20registries,%20and%20share%20your%20own%20tools%20using%20MCPServer.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/tools-mcp/mcp-overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/tools-mcp/dynamic-context",
            "title": "Dynamic Tool Context | Tools & MCP | Mastra Docs",
            "url": "https://mastra.ai/docs/tools-mcp/dynamic-context",
            "author": "",
            "text": "\n Mastra provides RuntimeContext, a system based on dependency injection, that allows you to pass dynamic, request-specific configuration to your tools during execution. This is useful when a tool’s behavior needs to change based on user identity, request headers, or other runtime factors, without altering the tool’s core code. \n 💡 Note: RuntimeContext is primarily used for passing data into tool executions. It’s distinct from agent memory, which handles conversation history and state persistence across multiple calls. \n Basic Usage \n To use RuntimeContext, first define a type structure for your dynamic configuration. Then, create an instance of RuntimeContext typed with your definition and set the desired values. Finally, include the runtimeContext instance in the options object when calling agent.generate() or agent.stream(). \n import { RuntimeContext } from \" @mastra/core/di \"; \n // Assume 'agent' is an already defined Mastra Agent instance \n \n // Define the context type \n type WeatherRuntimeContext = { \n \" temperature-scale \": \" celsius \" | \" fahrenheit \"; \n}; \n \n // Instantiate RuntimeContext and set values \n const runtimeContext = new RuntimeContext &lt; WeatherRuntimeContext &gt;(); \n runtimeContext. set ( \" temperature-scale \", \" celsius \"); \n \n // Pass to agent call \n const response = await agent. generate ( \" What's the weather like today? \", { \n runtimeContext, // Pass the context here \n}); \n \n console. log (response.text); \n Accessing Context in Tools \n Tools receive the runtimeContext as part of the second argument to their execute function. You can then use the.get() method to retrieve values. \n src/mastra/tools/weather-tool.ts import { createTool } from \" @mastra/core/tools \"; \n import { z } from \" zod \"; \n // Assume WeatherRuntimeContext is defined as above and accessible here \n \n // Dummy fetch function \n async function fetchWeather ( \n location: string, \n options: { temperatureUnit: \" celsius \" | \" fahrenheit \" } \n): Promise &lt; any &gt; { \n console. log ( ` Fetching weather for ${ location} in ${ options.temperatureUnit}`); \n // Replace with actual API call \n return { temperature: options.temperatureUnit === \" celsius \" ? 20 : 68 }; \n} \n \n export const weatherTool = createTool ({ \n id: \" getWeather \", \n description: \" Get the current weather for a location \", \n inputSchema: z. object ({ \n location: z. string (). describe ( \" The location to get weather for \"), \n }), \n // The tool's execute function receives runtimeContext \n execute: async ({ context, runtimeContext }) =&gt; { \n // Type-safe access to runtimeContext variables \n const temperatureUnit = runtimeContext. get ( \" temperature-scale \"); \n \n // Use the context value in the tool logic \n const weather = await fetchWeather (context.location, { \n temperatureUnit, \n }); \n \n return { result: ` The temperature is ${ weather.temperature} ° ${ temperatureUnit === \" celsius \" ? \" C \" : \" F \"}` }; \n }, \n}); \n When the agent uses weatherTool, the temperature-scale value set in the runtimeContext during the agent.generate() call will be available inside the tool’s execute function. \n Using with Server Middleware \n In server environments (like Express or Next.js), you can use middleware to automatically populate RuntimeContext based on incoming request data, such as headers or user sessions. \n Here’s an example using Mastra’s built-in server middleware support (which uses Hono internally) to set the temperature scale based on the Cloudflare CF-IPCountry header: \n import { Mastra } from \" @mastra/core \"; \n import { RuntimeContext } from \" @mastra/core/di \"; \n import { weatherAgent } from \"./agents/weather \"; // Assume agent is defined elsewhere \n \n // Define RuntimeContext type \n type WeatherRuntimeContext = { \n \" temperature-scale \": \" celsius \" | \" fahrenheit \"; \n}; \n \n export const mastra = new Mastra ({ \n agents: { \n weather: weatherAgent, \n }, \n server: { \n middleware: [ \n async (c, next) =&gt; { \n // Get the RuntimeContext instance \n const runtimeContext = c. get &lt; RuntimeContext &lt; WeatherRuntimeContext &gt;&gt;( \" runtimeContext \"); \n \n // Get country code from request header \n const country = c.req. header ( \" CF-IPCountry \"); \n \n // Set temperature scale based on country \n runtimeContext. set ( \n \" temperature-scale \", \n country === \" US \" ? \" fahrenheit \" : \" celsius \", \n ); \n \n // Continue request processing \n await next (); \n }, \n ], \n }, \n}); \n With this middleware in place, any agent call handled by this Mastra server instance will automatically have the temperature-scale set in its RuntimeContext based on the user’s inferred country, and tools like weatherTool will use it accordingly.",
            "image": "https://mastra.ai/api/og/docs?title=Dynamic%20Tool%20Context%20|%20Tools%20&%20MCP%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20Mastra%27s%20RuntimeContext%20to%20provide%20dynamic,%20request-specific%20configuration%20to%20tools.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/tools-mcp/dynamic-context",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/tools-mcp/advanced-usage",
            "title": "Advanced Tool Usage | Tools & MCP | Mastra Docs",
            "url": "https://mastra.ai/docs/tools-mcp/advanced-usage",
            "author": "",
            "text": "\n This page covers more advanced techniques and features related to using tools in Mastra. \n Abort Signals \n When you initiate an agent interaction using generate() or stream(), you can provide an AbortSignal. Mastra automatically forwards this signal to any tool executions that occur during that interaction. \n This allows you to cancel long-running operations within your tools, such as network requests or intensive computations, if the parent agent call is aborted. \n You access the abortSignal in the second parameter of the tool’s execute function. \n import { createTool } from \" @mastra/core/tools \"; \n import { z } from \" zod \"; \n \n export const longRunningTool = createTool ({ \n id: \" long-computation \", \n description: \" Performs a potentially long computation \", \n inputSchema: z. object ({ /* ... */ }), \n execute: async ({ context }, { abortSignal }) =&gt; { \n // Example: Forwarding signal to fetch \n const response = await fetch ( \" https://api.example.com/data \", { \n signal: abortSignal, // Pass the signal here \n }); \n \n if (abortSignal?.aborted) { \n console. log ( \" Tool execution aborted. \"); \n throw new Error ( \" Aborted \"); \n } \n \n // Example: Checking signal during a loop \n for ( let i = 0; i &lt; 1000000; i ++) { \n if (abortSignal?.aborted) { \n console. log ( \" Tool execution aborted during loop. \"); \n throw new Error ( \" Aborted \"); \n } \n // ... perform computation step ... \n } \n \n const data = await response. json (); \n return { result: data }; \n }, \\ n}); \n To use this, provide an AbortController ’s signal when calling the agent: \n import { Agent } from \" @mastra/core/agent \"; \n // Assume 'agent' is an Agent instance with longRunningTool configured \n \n const controller = new AbortController (); \n \n // Start the agent call \n const promise = agent. generate ( \" Perform the long computation. \", { \n abortSignal: controller.signal, \n}); \n \n // Sometime later, if needed: \n // controller.abort(); \n \n try { \n const result = await promise; \n console. log (result.text); \n} catch (error) { \n if (error.name === ' AbortError ') { \n console. log ( \" Agent generation was aborted. \"); \n } else { \n console. error ( \" An error occurred: \", error); \n } \n} \n AI SDK Tool Format \n Mastra maintains compatibility with the tool format used by the Vercel AI SDK ( ai package). You can define tools using the tool function from the ai package and use them directly within your Mastra agents alongside tools created with Mastra’s createTool. \n First, ensure you have the ai package installed: \n Here’s an example of a tool defined using the Vercel AI SDK format: \n src/mastra/tools/vercelWeatherTool.ts import { tool } from \" ai \"; \n import { z } from \" zod \"; \n \n export const vercelWeatherTool = tool ({ \n description: \" Fetches current weather using Vercel AI SDK format \", \n parameters: z. object ({ \n city: z. string (). describe ( \" The city to get weather for \"), \n }), \n execute: async ({ city }) =&gt; { \n console. log ( ` Fetching weather for ${ city} (Vercel format tool) `); \n // Replace with actual API call \n const data = await fetch ( ` https://api.example.com/weather?city= ${ city}`); \n return data. json (); \n }, \n}); \n You can then add this tool to your Mastra agent just like any other tool: \n src/mastra/agents/mixedToolsAgent.ts import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n import { vercelWeatherTool } from \"../tools/vercelWeatherTool \"; // Vercel AI SDK tool \n import { mastraTool } from \"../tools/mastraTool \"; // Mastra createTool tool \n \n export const mixedToolsAgent = new Agent ({ \n name: \" Mixed Tools Agent \", \n instructions: \n \" You can use tools defined in different formats. \", \n model: openai ( \" gpt-4o-mini \"), \n tools: { \n weatherVercel: vercelWeatherTool, \n someMastraTool: mastraTool, \n }, \n}); \n Mastra supports both tool formats, allowing you to mix and match as needed. Dynamic Tool Context Overview",
            "image": "https://mastra.ai/api/og/docs?title=Advanced%20Tool%20Usage%20|%20Tools%20&%20MCP%20|%20Mastra%20Docs&description=This%20page%20covers%20advanced%20features%20for%20Mastra%20tools,%20including%20abort%20signals%20and%20compatibility%20with%20the%20Vercel%20AI%20SDK%20tool%20format.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/tools-mcp/advanced-usage",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/memory/overview",
            "title": "Memory overview",
            "url": "https://mastra.ai/docs/memory/overview",
            "author": "",
            "text": "\n Memory is how agents manage the context that’s available to them, it’s a condensation of all chat messages into their context window. \n The Context Window \n The context window is the total information visible to the language model at any given time. \n In Mastra, context is broken up into three parts: system instructions and information about the user ( working memory), recent messages ( message history), and older messages that are relevant to the user’s query ( semantic recall). \n In addition, we provide memory processors to trim context or remove information if the context is too long. \n Quick Start \n The fastest way to see memory in action is using the built-in development playground. \n If you haven’t already, create a new Mastra project following the main Getting Started guide. \n 1. Install the memory package: \n npm install @mastra/memory@latest \n 2. Create an agent and attach a Memory instance: \n src/mastra/agents/index.ts import { Agent } from \" @mastra/core/agent \"; \n import { Memory } from \" @mastra/memory \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n export const myMemoryAgent = new Agent ({ \n name: \" MemoryAgent \", \n instructions: \"... \", \n model: openai ( \" gpt-4o \"), \n \n memory: new Memory (), \n}); \n 3. Start the Development Server: \n 4. Open the playground ( http://localhost:4111 ) and select your MemoryAgent: \n Send a few messages and notice that it remembers information across turns: \n ➡️ You: My favorite color is blue. \n ⬅️ Agent: Got it! I'll remember that your favorite color is blue. \n ➡️ You: What is my favorite color? \n ⬅️ Agent: Your favorite color is blue. \n Memory Threads \n Mastra organizes memory into threads, which are records that identify specific conversation histories, using two identifiers: \n \n threadId: A specific conversation id (e.g., support_123). \n resourceId: The user or entity id that owns each thread (e.g., user_123, org_456). \n \n const response = await myMemoryAgent. stream ( \" Hello, my name is Alice. \", { \n resourceId: \" user_alice \", \n threadId: \" conversation_123 \", \n}); \n Important: without these ID’s your agent will not use memory, even if memory is properly configured. The playground handles this for you, but you need to add ID’s yourself when using memory in your application. \n Conversation History \n By default, the Memory instance includes the last 40 messages from the current Memory thread in each new request. This provides the agent with immediate conversational context. \n const memory = new Memory ({ \n options: { \n lastMessages: 10, \n }, \n}); \n Important: Only send the newest user message in each agent call. Mastra handles retrieving and injecting the necessary history. Sending the full history yourself will cause duplication. See the AI SDK Memory Example for how to handle this with when using the useChat frontend hooks. \n Storage Configuration \n Conversation history relies on a storage adapter to store messages.\nBy default it uses the same storage provided to the main Mastra instance  \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { LibSQLStore } from \" @mastra/libsql \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n storage: new LibSQLStore ({ \n url: \" file:./local.db \", \n }), \n }), \n}); \n Storage code Examples: \n \n LibSQL \n Postgres \n Upstash \n \n Next Steps \n Now that you understand the core concepts, continue to semantic recall to learn how to add RAG memory to your Mastra agents. \n Alternatively you can visit the configuration reference for available options, or browse usage examples.",
            "image": "https://mastra.ai/api/og/docs?title=Memory%20overview&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/memory/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/memory/semantic-recall",
            "title": "Semantic Recall",
            "url": "https://mastra.ai/docs/memory/semantic-recall",
            "author": "",
            "text": "\n If you ask your friend what they did last weekend, they will search in their memory for events associated with “last weekend” and then tell you what they did. That’s sort of like how semantic recall works in Mastra. \n How Semantic Recall Works \n Semantic recall is RAG-based search that helps agents maintain context across longer interactions when messages are no longer within recent conversation history. \n It uses vector embeddings of messages for similarity search, integrates with various vector stores, and has configurable context windows around retrieved messages. \n \n When it’s enabled, new messages are used to query a vector DB for semantically similar messages. \n After getting a response from the LLM, all new messages (user, assistant, and tool calls/results) are inserted into the vector DB to be recalled in later interactions. \n Quick Start \n Semantic recall is enabled by default, so if you give your agent memory it will be included: \n import { Agent } from \" @mastra/core/agent \"; \n import { Memory } from \" @mastra/memory \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n name: \" SupportAgent \", \n instructions: \" You are a helpful support agent. \", \n model: openai ( \" gpt-4o \"), \n memory: new Memory (), \n}); \n Recall configuration \n The two main parameters that control semantic recall behavior are: \n \n topK: How many semantically similar messages to retrieve \n messageRange: How much surrounding context to include with each match \n \n const agent = new Agent ({ \n memory: new Memory ({ \n options: { \n semanticRecall: { \n topK: 3, // Retrieve 3 most similar messages \n messageRange: 2, // Include 2 messages before and after each match \n }, \n }, \n }), \n}); \n Storage configuration \n Semantic recall relies on a storage and vector db to store messages and their embeddings. \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { LibSQLStore, LibSQLVector } from \" @mastra/libsql \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // this is the default storage db if omitted \n storage: new LibSQLStore ({ \n url: \" file:./local.db \", \n }), \n // this is the default vector db if omitted \n vector: new LibSQLVector ({ \n connectionUrl: \" file:./local.db \", \n }), \n }), \n}); \n Storage/vector code Examples: \n \n LibSQL \n Postgres \n Upstash \n \n Embedder configuration \n Semantic recall relies on an embedding model to convert messages into embeddings. You can specify any embedding model  compatible with the AI SDK. \n To use FastEmbed (a local embedding model), install @mastra/fastembed: \n npm install @mastra/fastembed \n Then configure it in your memory: \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { fastembed } from \" @mastra/fastembed \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // ... other memory options \n embedder: fastembed, \n }), \n}); \n Alternatively, use a different provider like OpenAI: \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // ... other memory options \n embedder: openai. embedding ( \" text-embedding-3-small \"), \n }), \n}); \n Disabling \n There is a performance impact to using semantic recall. New messages are converted into embeddings and used to query a vector database before new messages are sent to the LLM. \n Semantic recall is enabled by default but can be disabled when not needed: \n const agent = new Agent ({ \n memory: new Memory ({ \n options: { \n semanticRecall: false, \n }, \n }), \n}); \n You might want to disable semantic recall in scenarios like: \n \n When conversation history provide sufficient context for the current conversation. \n In performance-sensitive applications, like realtime two-way audio, where the added latency of creating embeddings and running vector queries is noticeable. \n",
            "image": "https://mastra.ai/api/og/docs?title=Semantic%20Recall&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/memory/semantic-recall",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/memory/working-memory",
            "title": "Working Memory",
            "url": "https://mastra.ai/docs/memory/working-memory",
            "author": "",
            "text": "\n While conversation history and semantic recall help agents remember conversations, working memory allows them to maintain persistent information about users across interactions within a thread. \n Think of it as the agent’s active thoughts or scratchpad – the key information they keep available about the user or task. It’s similar to how a person would naturally remember someone’s name, preferences, or important details during a conversation. \n This is useful for maintaining ongoing state that’s always relevant and should always be available to the agent. \n Quick Start \n Here’s a minimal example of setting up an agent with working memory: \n import { Agent } from \" @mastra/core/agent \"; \n import { Memory } from \" @mastra/memory \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n // Create agent with working memory enabled \n const agent = new Agent ({ \n name: \" PersonalAssistant \", \n instructions: \" You are a helpful personal assistant. \", \n model: openai ( \" gpt-4o \"), \n memory: new Memory ({ \n options: { \n workingMemory: { \n enabled: true, \n use: \" tool-call \", // Will be the only option in a future breaking change release \n }, \n }, \n }), \n}); \n How it Works \n Working memory is a block of Markdown text that the agent is able to update over time to store continuously relevant information: \n \n Custom Templates \n Templates guide the agent on what information to track and update in working memory. While a default template is used if none is provided, you’ll typically want to define a custom template tailored to your agent’s specific use case to ensure it remembers the most relevant information. \n Here’s an example of a custom template. In this example the agent will store the users name, location, timezone, etc as soon as the user sends a message containing any of the info: \n const memory = new Memory ({ \n options: { \n workingMemory: { \n enabled: true, \n template: ` \n # User Profile \n \n ## Personal Info \n \n - Name: \n - Location: \n - Timezone: \n \n ## Preferences \n \n - Communication Style: [e.g., Formal, Casual] \n - Project Goal: \n - Key Deadlines: \n - [Deadline 1]: [Date] \n - [Deadline 2]: [Date] \n \n ## Session State \n \n - Last Task Discussed: \n - Open Questions: \n - [Question 1] \n - [Question 2] \n `, \n }, \n }, \n}); \n If your agent is not properly updating working memory when you expect it to, you can add system instructions on how and when to use this template in your agents instruction setting. \n Examples \n \n Streaming working memory \n Using a working memory template \n Semantic Recall Memory Processors",
            "image": "https://mastra.ai/api/og/docs?title=Working%20Memory&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/memory/working-memory",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/memory/memory-processors",
            "title": "Memory Processors",
            "url": "https://mastra.ai/docs/memory/memory-processors",
            "author": "",
            "text": "\n Memory Processors allow you to modify the list of messages retrieved from memory before they are added to the agent’s context window and sent to the LLM. This is useful for managing context size, filtering content, and optimizing performance. \n Processors operate on the messages retrieved based on your memory configuration (e.g., lastMessages, semanticRecall). They do not affect the new incoming user message. \n Built-in Processors \n Mastra provides built-in processors: \n TokenLimiter \n This processor is used to prevent errors caused by exceeding the LLM’s context window limit. It counts the tokens in the retrieved memory messages and removes the oldest messages until the total count is below the specified limit. \n import { Memory } from \" @mastra/memory \"; \n import { TokenLimiter } from \" @mastra/memory/processors \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n model: openai ( \" gpt-4o \"), \n memory: new Memory ({ \n processors: [ \n // Ensure the total tokens from memory don't exceed ~127k \n new TokenLimiter ( 127000), \n ], \n }), \n}); \n The TokenLimiter uses the o200k_base encoding by default (suitable for GPT-4o). You can specify other encodings if needed for different models: \n // Import the encoding you need (e.g., for older OpenAI models) \n import cl100k_base from \" js-tiktoken/ranks/cl100k_base \"; \n \n const memoryForOlderModel = new Memory ({ \n processors: [ \n new TokenLimiter ({ \n limit: 16000, // Example limit for a 16k context model \n encoding: cl100k_base, \n }), \n ], \n}); \n See the OpenAI cookbook  or js-tiktoken repo for more on encodings. \n ToolCallFilter \n This processor removes tool calls from the memory messages sent to the LLM. It saves tokens by excluding potentially verbose tool interactions from the context, which is useful if the details aren’t needed for future interactions. It’s also useful if you always want your agent to call a specific tool again and not rely on previous tool results in memory. \n import { Memory } from \" @mastra/memory \"; \n import { ToolCallFilter, TokenLimiter } from \" @mastra/memory/processors \"; \n \n const memoryFilteringTools = new Memory ({ \n processors: [ \n // Example 1: Remove all tool calls/results \n new ToolCallFilter (), \n \n // Example 2: Remove only noisy image generation tool calls/results \n new ToolCallFilter ({ exclude: [ \" generateImageTool \"] }), \n \n // Always place TokenLimiter last \n new TokenLimiter ( 127000), \n ], \n}); \n Applying Multiple Processors \n You can chain multiple processors. They execute in the order they appear in the processors array. The output of one processor becomes the input for the next. \n Order matters! It’s generally best practice to place TokenLimiter last in the chain. This ensures it operates on the final set of messages after other filtering has occurred, providing the most accurate token limit enforcement. \n import { Memory } from \" @mastra/memory \"; \n import { ToolCallFilter, TokenLimiter } from \" @mastra/memory/processors \"; \n // Assume a hypothetical 'PIIFilter' custom processor exists \n // import { PIIFilter } from './custom-processors'; \n \n const memoryWithMultipleProcessors = new Memory ({ \n processors: [ \n // 1. Filter specific tool calls first \n new ToolCallFilter ({ exclude: [ \" verboseDebugTool \"] }), \n // 2. Apply custom filtering (e.g., remove hypothetical PII - use with caution) \n // new PIIFilter(), \n // 3. Apply token limiting as the final step \n new TokenLimiter ( 127000), \n ], \n}); \n Creating Custom Processors \n You can create custom logic by extending the base MemoryProcessor class. \n import { Memory, CoreMessage } from \" @mastra/memory \"; \n import { MemoryProcessor, MemoryProcessorOpts } from \" @mastra/core/memory \"; \n \n class ConversationOnlyFilter extends MemoryProcessor { \n constructor () { \n // Provide a name for easier debugging if needed \n super({ name: \" ConversationOnlyFilter \" }); \n } \n \n process ( \n messages: CoreMessage [], \n _opts: MemoryProcessorOpts = {}, // Options passed during memory retrieval, rarely needed here \n ): CoreMessage [] { \n // Filter messages based on role \n return messages. filter ( \n (msg) =&gt; msg.role === \" user \" || msg.role === \" assistant \", \n ); \n } \n} \n \n // Use the custom processor \n const memoryWithCustomFilter = new Memory ({ \n processors: [ \n new ConversationOnlyFilter (), \n new TokenLimiter ( 127000), // Still apply token limiting \n ], \n}); \n When creating custom processors avoid mutating the input messages array or its objects directly.",
            "image": "https://mastra.ai/api/og/docs?title=Memory%20Processors&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/memory/memory-processors",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/overview",
            "title": "Handling Complex LLM Operations | Workflows | Mastra",
            "url": "https://mastra.ai/docs/workflows/overview",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "\n Workflows in Mastra help you orchestrate complex sequences of operations with features like branching, parallel execution, resource suspension, and more. \n When to use workflows \n Most AI applications need more than a single call to a language model. You may want to run multiple steps, conditionally skip certain paths, or even pause execution altogether until you receive user input. Sometimes your agent tool calling is not accurate enough. \n Mastra’s workflow system provides: \n \n A standardized way to define steps and link them together. \n Support for both simple (linear) and advanced (branching, parallel) paths. \n Debugging and observability features to track each workflow run. \n \n Example \n To create a workflow, you define one or more steps, link them, and then commit the workflow before starting it. \n Breaking Down the Workflow \n Let’s examine each part of the workflow creation process: \n 1. Creating the Workflow \n Here’s how you define a workflow in Mastra. The name field determines the workflow’s API endpoint ( /workflows/$NAME/), while the triggerSchema defines the structure of the workflow’s trigger data: \n src/mastra/workflow/index.ts const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n 2. Defining Steps \n Now, we’ll define the workflow’s steps. Each step can have its own input and output schemas. Here, stepOne doubles an input value, and stepTwo increments that result if stepOne was successful. (To keep things simple, we aren’t making any LLM calls in this example): \n src/mastra/workflow/index.ts const stepOne = new Step ({ \n id: \" stepOne \", \n outputSchema: z. object ({ \n doubledValue: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const doubledValue = context.triggerData.inputValue * 2; \n return { doubledValue }; \n }, \n}); \n \n const stepTwo = new Step ({ \n id: \" stepTwo \", \n execute: async ({ context }) =&gt; { \n const doubledValue = context. getStepResult (stepOne)?.doubledValue; \n if (! doubledValue) { \n return { incrementedValue: 0 }; \n } \n return { \n incrementedValue: doubledValue + 1, \n }; \n }, \n}); \n 3. Linking Steps \n Now, let’s create the control flow, and “commit” (finalize the workflow). In this case, stepOne runs first and is followed by stepTwo. \n src/mastra/workflow/index.ts myWorkflow. step (stepOne). then (stepTwo). commit (); \n Register the Workflow \n Register your workflow with Mastra to enable logging and telemetry: \n import { Mastra } from \" @mastra/core \"; \n \n export const mastra = new Mastra ({ \n workflows: { myWorkflow }, \n}); \n The workflow can also have the mastra instance injected into the context in the case where you need to create dynamic workflows: \n src/mastra/workflow/index.ts import { Mastra } from \" @mastra/core \"; \n \n const mastra = new Mastra (); \n \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n mastra, \n}); \n Executing the Workflow \n Execute your workflow programmatically or via API: \n src/mastra/run-workflow.ts import { mastra } from \"./index \"; \n \n // Get the workflow \n const myWorkflow = mastra. getWorkflow ( \" myWorkflow \"); \n const { runId, start } = myWorkflow. createRun (); \n \n // Start the workflow execution \n await start ({ triggerData: { inputValue: 45 } }); \n Or use the API (requires running mastra dev): \n // Create workflow run \n curl --location ' http://localhost:4111/api/workflows/myWorkflow/start-async ' \\ \n --header ' Content-Type: application/json ' \\ \n --data ' { \n \"inputValue\": 45 \n } ' \n This example shows the essentials: define your workflow, add steps, commit the workflow, then execute it. \n Defining Steps \n The basic building block of a workflow is a step. Steps are defined using schemas for inputs and outputs, and can fetch prior step results. \n Control Flow \n Workflows let you define a control flow to chain steps together in with parallel steps, branching paths, and more. \n Workflow Variables \n When you need to map data between steps or create dynamic data flows, workflow variables provide a powerful mechanism for passing information from one step to another and accessing nested properties within step outputs. \n Suspend and Resume \n When you need to pause execution for external data, user input, or asynchronous events, Mastra supports suspension at any step, persisting the state of the workflow so you can resume it later. \n Observability and Debugging \n Mastra workflows automatically log the input and output of each step within a workflow run, allowing you to send this data to your preferred logging, telemetry, or observability tools. \n You can: \n \n Track the status of each step (e.g., success, error, or suspended). \n Store run-specific metadata for analysis. \n Integrate with third-party observability platforms like Datadog or New Relic by forwarding logs. \n \n \n ## More Resources \n \n - The [Workflow Guide](../guides/ai-recruiter.mdx) in the Guides section is a tutorial that covers the main concepts. \n - [Sequential Steps workflow example](../../examples/workflows/sequential-steps.mdx) \n - [Parallel Steps workflow example](../../examples/workflows/parallel-steps.mdx) \n - [Branching Paths workflow example](../../examples/workflows/branching-paths.mdx) \n - [Workflow Variables example](../../examples/workflows/workflow-variables.mdx) \n - [Cyclical Dependencies workflow example](../../examples/workflows/cyclical-dependencies.mdx) \n - [Suspend and Resume workflow example](../../examples/workflows/suspend-and-resume.mdx)",
            "image": "https://mastra.ai/api/og/docs?title=Handling%20Complex%20LLM%20Operations%20|%20Workflows%20|%20Mastra&description=Workflows%20in%20Mastra%20help%20you%20orchestrate%20complex%20sequences%20of%20operations%20with%20features%20like%20branching,%20parallel%20execution,%20resource%20suspension,%20and%20more.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/steps",
            "title": "Creating Steps and Adding to Workflows | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/steps",
            "author": "",
            "text": "Defining Steps in a Workflow \n When you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. Steps provide a structured way to manage these tasks by defining inputs, outputs, and execution logic. \n The code below shows how to define these steps inline or separately. \n Inline Step Creation \n You can create steps directly within your workflow using and . This code shows how to define, link, and execute two steps in sequence. \n Creating Steps Separately \n If you prefer to manage your step logic in separate entities, you can define steps outside and then add them to your workflow. This code shows how to define steps independently and link them afterward. \n Overview Control Flow",
            "image": "https://mastra.ai/api/og/docs?title=Creating%20Steps%20and%20Adding%20to%20Workflows%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/control-flow",
            "title": "Branching, Merging, Conditions | Workflows | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/control-flow",
            "author": "",
            "text": "Control Flow in Workflows: Branching, Merging, and Conditions \n When you create a multi-step process, you may need to run steps in parallel, chain them sequentially, or follow different paths based on outcomes. This page describes how you can manage branching, merging, and conditions to construct workflows that meet your logic requirements. The code snippets show the key patterns for structuring complex control flow. \n Parallel Execution \n You can run multiple steps at the same time if they don’t depend on each other. This approach can speed up your workflow when steps perform independent tasks. The code below shows how to add two steps in parallel: \n See the Parallel Steps example for more details. \n Sequential Execution \n Sometimes you need to run steps in strict order to ensure outputs from one step become inputs for the next. Use .then() to link dependent operations. The code below shows how to chain steps sequentially: \n See the Sequential Steps example for more details. \n Branching and Merging Paths \n When different outcomes require different paths, branching is helpful. You can also merge paths later once they complete. The code below shows how to branch after stepA and later converge on stepF: \n In this example: \n \n stepA leads to stepB, then to stepD. \n Separately, stepA also triggers stepC, which in turn leads to stepE. \n The workflow waits for both stepD and stepE to finish before proceeding to stepF. \n \n See the Branching Paths example for more details. \n Cyclical Dependencies \n You can loop back to earlier steps based on conditions, allowing you to repeat tasks until certain results are achieved. The code below shows a workflow that repeats fetchData when a status is “retry”: \n If processData returns “success,” finalizeData runs. If it returns “retry,” the workflow loops back to fetchData. \n See the Cyclical Dependencies example for more details. \n Use the when property to control whether a step runs based on data from previous steps. Below are three ways to specify conditions. \n Option 1: Function \n Option 2: Query Object \n Option 3: Simple Path Comparison \n Accessing Previous Step Results \n Steps access data from previous steps through the object. The context contains a record of all step results and their payloads. \n Using getStepPayload \n retrieves a step’s output with type safety: \n Using Path Notation \n Path notation accesses step results through the machine context. For example, to access the status of the step: \n The context object maintains type information when used with TypeScript. Nested objects in step outputs can be accessed with either method. Steps Suspend &amp; Resume",
            "image": "https://mastra.ai/api/og/docs?title=Branching%2C%20Merging%2C%20Conditions%20%7C%20Workflows%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/variables",
            "title": "Data Mapping with Workflow Variables | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/variables",
            "author": "",
            "text": "\n Workflow variables in Mastra provide a powerful mechanism for mapping data between steps, allowing you to create dynamic data flows and pass information from one step to another. \n Understanding Workflow Variables \n In Mastra workflows, variables serve as a way to: \n \n Map data from trigger inputs to step inputs \n Pass outputs from one step to inputs of another step \n Access nested properties within step outputs \n Create more flexible and reusable workflow steps \n \n Using Variables for Data Mapping \n Basic Variable Mapping \n You can map data between steps using the variables property when adding a step to your workflow: \n src/mastra/workflows/index.ts const workflow = new Workflow ({ \n name: ' data-mapping-workflow ', \n triggerSchema: z. object ({ \n inputData: z. string (), \n }), \n}); \n \n workflow \n . step (step1, { \n variables: { \n // Map trigger data to step input \n inputData: { step: ' trigger ', path: ' inputData ' } \n } \n }) \n . then (step2, { \n variables: { \n // Map output from step1 to input for step2 \n previousValue: { step: step1, path: ' outputField ' } \n } \n }) \n . commit (); \n \n // Register the workflow with Mastra \n export const mastra = new Mastra ({ \n workflows: { workflow }, \n }); \n Accessing Nested Properties \n You can access nested properties using dot notation in the path field: \n src/mastra/workflows/index.ts workflow \n . step (step1) \n . then (step2, { \n variables: { \n // Access a nested property from step1's output \n nestedValue: { step: step1, path: ' nested.deeply.value ' } \n } \n }) \n . commit (); \n Mapping Entire Objects \n You can map an entire object by using. as the path: \n src/mastra/workflows/index.ts workflow \n . step (step1, { \n variables: { \n // Map the entire trigger data object \n triggerData: { step: ' trigger ', path: '. ' } \n } \n }) \n . commit (); \n Variables in Loops \n Variables can also be passed to while and until loops. This is useful for passing data between iterations or from outside steps: \n src/mastra/workflows/loop-variables.ts // Step that increments a counter \n const incrementStep = new Step ({ \n id: ' increment ', \n inputSchema: z. object ({ \n // Previous value from last iteration \n prevValue: z. number (). optional (), \n }), \n outputSchema: z. object ({ \n // Updated counter value \n updatedCounter: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const { prevValue = 0 } = context.inputData; \n return { updatedCounter: prevValue + 1 }; \n }, \n}); \n \n const workflow = new Workflow ({ \n name: ' counter ' \n}); \n \n workflow \n . step (incrementStep) \n . while ( \n async ({ context }) =&gt; { \n // Continue while counter is less than 10 \n const result = context. getStepResult (incrementStep); \n return (result?.updatedCounter ?? 0) &lt; 10; \n }, \n incrementStep, \n { \n // Pass previous value to next iteration \n prevValue: { \n step: incrementStep, \n path: ' updatedCounter ' \n } \n } \n ); \n Variable Resolution \n When a workflow executes, Mastra resolves variables at runtime by: \n \n Identifying the source step specified in the step property \n Retrieving the output from that step \n Navigating to the specified property using the path \n Injecting the resolved value into the target step’s context as the inputData property \n \n Examples \n Mapping from Trigger Data \n This example shows how to map data from the workflow trigger to a step: \n src/mastra/workflows/trigger-mapping.ts import { Step, Workflow, Mastra } from \" @mastra/core \"; \n import { z } from \" zod \"; \n \n // Define a step that needs user input \n const processUserInput = new Step ({ \n id: \" processUserInput \", \n execute: async ({ context }) =&gt; { \n // The inputData will be available in context because of the variable mapping \n const { inputData } = context.inputData; \n \n return { \n processedData: ` Processed: ${ inputData}` \n }; \n }, \n}); \n \n // Create the workflow \n const workflow = new Workflow ({ \n name: \" trigger-mapping \", \n triggerSchema: z. object ({ \n inputData: z. string (), \n }), \n}); \n \n // Map the trigger data to the step \n workflow \n . step (processUserInput, { \n variables: { \n inputData: { step: ' trigger ', path: ' inputData ' }, \n } \n }) \n . commit (); \n \n // Register the workflow with Mastra \n export const mastra = new Mastra ({ \n workflows: { workflow }, \n }); \n Mapping Between Steps \n This example demonstrates mapping data from one step to another: \n src/mastra/workflows/step-mapping.ts import { Step, Workflow, Mastra } from \" @mastra/core \"; \n import { z } from \" zod \"; \n \n // Step 1: Generate data \n const generateData = new Step ({ \n id: \" generateData \", \n outputSchema: z. object ({ \n nested: z. object ({ \n value: z. string (), \n }), \n }), \n execute: async () =&gt; { \n return { \n nested: { \n value: \" step1-data \" \n } \n }; \n }, \n}); \n \n // Step 2: Process the data from step 1 \n const processData = new Step ({ \n id: \" processData \", \n inputSchema: z. object ({ \n previousValue: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n // previousValue will be available because of the variable mapping \n const { previousValue } = context.inputData; \n \n return { \n result: ` Processed: ${ previousValue}` \n }; \n }, \n}); \n \n // Create the workflow \n const workflow = new Workflow ({ \n name: \" step-mapping \", \n}); \n \n // Map data from step1 to step2 \n workflow \n . step (generateData) \n . then (processData, { \n variables: { \n // Map the nested.value property from generateData's output \n previousValue: { step: generateData, path: ' nested.value ' }, \n } \n }) \n . commit (); \n \n // Register the workflow with Mastra \n export const mastra = new Mastra ({ \n workflows: { workflow }, \n }); \n Type Safety \n Mastra provides type safety for variable mappings when using TypeScript: \n src/mastra/workflows/type-safe.ts import { Step, Workflow, Mastra } from \" @mastra/core \"; \n import { z } from \" zod \"; \n \n // Define schemas for better type safety \n const triggerSchema = z. object ({ \n inputValue: z. string (), \n}); \n \n type TriggerType = z. infer &lt; typeof triggerSchema&gt;; \n \n // Step with typed context \n const step1 = new Step ({ \n id: \" step1 \", \n outputSchema: z. object ({ \n nested: z. object ({ \n value: z. string (), \n }), \n }), \n execute: async ({ context }) =&gt; { \n // TypeScript knows the shape of triggerData \n const triggerData = context. getStepResult &lt; TriggerType &gt;( ' trigger '); \n \n return { \n nested: { \n value: ` processed- ${ triggerData?.inputValue}` \n } \n }; \n }, \n}); \n \n // Create the workflow with the schema \n const workflow = new Workflow ({ \n name: \" type-safe-workflow \", \n triggerSchema, \n}); \n \n workflow. step (step1). commit (); \n \n // Register the workflow with Mastra \n export const mastra = new Mastra ({ \n workflows: { workflow }, \n }); \n Best Practices \n \n \n Validate Inputs and Outputs: Use inputSchema and outputSchema to ensure data consistency. \n \n \n Keep Mappings Simple: Avoid overly complex nested paths when possible. \n \n \n Consider Default Values: Handle cases where mapped data might be undefined. \n \n \n Comparison with Direct Context Access \n While you can access previous step results directly via context.steps, using variable mappings offers several advantages: \n Feature Variable Mapping Direct Context Access Clarity Explicit data dependencies Implicit dependencies Reusability Steps can be reused with different mappings Steps are tightly coupled Type Safety Better TypeScript integration Requires manual type assertions",
            "image": "https://mastra.ai/api/og/docs?title=Data%20Mapping%20with%20Workflow%20Variables%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20workflow%20variables%20to%20map%20data%20between%20steps%20and%20create%20dynamic%20data%20flows%20in%20your%20Mastra%20workflows.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/variables",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/suspend-and-resume",
            "title": "Suspend & Resume Workflows | Human-in-the-Loop | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/suspend-and-resume",
            "author": "",
            "text": "\n Complex workflows often need to pause execution while waiting for external input or resources. \n Mastra’s suspend and resume features let you pause workflow execution at any step, persist the workflow state, and continue when ready. \n When to Use Suspend/Resume \n Common scenarios for suspending workflows include: \n \n Waiting for human approval or input \n Pausing until external API resources become available \n Collecting additional data needed for later steps \n Rate limiting or throttling expensive operations \n \n Basic Suspend Example \n Here’s a simple workflow that suspends when a value is too low and resumes when given a higher value: \n Watching and Resuming \n To handle suspended workflows, use the method to monitor workflow status and to continue execution: \n \n See the Suspend and Resume Example for a complete working example \n Check the Step Class Reference for suspend/resume API details \n Review Workflow Observability for monitoring suspended workflows \n Control Flow Overview",
            "image": "https://mastra.ai/api/og/docs?title=Suspend%20%26%20Resume%20Workflows%20%7C%20Human-in-the-Loop%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/dynamic-workflows",
            "title": "Dynamic Workflows | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/dynamic-workflows",
            "author": "",
            "text": "\n This guide demonstrates how to create dynamic workflows within a workflow step. This advanced pattern allows you to create and execute workflows on the fly based on runtime conditions. \n Overview \n Dynamic workflows are useful when you need to create workflows based on runtime data. \n Implementation \n The key to creating dynamic workflows is accessing the Mastra instance from within a step’s execute function and using it to create and run a new workflow. \n Basic Example \n import { Mastra, Step, Workflow } from ' @mastra/core '; \n import { z } from ' zod '; \n \n const isMastra = (mastra: any): mastra is Mastra =&gt; { \n return mastra &amp;&amp; typeof mastra === ' object ' &amp;&amp; mastra instanceof Mastra; \n}; \n \n // Step that creates and runs a dynamic workflow \n const createDynamicWorkflow = new Step ({ \n id: ' createDynamicWorkflow ', \n outputSchema: z. object ({ \n dynamicWorkflowResult: z. any (), \n }), \n execute: async ({ context, mastra }) =&gt; { \n if (! mastra) { \n throw new Error ( ' Mastra instance not available '); \n } \n \n if (! isMastra (mastra)) { \n throw new Error ( ' Invalid Mastra instance '); \n } \n \n const inputData = context.triggerData.inputData; \n \n // Create a new dynamic workflow \n const dynamicWorkflow = new Workflow ({ \n name: ' dynamic-workflow ', \n mastra, // Pass the mastra instance to the new workflow \n triggerSchema: z. object ({ \n dynamicInput: z. string (), \n }), \n }); \n \n // Define steps for the dynamic workflow \n const dynamicStep = new Step ({ \n id: ' dynamicStep ', \n execute: async ({ context }) =&gt; { \n const dynamicInput = context.triggerData.dynamicInput; \n return { \n processedValue: ` Processed: ${ dynamicInput}`, \n }; \n }, \n }); \n \n // Build and commit the dynamic workflow \n dynamicWorkflow. step (dynamicStep). commit (); \n \n // Create a run and execute the dynamic workflow \n const run = dynamicWorkflow. createRun (); \n const result = await run. start ({ \n triggerData: { \n dynamicInput: inputData, \n }, \n }); \n \n let dynamicWorkflowResult; \n \n if (result.results[ ' dynamicStep ']?.status === ' success ') { \n dynamicWorkflowResult = result.results[ ' dynamicStep ']?.output.processedValue; \n } else { \n throw new Error ( ' Dynamic workflow failed '); \n } \n \n // Return the result from the dynamic workflow \n return { \n dynamicWorkflowResult, \n }; \n }, \n}); \n \n // Main workflow that uses the dynamic workflow creator \n const mainWorkflow = new Workflow ({ \n name: ' main-workflow ', \n triggerSchema: z. object ({ \n inputData: z. string (), \n }), \n mastra: new Mastra (), \n}); \n \n mainWorkflow. step (createDynamicWorkflow). commit (); \n \n // Register the workflow with Mastra \n export const mastra = new Mastra ({ \n workflows: { mainWorkflow }, \n}); \n \n const run = mainWorkflow. createRun (); \n const result = await run. start ({ \n triggerData: { \n inputData: ' test ', \n }, \n}); \n Advanced Example: Workflow Factory \n You can create a workflow factory that generates different workflows based on input parameters: \n \n const isMastra = (mastra: any): mastra is Mastra =&gt; { \n return mastra &amp;&amp; typeof mastra === ' object ' &amp;&amp; mastra instanceof Mastra; \n}; \n \n const workflowFactory = new Step ({ \n id: ' workflowFactory ', \n inputSchema: z. object ({ \n workflowType: z. enum ([ ' simple ', ' complex ']), \n inputData: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. any (), \n }), \n execute: async ({ context, mastra }) =&gt; { \n if (! mastra) { \n throw new Error ( ' Mastra instance not available '); \n } \n \n if (! isMastra (mastra)) { \n throw new Error ( ' Invalid Mastra instance '); \n } \n \n // Create a new dynamic workflow based on the type \n const dynamicWorkflow = new Workflow ({ \n name: ` dynamic- ${ context.workflowType} -workflow `, \n mastra, \n triggerSchema: z. object ({ \n input: z. string (), \n }), \n }); \n \n if (context.workflowType === ' simple ') { \n // Simple workflow with a single step \n const simpleStep = new Step ({ \n id: ' simpleStep ', \n execute: async ({ context }) =&gt; { \n return { \n result: ` Simple processing: ${ context.triggerData.input}`, \n }; \n }, \n }); \n \n dynamicWorkflow. step (simpleStep). commit (); \n } else { \n // Complex workflow with multiple steps \n const step1 = new Step ({ \n id: ' step1 ', \n outputSchema: z. object ({ \n intermediateResult: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n return { \n intermediateResult: ` First processing: ${ context.triggerData.input}`, \n }; \n }, \n }); \n \n const step2 = new Step ({ \n id: ' step2 ', \n execute: async ({ context }) =&gt; { \n const intermediate = context. getStepResult (step1).intermediateResult; \n return { \n finalResult: ` Second processing: ${ intermediate}`, \n }; \n }, \n }); \n \n dynamicWorkflow. step (step1). then (step2). commit (); \n } \n \n // Execute the dynamic workflow \n const run = dynamicWorkflow. createRun (); \n const result = await run. start ({ \n triggerData: { \n input: context.inputData, \n }, \n }); \n \n // Return the appropriate result based on workflow type \n if (context.workflowType === ' simple ') { \n return { \n // @ts-ignore \n result: result.results[ ' simpleStep ']?.output, \n }; \n } else { \n return { \n // @ts-ignore \n result: result.results[ ' step2 ']?.output, \n }; \n } \n }, \n}); \n Important Considerations \n \n \n Mastra Instance: The mastra parameter in the execute function provides access to the Mastra instance, which is essential for creating dynamic workflows. \n \n \n Error Handling: Always check if the Mastra instance is available before attempting to create a dynamic workflow. \n \n \n Resource Management: Dynamic workflows consume resources, so be mindful of creating too many workflows in a single execution. \n \n \n Workflow Lifecycle: Dynamic workflows are not automatically registered with the main Mastra instance. They exist only for the duration of the step execution unless you explicitly register them. \n \n \n Debugging: Debugging dynamic workflows can be challenging. Consider adding detailed logging to track their creation and execution. \n \n \n Use Cases \n \n Conditional Workflow Selection: Choose different workflow patterns based on input data \n Parameterized Workflows: Create workflows with dynamic configurations \n Workflow Templates: Use templates to generate specialized workflows \n Multi-tenant Applications: Create isolated workflows for different tenants \n \n Conclusion \n Dynamic workflows provide a powerful way to create flexible, adaptable workflow systems. By leveraging the Mastra instance within step execution, you can create workflows that respond to runtime conditions and requirements. Suspend &amp; Resume Error Handling",
            "image": "https://mastra.ai/api/og/docs?title=Dynamic%20Workflows%20|%20Mastra%20Docs&description=Learn%20how%20to%20create%20dynamic%20workflows%20within%20workflow%20steps,%20allowing%20for%20flexible%20workflow%20creation%20based%20on%20runtime%20conditions.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/dynamic-workflows",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/error-handling",
            "title": "Error Handling in Workflows | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/error-handling",
            "publishedDate": "2000-01-01T00:00:00.000Z",
            "author": "",
            "text": "\n Robust error handling is essential for production workflows. Mastra provides several mechanisms to handle errors gracefully, allowing your workflows to recover from failures or gracefully degrade when necessary. \n Overview \n Error handling in Mastra workflows can be implemented using: \n \n Step Retries - Automatically retry failed steps \n Conditional Branching - Create alternative paths based on step success or failure \n Error Monitoring - Watch workflows for errors and handle them programmatically \n Result Status Checks - Check the status of previous steps in subsequent steps \n \n Step Retries \n Mastra provides a built-in retry mechanism for steps that fail due to transient errors. This is particularly useful for steps that interact with external services or resources that might experience temporary unavailability. \n Basic Retry Configuration \n You can configure retries at the workflow level or for individual steps: \n // Workflow-level retry configuration \n const workflow = new Workflow ({ \n name: ' my-workflow ', \n retryConfig: { \n attempts: 3, // Number of retry attempts \n delay: 1000, // Delay between retries in milliseconds \n }, \n}); \n \n // Step-level retry configuration (overrides workflow-level) \n const apiStep = new Step ({ \n id: ' callApi ', \n execute: async () =&gt; { \n // API call that might fail \n }, \n retryConfig: { \n attempts: 5, // This step will retry up to 5 times \n delay: 2000, // With a 2-second delay between retries \n }, \n}); \n For more details about step retries, see the Step Retries reference. \n Conditional Branching \n You can create alternative workflow paths based on the success or failure of previous steps using conditional logic: \n // Create a workflow with conditional branching \n const workflow = new Workflow ({ \n name: ' error-handling-workflow ', \n}); \n \n workflow \n . step (fetchDataStep) \n . then (processDataStep, { \n // Only execute processDataStep if fetchDataStep was successful \n when: ({ context }) =&gt; { \n return context.steps.fetchDataStep?.status === ' success '; \n }, \n }) \n . then (fallbackStep, { \n // Execute fallbackStep if fetchDataStep failed \n when: ({ context }) =&gt; { \n return context.steps.fetchDataStep?.status === ' failed '; \n }, \n }) \n . commit (); \n Error Monitoring \n You can monitor workflows for errors using the watch method: \n const { start, watch } = workflow. createRun (); \n \n watch ( async ({ results }) =&gt; { \n // Check for any failed steps \n const failedSteps = Object. entries (results) \n . filter (([_, step]) =&gt; step.status === \" failed \") \n . map (([stepId]) =&gt; stepId); \n \n if (failedSteps. length &gt; 0) { \n console. error ( ` Workflow has failed steps: ${ failedSteps. join ( ', ')}`); \n // Take remedial action, such as alerting or logging \n } \n}); \n \n await start (); \n Handling Errors in Steps \n Within a step’s execution function, you can handle errors programmatically: \n const robustStep = new Step ({ \n id: ' robustStep ', \n execute: async ({ context }) =&gt; { \n try { \n // Attempt the primary operation \n const result = await someRiskyOperation (); \n return { success: true, data: result }; \n } catch (error) { \n // Log the error \n console. error ( ' Operation failed: ', error); \n \n // Return a graceful fallback result instead of throwing \n return { \n success: false, \n error: error.message, \n fallbackData: ' Default value ' \n }; \n } \n }, \n}); \n Checking Previous Step Results \n You can make decisions based on the results of previous steps: \n const finalStep = new Step ({ \n id: ' finalStep ', \n execute: async ({ context }) =&gt; { \n // Check results of previous steps \n const step1Success = context.steps.step1?.status === ' success '; \n const step2Success = context.steps.step2?.status === ' success '; \n \n if (step1Success &amp;&amp; step2Success) { \n // All steps succeeded \n return { status: ' complete ', result: ' All operations succeeded ' }; \n } else if (step1Success) { \n // Only step1 succeeded \n return { status: ' partial ', result: ' Partial completion ' }; \n } else { \n // Critical failure \n return { status: ' failed ', result: ' Critical steps failed ' }; \n } \n }, \n}); \n Best Practices for Error Handling \n \n \n Use retries for transient failures: Configure retry policies for steps that might experience temporary issues. \n \n \n Provide fallback paths: Design workflows with alternative paths for when critical steps fail. \n \n \n Be specific about error scenarios: Use different handling strategies for different types of errors. \n \n \n Log errors comprehensively: Include context information when logging errors to aid in debugging. \n \n \n Return meaningful data on failure: When a step fails, return structured data about the failure to help downstream steps make decisions. \n \n \n Consider idempotency: Ensure steps can be safely retried without causing duplicate side effects. \n \n \n Monitor workflow execution: Use the watch method to actively monitor workflow execution and detect errors early. \n \n \n Advanced Error Handling \n For more complex error handling scenarios, consider: \n \n Implementing circuit breakers: If a step fails repeatedly, stop retrying and use a fallback strategy \n Adding timeout handling: Set time limits for steps to prevent workflows from hanging indefinitely \n Creating dedicated error recovery workflows: For critical workflows, create separate recovery workflows that can be triggered when the main workflow fails \n \n Related \n \n Step Retries Reference \n Watch Method Reference \n Step Conditions \n Control Flow \n",
            "image": "https://mastra.ai/api/og/docs?title=Error%20Handling%20in%20Workflows%20|%20Mastra%20Docs&description=Learn%20how%20to%20handle%20errors%20in%20Mastra%20workflows%20using%20step%20retries,%20conditional%20branching,%20and%20monitoring.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/error-handling",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/nested-workflows",
            "title": "Nested Workflows",
            "url": "https://mastra.ai/docs/workflows/nested-workflows",
            "author": "",
            "text": "\n Mastra allows you to use workflows as steps within other workflows, enabling you to create modular and reusable workflow components. This feature helps in organizing complex workflows into smaller, manageable pieces and promotes code reuse. \n It is also visually easier to understand the flow of a workflow when you can see the nested workflows as steps in the parent workflow. \n Basic Usage \n You can use a workflow as a step directly in another workflow using the step() method: \n // Create a nested workflow \n const nestedWorkflow = new Workflow ({ name: \" nested-workflow \" }) \n . step (stepA) \n . then (stepB) \n . commit (); \n \n // Use the nested workflow in a parent workflow \n const parentWorkflow = new Workflow ({ name: \" parent-workflow \" }) \n . step (nestedWorkflow, { \n variables: { \n city: { \n step: \" trigger \", \n path: \" myTriggerInput \", \n }, \n }, \n }) \n . then (stepC) \n . commit (); \n When a workflow is used as a step: \n \n It is automatically converted to a step using the workflow’s name as the step ID \n The workflow’s results are available in the parent workflow’s context \n The nested workflow’s steps are executed in their defined order \n \n Accessing Results \n Results from a nested workflow are available in the parent workflow’s context under the nested workflow’s name. The results include all step outputs from the nested workflow: \n const { results } = await parentWorkflow. start (); \n // Access nested workflow results \n const nestedWorkflowResult = results[ \" nested-workflow \"]; \n if (nestedWorkflowResult.status === \" success \") { \n const nestedResults = nestedWorkflowResult.output.results; \n} \n Control Flow with Nested Workflows \n Nested workflows support all the control flow features available to regular steps: \n Parallel Execution \n Multiple nested workflows can be executed in parallel: \n parentWorkflow \n . step (nestedWorkflowA) \n . step (nestedWorkflowB) \n . after ([nestedWorkflowA, nestedWorkflowB]) \n . step (finalStep); \n Or using step() with an array of workflows: \n parentWorkflow. step ([nestedWorkflowA, nestedWorkflowB]). then (finalStep); \n In this case, then() will implicitly wait for all the workflows to finish before executing the final step. \n If-Else Branching \n Nested workflows can be used in if-else branches using the new syntax that accepts both branches as arguments: \n // Create nested workflows for different paths \n const workflowA = new Workflow ({ name: \" workflow-a \" }) \n . step (stepA1) \n . then (stepA2) \n . commit (); \n \n const workflowB = new Workflow ({ name: \" workflow-b \" }) \n . step (stepB1) \n . then (stepB2) \n . commit (); \n \n // Use the new if-else syntax with nested workflows \n parentWorkflow \n . step (initialStep) \n . if ( \n async ({ context }) =&gt; { \n // Your condition here \n return someCondition; \n }, \n workflowA, // if branch \n workflowB, // else branch \n ) \n . then (finalStep) \n . commit (); \n The new syntax is more concise and clearer when working with nested workflows. When the condition is: \n \n true: The first workflow (if branch) is executed \n false: The second workflow (else branch) is executed \n \n The skipped workflow will have a status of skipped in the results: \n The.then(finalStep) call following the if-else block will merge the if and else branches back into a single execution path. \n Looping \n Nested workflows can use.until() and.while() loops same as any other step. One interesting new pattern is to pass a workflow directly as the loop-back argument to keep executing that nested workflow until something is true about its results: \n parentWorkflow \n . step (firstStep) \n . while ( \n ({ context }) =&gt; \n context. getStepResult ( \" nested-workflow \").output.results.someField === \n \" someValue \", \n nestedWorkflow, \n ) \n . step (finalStep) \n . commit (); \n Watching Nested Workflows \n You can watch the state changes of nested workflows using the watch method on the parent workflow. This is useful for monitoring the progress and state transitions of complex workflows: \n const parentWorkflow = new Workflow ({ name: \" parent-workflow \" }) \n . step ([nestedWorkflowA, nestedWorkflowB]) \n . then (finalStep) \n . commit (); \n \n const run = parentWorkflow. createRun (); \n const unwatch = parentWorkflow. watch ((state) =&gt; { \n console. log ( \" Current state: \", state.value); \n // Access nested workflow states in state.context \n}); \n \n await run. start (); \n unwatch (); // Stop watching when done \n Suspending and Resuming \n Nested workflows support suspension and resumption, allowing you to pause and continue workflow execution at specific points. You can suspend either the entire nested workflow or specific steps within it: \n // Define a step that may need to suspend \n const suspendableStep = new Step ({ \n id: \" other \", \n description: \" Step that may need to suspend \", \n execute: async ({ context, suspend }) =&gt; { \n if (! wasSuspended) { \n wasSuspended = true; \n await suspend (); \n } \n return { other: 26 }; \n }, \n}); \n \n // Create a nested workflow with suspendable steps \n const nestedWorkflow = new Workflow ({ name: \" nested-workflow-a \" }) \n . step (startStep) \n . then (suspendableStep) \n . then (finalStep) \n . commit (); \n \n // Use in parent workflow \n const parentWorkflow = new Workflow ({ name: \" parent-workflow \" }) \n . step (beginStep) \n . then (nestedWorkflow) \n . then (lastStep) \n . commit (); \n \n // Start the workflow \n const run = parentWorkflow. createRun (); \n const { runId, results } = await run. start ({ triggerData: { startValue: 1 } }); \n \n // Check if a specific step in the nested workflow is suspended \n if (results[ \" nested-workflow-a \"].output.results.other.status === \" suspended \") { \n // Resume the specific suspended step using dot notation \n const resumedResults = await run. resume ({ \n stepId: \" nested-workflow-a.other \", \n context: { startValue: 1 }, \n }); \n \n // The resumed results will contain the completed nested workflow \n expect (resumedResults.results[ \" nested-workflow-a \"].output.results). toEqual ({ \n start: { output: { newValue: 1 }, status: \" success \" }, \n other: { output: { other: 26 }, status: \" success \" }, \n final: { output: { finalValue: 27 }, status: \" success \" }, \n }); \n} \n When resuming a nested workflow: \n \n Use the nested workflow’s name as the stepId when calling resume() to resume the entire workflow \n Use dot notation ( nested-workflow.step-name) to resume a specific step within the nested workflow \n The nested workflow will continue from the suspended step with the provided context \n You can check the status of specific steps in the nested workflow’s results using results[\"nested-workflow\"].output.results \n \n Result Schemas and Mapping \n Nested workflows can define their result schema and mapping, which helps in type safety and data transformation. This is particularly useful when you want to ensure the nested workflow’s output matches a specific structure or when you need to transform the results before they’re used in the parent workflow. \n // Create a nested workflow with result schema and mapping \n const nestedWorkflow = new Workflow ({ \n name: \" nested-workflow \", \n result: { \n schema: z. object ({ \n total: z. number (), \n items: z. array ( \n z. object ({ \n id: z. string (), \n value: z. number (), \n }), \n ), \n }), \n mapping: { \n // Map values from step results using variables syntax \n total: { step: \" step-a \", path: \" count \" }, \n items: { step: \" step-b \", path: \" items \" }, \n }, \n }, \n}) \n . step (stepA) \n . then (stepB) \n . commit (); \n \n // Use in parent workflow with type-safe results \n const parentWorkflow = new Workflow ({ name: \" parent-workflow \" }) \n . step (nestedWorkflow) \n . then ( async ({ context }) =&gt; { \n const result = context. getStepResult ( \" nested-workflow \"); \n // TypeScript knows the structure of result \n console. log (result.total); // number \n console. log (result.items); // Array&lt;{ id: string, value: number }&gt; \n return { success: true }; \n }) \n . commit (); \n Best Practices \n \n Modularity: Use nested workflows to encapsulate related steps and create reusable workflow components. \n Naming: Give nested workflows descriptive names as they will be used as step IDs in the parent workflow. \n Error Handling: Nested workflows propagate their errors to the parent workflow, so handle errors appropriately. \n State Management: Each nested workflow maintains its own state but can access the parent workflow’s context. \n Suspension: When using suspension in nested workflows, consider the entire workflow’s state and handle resumption appropriately. \n \n Example \n Here’s a complete example showing various features of nested workflows: \n const workflowA = new Workflow ({ \n name: \" workflow-a \", \n result: { \n schema: z. object ({ \n activities: z. string (), \n }), \n mapping: { \n activities: { \n step: planActivities, \n path: \" activities \", \n }, \n }, \n }, \n}) \n . step (fetchWeather) \n . then (planActivities) \n . commit (); \n \n const workflowB = new Workflow ({ \n name: \" workflow-b \", \n result: { \n schema: z. object ({ \n activities: z. string (), \n }), \n mapping: { \n activities: { \n step: planActivities, \n path: \" activities \", \n }, \n }, \n }, \n}) \n . step (fetchWeather) \n . then (planActivities) \n . commit (); \n \n const weatherWorkflow = new Workflow ({ \n name: \" weather-workflow \", \n triggerSchema: z. object ({ \n cityA: z. string (). describe ( \" The city to get the weather for \"), \n cityB: z. string (). describe ( \" The city to get the weather for \"), \n }), \n result: { \n schema: z. object ({ \n activitiesA: z. string (), \n activitiesB: z. string (), \n }), \n mapping: { \n activitiesA: { \n step: workflowA, \n path: \" result.activities \", \n }, \n activitiesB: { \n step: workflowB, \n path: \" result.activities \", \n }, \n }, \n }, \n}) \n . step (workflowA, { \n variables: { \n city: { \n step: \" trigger \", \n path: \" cityA \", \n }, \n }, \n }) \n . step (workflowB, { \n variables: { \n city: { \n step: \" trigger \", \n path: \" cityB \", \n }, \n }, \n }); \n \n weatherWorkflow. commit (); \n In this example: \n \n We define schemas for type safety across all workflows \n Each step has proper input and output schemas \n The nested workflows have their own trigger schemas and result mappings \n Data is passed through using variables syntax in the.step() calls \n The main workflow combines data from both nested workflows \n",
            "image": "https://mastra.ai/api/og/docs?title=Nested%20Workflows&description=undefined",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/nested-workflows",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows/runtime-variables",
            "title": "Runtime variables - dependency injection | Workflows | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows/runtime-variables",
            "author": "",
            "text": "Workflow Runtime Variables \n Mastra provides a powerful dependency injection system that enables you to configure your workflows and steps with runtime variables. This feature is essential for creating flexible and reusable workflows that can adapt their behavior based on runtime configuration. \n Overview \n The dependency injection system allows you to: \n \n Pass runtime configuration variables to workflows through a type-safe runtimeContext \n Access these variables within step execution contexts \n Modify workflow behavior without changing the underlying code \n Share configuration across multiple steps within the same workflow \n \n Basic Usage \n const myWorkflow = mastra. getWorkflow ( \" myWorkflow \"); \n const { runId, start, resume } = myWorkflow. createRun (); \n \n // Define your runtimeContext's type structure \n type WorkflowRuntimeContext = { \n multiplier: number; \n}; \n \n const runtimeContext = new RuntimeContext &lt; WorkflowRuntimeContext &gt;(); \n runtimeContext. set ( \" multiplier \", 5); \n \n // Start the workflow execution with runtimeContext \n await start ({ \n triggerData: { inputValue: 45 }, \n runtimeContext, \n}); \n Using with REST API \n Here’s how to dynamically set a multiplier value from an HTTP header: \n import { Mastra } from \" @mastra/core \"; \n import { RuntimeContext } from \" @mastra/core/di \"; \n import { workflow as myWorkflow } from \"./workflows \"; \n \n // Define runtimeContext type with clear, descriptive types \n type WorkflowRuntimeContext = { \n multiplier: number; \n}; \n \n export const mastra = new Mastra ({ \n workflows: { \n myWorkflow, \n }, \n server: { \n middleware: [ \n async (c, next) =&gt; { \n const multiplier = c.req. header ( \" x-multiplier \"); \n const runtimeContext = c. get &lt; WorkflowRuntimeContext &gt;( \" runtimeContext \"); \n \n // Parse and validate the multiplier value \n const multiplierValue = parseInt (multiplier || \" 1 \", 10); \n if ( isNaN (multiplierValue)) { \n throw new Error ( \" Invalid multiplier value \"); \n } \n \n runtimeContext. set ( \" multiplier \", multiplierValue); \n \n await next (); // Don't forget to call next() \n }, \n ], \n }, \n}); \n Creating Steps with Variables \n Steps can access runtimeContext variables and must conform to the workflow’s runtimeContext type: \n import { Step } from \" @mastra/core/workflow \"; \n import { z } from \" zod \"; \n \n // Define step input/output types \n interface StepInput { \n inputValue: number; \n} \n \n interface StepOutput { \n incrementedValue: number; \n} \n \n const stepOne = new Step ({ \n id: \" stepOne \", \n description: \" Multiply the input value by the configured multiplier \", \n execute: async ({ context, runtimeContext }) =&gt; { \n try { \n // Type-safe access to runtimeContext variables \n const multiplier = runtimeContext. get ( \" multiplier \"); \n if (multiplier === undefined) { \n throw new Error ( \" Multiplier not configured in runtimeContext \"); \n } \n \n // Get and validate input \n const inputValue = \n context. getStepResult &lt; StepInput &gt;( \" trigger \")?.inputValue; \n if (inputValue === undefined) { \n throw new Error ( \" Input value not provided \"); \n } \n \n const result: StepOutput = { \n incrementedValue: inputValue * multiplier, \n }; \n \n return result; \n } catch (error) { \n console. error ( ` Error in stepOne: ${ error.message}`); \n throw error; \n } \n }, \n}); \n Error Handling \n When working with runtime variables in workflows, it’s important to handle potential errors: \n \n Missing Variables: Always check if required variables exist in the runtimeContext \n Type Mismatches: Use TypeScript’s type system to catch type errors at compile time \n Invalid Values: Validate variable values before using them in your steps \n \n // Example of defensive programming with runtimeContext variables \n const multiplier = runtimeContext. get ( \" multiplier \"); \n if (multiplier === undefined) { \n throw new Error ( \" Multiplier not configured in runtimeContext \"); \n} \n \n // Type and value validation \n if ( typeof multiplier !== \" number \" || multiplier &lt;= 0) { \n throw new Error ( ` Invalid multiplier value: ${ multiplier}`); \n} \n Best Practices \n \n Type Safety: Always define proper types for your runtimeContext and step inputs/outputs \n Validation: Validate all inputs and runtimeContext variables before using them \n Error Handling: Implement proper error handling in your steps \n Documentation: Document the expected runtimeContext variables for each workflow \n Default Values: Provide sensible defaults when possible \n Nested Workflows new Overview new",
            "image": "https://mastra.ai/api/og/docs?title=Runtime%20variables%20-%20dependency%20injection%20|%20Workflows%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20Mastra%27s%20dependency%20injection%20system%20to%20provide%20runtime%20configuration%20to%20workflows%20and%20steps.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows/runtime-variables",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows-vnext/overview",
            "title": "Handling Complex LLM Operations | Workflows (vNext) | Mastra",
            "url": "https://mastra.ai/docs/workflows-vnext/overview",
            "author": "",
            "text": "Getting Started \n To use vNext workflows, first import the necessary functions from the vNext module: \n import { createWorkflow, createStep } from \" @mastra/core/workflows/vNext \"; \n import { z } from \" zod \"; // For schema validation \n Key Concepts \n vNext workflows consist of: \n \n Schemas: Type definitions for inputs and outputs using Zod \n Steps: Individual units of work with defined inputs and outputs \n Workflows: Orchestrations of steps with defined execution patterns. A workflow is also a step and can be used as such in other workflows. \n Workflow execution flow: How steps are executed and connected to each other \n \n Schemas are defined using Zod both for inputs and outputs of steps and workflows. Schemas can also dictate what data does a step take when resuming from a suspended state, as well as what contextual information should be passed when suspending a step’s execution. \n The inputs and outputs of steps that are connected together should match: the inputSchema of a step should be the same as the outputSchema of the previous step, for instance. The same is true, when using workflows as steps in other workflows, the workflow’s inputSchema should match the outputSchema of the step it is used as. \n Steps are run using an execute function that receives a context object with inputs from the previous step and/or resume data if the step is being resumed from a suspended state. The execute function should return a value that matches its outputSchema. \n Primitives such as.then(),.parallel() and.branch() describe the execution flow of workflows, and how the steps within them are connected. Running workflows (whether standalone or as a step), their execution is dictated by their execution flow instead of an execute function. The final result of a workflow will always be the result of its last step, which should match the workflow’s outputSchema. \n Creating Workflows \n Steps \n Steps are the building blocks of workflows. Create a step using createStep: \n const myStep = createStep ({ \n id: \" my-step \", \n description: \" Does something useful \", \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n outputValue: z. string (), \n }), \n resumeSchema: z. object ({ \n resumeValue: z. string (), \n }), \n suspendSchema: z. object ({ \n suspendValue: z. string (), \n }), \n execute: async ({ \n inputData, \n mastra, \n getStepResult, \n getInitData, \n runtimeContext, \n }) =&gt; { \n const otherStepOutput = getStepResult (step2); \n const initData = getInitData &lt; typeof workflow&gt;(); // typed as the workflow input schema \n return { \n outputValue: ` Processed: ${ inputData.inputValue}, ${ initData.startValue} (runtimeContextValue: ${ runtimeContext. get ( \" runtimeContextValue \")}) `, \n }; \n }, \n}); \n Each step requires: \n \n id: Unique identifier for the step \n inputSchema: Zod schema defining expected input \n outputSchema: Zod schema defining output shape \n resumeSchema: Optional. Zod schema defining resume input \n suspendSchema: Optional. Zod schema defining suspend input \n execute: Async function that performs the step’s work \n \n The execute function receives a context object with: \n \n inputData: The input data matching the inputSchema \n resumeData: The resume data matching the resumeSchema, when resuming the step from a suspended state. Only exists if the step is being resumed. \n mastra: Access to mastra services (agents, tools, etc.) \n getStepResult: Function to access results from other steps \n getInitData: Function to access the initial input data of the workflow in any step \n suspend: Function to pause workflow execution (for user interaction) \n \n Workflow Structure \n Create a workflow using createWorkflow: \n const myWorkflow = createWorkflow ({ \n id: \" my-workflow \", \n inputSchema: z. object ({ \n startValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n steps: [step1, step2, step3], // Declare steps used in this workflow \n}); \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n myWorkflow, \n }, \n}); \n \n const run = mastra. vnext_getWorkflow ( \" myWorkflow \"). createRun (); \n The steps property in the workflow options provides type safety for accessing step results. When you declare the steps used in your workflow, TypeScript will ensure type safety when accessing result.steps: \n // With steps declared in workflow options \n const workflow = createWorkflow ({ \n id: \" my-workflow \", \n inputSchema: z. object ({}), \n outputSchema: z. object ({}), \n steps: [step1, step2], // TypeScript knows these steps exist \n}); \n \n const result = await workflow. createRun (). start ({ inputData: {} }); \n if (result.status === \" success \") { \n console. log (result.result); // only exists if status is success \n} else if (result.status === \" failed \") { \n console. error (result.error); // only exists if status is failed, this is an instance of Error \n throw result.error; \n} else if (result.status === \" suspended \") { \n console. log (result.suspended); // only exists if status is suspended \n} \n \n // TypeScript knows these properties exist and their types \n console. log (result.steps.step1.output); // Fully typed \n console. log (result.steps.step2.output); // Fully typed \n Workflow definition requires: \n \n id: Unique identifier for the workflow \n inputSchema: Zod schema defining workflow input \n outputSchema: Zod schema defining workflow output \n steps: Array of steps used in the workflow (optional, but recommended for type safety) \n \n Re-using steps and nested workflows \n You can re-use steps and nested workflows by cloning them: \n const clonedStep = cloneStep (myStep, { id: \" cloned-step \" }); \n const clonedWorkflow = cloneWorkflow (myWorkflow, { id: \" cloned-workflow \" }); \n This way you can use the same step or nested workflow in the same workflow multiple times. \n import { \n createWorkflow, \n createStep, \n cloneStep, \n cloneWorkflow, \n} from \" @mastra/core/workflows/vNext \"; \n \n const myWorkflow = createWorkflow ({ \n id: \" my-workflow \", \n steps: [step1, step2, step3], \n}); \n myWorkflow. then (step1). then (step2). then (step3). commit (); \n \n const parentWorkflow = createWorkflow ({ \n id: \" parent-workflow \", \n steps: [myWorkflow, step4], \n}); \n parentWorkflow \n . then (myWorkflow) \n . then (step4) \n . then ( cloneWorkflow (myWorkflow, { id: \" cloned-workflow \" })) \n . then ( cloneStep (step4, { id: \" cloned-step-4 \" })) \n . commit (); \n Running Workflows \n After defining a workflow, run it with: \n // Create a run instance \n const run = myWorkflow. createRun (); \n \n // Start the workflow with input data \n const result = await run. start ({ \n inputData: { \n startValue: \" initial data \", \n }, \n}); \n \n // Access the results \n console. log (result.steps); // All step results \n console. log (result.steps[ \" step-id \"].output); // Output from a specific step \n \n if (result.status === \" success \") { \n console. log (result.result); // The final result of the workflow, result of the last step (or `.map()` output, if used as last step) \n} else if (result.status === \" suspended \") { \n const resumeResult = await run. resume ({ \n step: result.suspended[ 0], // there is always at least one step id in the suspended array, in this case we resume the first suspended execution path \n resumeData: { \n /* user input */ \n }, \n }); \n} else if (result.status === \" failed \") { \n console. error (result.error); // only exists if status is failed, this is an instance of Error \n} \n Workflow Execution Result Schema \n The result of running a workflow (either from start() or resume()) follows this TypeScript interface: \n export type WorkflowResult &lt;... &gt; = \n | { \n status: ' success '; \n result: z. infer &lt; TOutput &gt;; \n steps: { \n [ K in keyof StepsRecord &lt; TSteps &gt;]: StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema '] extends undefined \n ? StepResult &lt; unknown &gt; \n : StepResult &lt; z. infer &lt; NonNullable &lt; StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema ']&gt;&gt;&gt;; \n }; \n } \n | { \n status: ' failed '; \n steps: { \n [ K in keyof StepsRecord &lt; TSteps &gt;]: StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema '] extends undefined \n ? StepResult &lt; unknown &gt; \n : StepResult &lt; z. infer &lt; NonNullable &lt; StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema ']&gt;&gt;&gt;; \n }; \n error: Error; \n } \n | { \n status: ' suspended '; \n steps: { \n [ K in keyof StepsRecord &lt; TSteps &gt;]: StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema '] extends undefined \n ? StepResult &lt; unknown &gt; \n : StepResult &lt; z. infer &lt; NonNullable &lt; StepsRecord &lt; TSteps &gt;[ K][ ' outputSchema ']&gt;&gt;&gt;; \n }; \n suspended: [ string [], ...string [][]]; \n }; \n Result Properties Explained \n \n \n status: Indicates the final state of the workflow execution \n \n 'success': Workflow completed successfully \n 'failed': Workflow encountered an error \n 'suspended': Workflow is paused waiting for user input \n \n \n \n result: Contains the final output of the workflow, typed according to the workflow’s outputSchema \n \n \n suspended: Optional array of step IDs that are currently suspended. Only present when status is 'suspended' \n \n \n steps: A record containing the results of all executed steps \n \n Keys are step IDs \n Values are StepResult objects containing the step’s output \n Type-safe based on each step’s outputSchema \n \n \n \n error: Optional error object present when status is 'failed' \n \n \n Watching Workflow Execution \n You can also watch workflow execution: \n const run = myWorkflow. createRun (); \n \n // Add a watcher to monitor execution \n run. watch (event =&gt; { \n console. log ( ' Step completed: ', event.payload.currentStep.id); \n}); \n \n // Start the workflow \n const result = await run. start ({ inputData: {...} }); \n The event object has the following schema: \n type WatchEvent = { \n type: \" watch \"; \n payload: { \n currentStep?: { \n id: string; \n status: \" running \" | \" completed \" | \" failed \" | \" suspended \"; \n output?: Record &lt; string, any &gt;; \n payload?: Record &lt; string, any &gt;; \n }; \n workflowState: { \n status: \" running \" | \" success \" | \" failed \" | \" suspended \"; \n steps: Record &lt; \n string, \n { \n status: \" running \" | \" completed \" | \" failed \" | \" suspended \"; \n output?: Record &lt; string, any &gt;; \n payload?: Record &lt; string, any &gt;; \n } \n &gt;; \n result?: Record &lt; string, any &gt;; \n error?: Record &lt; string, any &gt;; \n payload?: Record &lt; string, any &gt;; \n }; \n }; \n eventTimestamp: Date; \n}; \n The currentStep property is only present when the workflow is running. When the workflow is finished the status on workflowState is changed, as well as the result and error properties. At the same time the currentStep property is removed.",
            "image": "https://mastra.ai/api/og/docs?title=Handling%20Complex%20LLM%20Operations%20|%20Workflows%20(vNext)%20|%20Mastra&description=Workflows%20(vNext)%20in%20Mastra%20help%20you%20orchestrate%20complex%20sequences%20of%20operations%20with%20features%20like%20branching,%20parallel%20execution,%20resource%20suspension,%20and%20more.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows-vnext/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows-vnext/flow-control",
            "title": "Branching, Merging, Conditions | Workflows (vNext) | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows-vnext/flow-control",
            "author": "",
            "text": "Sequential Flow \n Chain steps to execute in sequence using.then(): \n myWorkflow. then (step1). then (step2). then (step3). commit (); \n The output from each step is automatically passed to the next step if schemas match. If the schemas don’t match, you can use the map function to transform the output to the expected schema.\nStep chaining is type-safe and checked at compile time. \n Parallel Execution \n Execute steps in parallel using.parallel(): \n myWorkflow. parallel ([step1, step2]). then (step3). commit (); \n This executes all steps in the array concurrently, then continues to the next step after all parallel steps complete. \n You can also execute entire workflows in parallel: \n myWorkflow \n . parallel ([nestedWorkflow1, nestedWorkflow2]) \n . then (finalStep) \n . commit (); \n Parallel steps receive previous step results as input. Their outputs are passed into the next step input as an object where the key is the step id and the value is the step output, for example the above example outputs an object with two keys nestedWorkflow1 and nestedWorkflow2 with the outputs of the respective workflows as values. \n Conditional Branching \n Create conditional branches using.branch(): \n myWorkflow \n . then (initialStep) \n . branch ([ \n [ async ({ inputData }) =&gt; inputData.value &gt; 50, highValueStep], \n [ async ({ inputData }) =&gt; inputData.value &gt; 10 &amp;&amp; inputData.value &lt;= 50, lowValueStep], \n [ async ({ inputData }) =&gt; inputData.value &lt;= 10, extremelyLowValueStep], \n ]) \n . then (finalStep) \n . commit (); \n Branch conditions are evaluated sequentially, and all steps with matching conditions are executed in parallel. If inputData.value is 5 then both lowValueStep and extremelyLowValueStep will be run. \n Each conditional step (like highValueStep or lowValueStep) receives as input the output of the previous step ( initialStep in this case). The output of each matching conditional step is collected. The next step after the branch ( finalStep) receives an object containing the outputs of all the steps that were run in the branch. The keys of this object are the step IDs, and the values are the outputs of those steps ( { lowValueStep: &lt;output of lowValueStep&gt;, extremelyLowValueStep: &lt;output of extremelyLowValueStep&gt; }). \n Loops \n vNext supports two types of loops. When looping a step (or nested workflow or any other step-compatible construct), the inputData of the loop is the output of the previous step initially, but any subsequent inputData is the output of the loop step itself. Thus for looping, the initial loop state should either match the previous step output or be derived using the map function. \n Do-While Loop: Executes a step repeatedly while a condition is true. \n myWorkflow \n . dowhile (incrementStep, async ({ inputData }) =&gt; inputData.value &lt; 10) \n . then (finalStep) \n . commit (); \n Do-Until Loop: Executes a step repeatedly until a condition becomes true. \n myWorkflow \n . dountil (incrementStep, async ({ inputData }) =&gt; inputData.value &gt;= 10) \n . then (finalStep) \n . commit (); \n const workflow = createWorkflow ({ \n id: \" increment-workflow \", \n inputSchema: z. object ({ \n value: z. number (), \n }), \n outputSchema: z. object ({ \n value: z. number (), \n }), \n}) \n . dountil (incrementStep, async ({ inputData }) =&gt; inputData.value &gt;= 10) \n . then (finalStep); \n Foreach \n Foreach is a step that executes a step for each item in an array type input. \n const mapStep = createStep ({ \n id: \" map \", \n description: \" Maps (+11) on the current value \", \n inputSchema: z. object ({ \n value: z. number (), \n }), \n outputSchema: z. object ({ \n value: z. number (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { value: inputData.value + 11 }; \n }, \n}); \n \n const finalStep = createStep ({ \n id: \" final \", \n description: \" Final step that prints the result \", \n inputSchema: z. array (z. object ({ value: z. number () })), \n outputSchema: z. object ({ \n finalValue: z. number (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { finalValue: inputData. reduce ((acc, curr) =&gt; acc + curr.value, 0) }; \n }, \n}); \n \n const counterWorkflow = createWorkflow ({ \n steps: [mapStep, finalStep], \n id: \" counter-workflow \", \n inputSchema: z. array (z. object ({ value: z. number () })), \n outputSchema: z. object ({ \n finalValue: z. number (), \n }), \n}); \n \n counterWorkflow. foreach (mapStep). then (finalStep). commit (); \n \n const run = counterWorkflow. createRun (); \n const result = await run. start ({ \n inputData: [{ value: 1 }, { value: 22 }, { value: 333 }], \n}); \n \n if (result.status === \" success \") { \n console. log (result.result); // only exists if status is success \n} else if (result.status === \" failed \") { \n console. error (result.error); // only exists if status is failed, this is an instance of Error \n} \n The loop executes the step for each item in the input array in sequence one at a time. The optional concurrency option allows you to execute steps in parallel with a limit on the number of concurrent executions. \n counterWorkflow. foreach (mapStep, { concurrency: 2 }). then (finalStep). commit (); \n Nested Workflows \n vNext supports composing workflows by nesting them: \n const nestedWorkflow = createWorkflow ({ \n id: ' nested-workflow ', \n inputSchema: z. object ({...}), \n outputSchema: z. object ({...}), \n}) \n . then (step1) \n . then (step2) \n . commit (); \n \n const mainWorkflow = createWorkflow ({ \n id: ' main-workflow ', \n inputSchema: z. object ({...}), \n outputSchema: z. object ({...}), \n}) \n . then (initialStep) \n . then (nestedWorkflow) \n . then (finalStep) \n . commit (); \n In the above example, the nestedWorkflow is used as a step in the mainWorkflow, where the inputSchema of nestedWorkflow matches the outputSchema of initialStep, and the outputSchema of nestedWorkflow matches the inputSchema of finalStep. \n Nested workflows are the main (and only) way compose execution flows beyond simple sequential execution. When using.branch() or.parallel() to compose execution flows, executing more than just one step necessarily requires a nested workflow, and as a byproduct, a description of how these steps are to be executed. \n const planBothWorkflow = createWorkflow ({ \n id: \" plan-both-workflow \", \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n steps: [planActivities, planIndoorActivities, sythesizeStep], \n}) \n . parallel ([planActivities, planIndoorActivities]) \n . then (sythesizeStep) \n . commit (); \n \n const weatherWorkflow = createWorkflow ({ \n id: \" weather-workflow-step3-concurrency \", \n inputSchema: z. object ({ \n city: z. string (). describe ( \" The city to get the weather for \"), \n }), \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n steps: [fetchWeather, planBothWorkflow, planActivities], \n}) \n . then (fetchWeather) \n . branch ([ \n [ \n async ({ inputData }) =&gt; { \n return inputData?.precipitationChance &gt; 20; \n }, \n planBothWorkflow, \n ], \n [ \n async ({ inputData }) =&gt; { \n return inputData?.precipitationChance &lt;= 20; \n }, \n planActivities, \n ], \n ]); \n Nested workflows only have their final result (result of the last step) as their step output.",
            "image": "https://mastra.ai/api/og/docs?title=Branching,%20Merging,%20Conditions%20|%20Workflows%20(vNext)%20|%20Mastra%20Docs&description=Control%20flow%20in%20Mastra%20(vNext)%20workflows%20allows%20you%20to%20manage%20branching,%20merging,%20and%20conditions%20to%20construct%20workflows%20that%20meet%20your%20logic%20requirements.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows-vnext/flow-control",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows-vnext/suspend-and-resume",
            "title": "Suspend & Resume Workflows (vNext) | Human-in-the-Loop | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows-vnext/suspend-and-resume",
            "author": "",
            "text": "\n Complex workflows often need to pause execution while waiting for external input or resources. \n Mastra’s suspend and resume features let you pause workflow execution at any step, persist the workflow snapshot to storage, and resume execution from the saved snapshot when ready.\nThis entire process is automatically managed by Mastra. No config needed, or manual step required from the user. \n Storing the workflow snapshot to storage (LibSQL by default) means that the workflow state is permanently preserved across sessions, deployments, and server restarts. This persistence is crucial for workflows that might remain suspended for minutes, hours, or even days while waiting for external input or resources. \n When to Use Suspend/Resume \n Common scenarios for suspending workflows include: \n \n Waiting for human approval or input \n Pausing until external API resources become available \n Collecting additional data needed for later steps \n Rate limiting or throttling expensive operations \n Handling event-driven processes with external triggers \n \n How to suspend a step \n const humanInputStep = createStep ({ \n id: \" human-input \", \n inputSchema: z. object ({ \n suggestions: z. array (z. string ()), \n vacationDescription: z. string (), \n }), \n resumeSchema: z. object ({ \n selection: z. string (), \n }), \n suspendSchema: z. object ({}), \n outputSchema: z. object ({ \n selection: z. string (). describe ( \" The selection of the user \"), \n vacationDescription: z. string (), \n }), \n execute: async ({ inputData, resumeData, suspend }) =&gt; { \n if (! resumeData?.selection) { \n await suspend ({}); \n return { \n selection: \"\", \n vacationDescription: inputData?.vacationDescription, \n }; \n } \n return { \n selection: resumeData.selection, \n vacationDescription: inputData?.vacationDescription, \n }; \n }, \n}); \n How to resume step execution \n Identifying suspended state \n When running a workflow, its state can be one of the following: \n \n running - The workflow is currently running \n suspended - The workflow is suspended \n success - The workflow has completed \n failed - The workflow has failed \n \n When the state is suspended, you can identify any and all steps that have been suspended by looking at the suspended property of the workflow. \n const run = counterWorkflow. createRun (); \n const result = await run. start ({ inputData: { startValue: 0 } }); \n \n if (result.status === \" suspended \") { \n const resumedResults = await run. resume ({ \n step: result.suspended[ 0], \n resumeData: { newValue: 0 }, \n }); \n} \n In this case, the logic resumes whatever is the first step reported as suspended. \n The suspended property is of type string[][], where every array is a path to a step that has been suspended, the first element being the step id on the main workflow. If that step is a workflow itself, the second element is the step id on the nested workflow that was suspended, unless it is a workflow itself, in which case the third element is the step id on the nested workflow that was suspended, and so on. \n Resume \n // After getting user input \n const result = await workflowRun. resume ({ \n step: userInputStep, // or 'myStepId' as a string \n resumeData: { \n userSelection: \" User's choice \", \n }, \n}); \n To resume a suspended nested workflow: \n const result = await workflowRun. resume ({ \n step: [nestedWorkflow, userInputStep], // or ['nestedWorkflowId', 'myStepId'] as a string array \n resumeData: { \n userSelection: \" User's choice \", \n }, \n});",
            "image": "https://mastra.ai/api/og/docs?title=Suspend%20&%20Resume%20Workflows%20(vNext)%20|%20Human-in-the-Loop%20|%20Mastra%20Docs&description=Suspend%20and%20resume%20in%20Mastra%20vNext%20workflows%20allows%20you%20to%20pause%20execution%20while%20waiting%20for%20external%20input%20or%20resources.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows-vnext/suspend-and-resume",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows-vnext/input-data-mapping",
            "title": "Input Data Mapping with Workflow (vNext) | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows-vnext/input-data-mapping",
            "author": "",
            "text": "\n Input data mapping allows explicit mapping of values for the inputs of the next step. These values can come from a number of sources: \n \n The outputs of a previous step \n The runtime context \n A constant value \n The initial input of the workflow \n \n myWorkflow \n . then (step1) \n . map ({ \n transformedValue: { \n step: step1, \n path: \" nestedValue \", \n }, \n runtimeContextValue: { \n runtimeContextPath: \" runtimeContextValue \", \n schema: z. number (), \n }, \n constantValue: { \n value: 42, \n schema: z. number (), \n }, \n initDataValue: { \n initData: myWorkflow, \n path: \" startValue \", \n }, \n }) \n . then (step2) \n . commit (); \n There are many cases where.map() can be useful in matching inputs to outputs, whether it’s renaming outputs to match inputs or mapping complex data structures or other previous step outputs. \n Renaming outputs \n One use case for input mappings is renaming outputs to match inputs: \n const step1 = createStep ({ \n id: \" step1 \", \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n outputValue: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { outputValue: inputData.inputValue }; \n }, \n}); \n \n const step2 = createStep ({ \n id: \" step2 \", \n inputSchema: z. object ({ \n unexpectedName: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { result: inputData.outputValue }; \n }, \n}); \n const workflow = createWorkflow ({ \n id: \" my-workflow \", \n steps: [step1, step2], \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n}); \n \n workflow \n . then (step1) \n . map ({ \n unexpectedName: { \n step: step1, \n path: \" outputValue \", \n }, \n }) \n . then (step2) \n . commit (); \n Using workflow inputs as later step inputs \n const step1 = createStep ({ \n id: \" step1 \", \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n outputValue: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { outputValue: inputData.inputValue }; \n }, \n}); \n \n const step2 = createStep ({ \n id: \" step2 \", \n inputSchema: z. object ({ \n outputValue: z. string (), \n initialValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { result: inputData.outputValue }; \n }, \n}); \n \n const workflow = createWorkflow ({ \n id: \" my-workflow \", \n steps: [step1, step2], \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n}); \n \n workflow \n . then (step1) \n . map ({ \n outputValue: { \n step: step1, \n path: \" outputValue \", \n }, \n initialValue: { \n initData: workflow, \n path: \" inputValue \", \n }, \n }) \n . then (step2) \n . commit (); \n Using multiple outputs of previous steps \n const step1 = createStep ({ \n id: \" step1 \", \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n outputValue: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { outputValue: inputData.inputValue }; \n }, \n}); \n \n const step2 = createStep ({ \n id: \" step2 \", \n inputSchema: z. object ({ \n outputValue: z. string (), \n initialValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { result: inputData.outputValue }; \n }, \n}); \n \n const step3 = createStep ({ \n id: \" step3 \", \n inputSchema: z. object ({ \n currentResult: z. string (), \n intermediateValue: z. string (), \n initialValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ inputData }) =&gt; { \n return { \n result: \n inputData.result + \n \" \" + \n inputData.intermediateValue + \n \" \" + \n inputData.initialValue, \n }; \n }, \n}); \n \n const workflow = createWorkflow ({ \n id: \" my-workflow \", \n steps: [step1, step2], \n inputSchema: z. object ({ \n inputValue: z. string (), \n }), \n outputSchema: z. object ({ \n result: z. string (), \n }), \n}); \n \n workflow \n . then (step1) \n . then (step2) \n . map ({ \n initialValue: { \n initData: workflow, \n path: \" inputValue \", \n }, \n currentResult: { \n step: step2, \n path: \" result \", \n }, \n intermediateValue: { \n step: step1, \n path: \" outputValue \", \n }, \n }) \n . then (step3) \n . commit ();",
            "image": "https://mastra.ai/api/og/docs?title=Input%20Data%20Mapping%20with%20Workflow%20(vNext)%20|%20Mastra%20Docs&description=Learn%20how%20to%20use%20workflow%20input%20mapping%20to%20create%20more%20dynamic%20data%20flows%20in%20your%20Mastra%20workflows%20(vNext).",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows-vnext/input-data-mapping",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/workflows-vnext/using-with-agents-and-tools",
            "title": "Using Workflows with Agents and Tools | Workflows (vNext) | Mastra Docs",
            "url": "https://mastra.ai/docs/workflows-vnext/using-with-agents-and-tools",
            "author": "",
            "text": "Agent as a step \n vNext workflows can use Mastra agents directly as steps using createStep(agent): \n // Agent defined elsewhere \n const myAgent = new Agent ({ \n name: \" myAgent \", \n instructions: \"... \", \n model: openai ( \" gpt-4 \"), \n}); \n \n // Create Mastra instance with agent \n const mastra = new Mastra ({ \n agents: { \n myAgent, \n }, \n vnext_workflows: { \n myWorkflow, \n }, \n}); \n \n // Use agent in workflow \n myWorkflow \n . then (preparationStep) \n . map ({ \n prompt: { \n step: preparationStep, \n path: \" formattedPrompt \", \n }, \n }) \n . then ( createStep (myAgent)) // Use agent directly as a step \n . then (processResultStep) \n . commit (); \n Tools as a step \n vNext workflows can use Mastra tools directly as steps using createStep(tool): \n const myTool = createTool ({ \n id: \" my-tool \", \n description: \" My tool \", \n inputSchema: z. object ({}), \n outputSchema: z. object ({}), \n execute: async ({ inputData }) =&gt; { \n return { result: \" success \" }; \n }, \n}); \n \n myWorkflow. then ( createStep (myTool)). then (finalStep). commit (); \n Workflow as a tool in an agent \n import { Agent } from \" @mastra/core/agent \"; \n import { createTool } from \" @mastra/core/tools \"; \n import { createWorkflow, createStep } from \" @mastra/core/workflows/vNext \"; \n \n const weatherWorkflow = createWorkflow ({ \n steps: [fetchWeather, planActivities], \n id: \" weather-workflow-step1-single-day \", \n inputSchema: z. object ({ \n city: z. string (). describe ( \" The city to get the weather for \"), \n }), \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n}) \n . then (fetchWeather) \n . then (planActivities); \n \n const activityPlannerTool = createTool ({ \n id: \" get-weather-specific-activities \", \n description: \" Get weather-specific activities for a city \", \n inputSchema: z. object ({ \n city: z. string (), \n }), \n outputSchema: z. object ({ \n activities: z. array (z. string ()), \n }), \n execute: async ({ context, mastra }) =&gt; { \n const plannerWorkflow = mastra?. getWorkflow ( \" my-workflow \"); \n if (! plannerWorkflow) { \n throw new Error ( \" Planner workflow not found \"); \n } \n \n const run = plannerWorkflow. createRun (); \n const results = await run. start ({ \n triggerData: { \n city: context.city, \n }, \n }); \n const planActivitiesStep = results.results[ \" plan-activities \"]; \n if (planActivitiesStep.status === \" success \") { \n return planActivitiesStep.output; \n } \n \n return { \n activities: \" No activities found \", \n }; \n }, \n}); \n \n const activityPlannerAgent = new Agent ({ \n name: \" activityPlannerAgent \", \n model: openai ( \" gpt-4o \"), \n instructions: ` \n You are an activity planner. You have access to a tool that will help you get weather-specific activities for any city. The tool uses agents to plan the activities, you just need to provide the city. Whatever information you get back, return it as is and add your own thoughts on top of it. \n `, \n tools: { activityPlannerTool }, \n}); \n \n export const mastra = new Mastra ({ \n vnext_workflows: { \n \" my-workflow \": myWorkflow, \n }, \n agents: { \n activityPlannerAgent, \n }, \n}); Input Data Mapping Overview",
            "image": "https://mastra.ai/api/og/docs?title=Using%20Workflows%20with%20Agents%20and%20Tools%20|%20Workflows%20(vNext)%20|%20Mastra%20Docs&description=Steps%20in%20Mastra%20workflows%20(vNext)%20provide%20a%20structured%20way%20to%20manage%20operations%20by%20defining%20inputs,%20outputs,%20and%20execution%20logic.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/workflows-vnext/using-with-agents-and-tools",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/rag/overview",
            "title": "RAG (Retrieval-Augmented Generation) in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/rag/overview",
            "author": "",
            "text": "\n RAG in Mastra helps you enhance LLM outputs by incorporating relevant context from your own data sources, improving accuracy and grounding responses in real information. \n Mastra’s RAG system provides: \n \n Standardized APIs to process and embed documents \n Support for multiple vector stores \n Chunking and embedding strategies for optimal retrieval \n Observability for tracking embedding and retrieval performance \n \n Example \n To implement RAG, you process your documents into chunks, create embeddings, store them in a vector database, and then retrieve relevant context at query time. \n This example shows the essentials: initialize a document, create chunks, generate embeddings, store them, and query for similar content. \n Document Processing \n The basic building block of RAG is document processing. Documents can be chunked using various strategies (recursive, sliding window, etc.) and enriched with metadata. See the chunking and embedding doc. \n Vector Storage \n Mastra supports multiple vector stores for embedding persistence and similarity search, including pgvector, Pinecone, and Qdrant. See the vector database doc. \n Observability and Debugging \n Mastra’s RAG system includes observability features to help you optimize your retrieval pipeline: \n \n Track embedding generation performance and costs \n Monitor chunk quality and retrieval relevance \n Analyze query patterns and cache hit rates \n Export metrics to your observability platform \n \n See the OTel Configuration page for more details. \n More resources \n \n Chain of Thought RAG Example \n All RAG Examples (including different chunking strategies, embedding models, and vector stores) \n Suspend &amp; Resume Chunking and Embedding",
            "image": "https://mastra.ai/api/og/docs?title=RAG%20(Retrieval-Augmented%20Generation)%20in%20Mastra%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/rag/chunking-and-embedding",
            "title": "Chunking and Embedding Documents | RAG | Mastra Docs",
            "url": "https://mastra.ai/docs/rag/chunking-and-embedding",
            "author": "",
            "text": "\n Before processing, create a MDocument instance from your content. You can initialize it from various formats: \n Step 1: Document Processing \n Use to split documents into manageable pieces. Mastra supports multiple chunking strategies optimized for different document types: \n \n: Smart splitting based on content structure \n: Simple character-based splits \n: Token-aware splitting \n: Markdown-aware splitting \n: HTML structure-aware splitting \n: JSON structure-aware splitting \n: LaTeX structure-aware splitting \n \n Here’s an example of how to use the strategy: \n Note: Metadata extraction may use LLM calls, so ensure your API key is set. \n We go deeper into chunking strategies in our chunk documentation. \n Step 2: Embedding Generation \n Transform chunks into embeddings using your preferred provider. Mastra supports both OpenAI and Cohere embeddings: \n Using OpenAI \n Using Cohere \n The embedding functions return vectors, arrays of numbers representing the semantic meaning of your text, ready for similarity searches in your vector database. \n Example: Complete Pipeline \n Here’s an example showing document processing and embedding generation with both providers: \n This example demonstrates how to process a document, split it into chunks, generate embeddings with both OpenAI and Cohere, and store the results in a vector database. \n For more examples of different chunking strategies and embedding configurations, see: \n \n Adjust Chunk Size \n Adjust Chunk Delimiters \n Embed Text with Cohere \n Overview Vector Databases",
            "image": "https://mastra.ai/api/og/docs?title=Chunking%20and%20Embedding%20Documents%20%7C%20RAG%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/rag/vector-databases",
            "title": "Storing Embeddings in A Vector Database | Mastra Docs",
            "url": "https://mastra.ai/docs/rag/vector-databases",
            "author": "",
            "text": "\n After generating embeddings, you need to store them in a database that supports vector similarity search. Mastra provides a consistent interface for storing and querying embeddings across different vector databases. \n Supported databases \n PostgreSQL with PgVector \n Best for teams already using PostgreSQL who want to minimize infrastructure complexity: \n Using Vector Storage \n Once initialized, all vector stores share the same interface for creating indexes, upserting embeddings, and querying. \n Creating Indexes \n Before storing embeddings, you need to create an index with the appropriate dimension size for your embedding model: \n The dimension size must match the output dimension of your chosen embedding model. Common dimension sizes are: \n \n OpenAI text-embedding-3-small: 1536 dimensions \n OpenAI text-embedding-3-large: 3072 dimensions \n Cohere embed-multilingual-v3: 1024 dimensions \n \n Upserting Embeddings \n After creating an index, you can store embeddings along with their basic metadata: \n The upsert operation: \n \n Takes an array of embedding vectors and their corresponding metadata \n Updates existing vectors if they share the same ID \n Creates new vectors if they don’t exist \n Automatically handles batching for large datasets \n \n Adding Metadata \n Vector stores support rich metadata for advanced filtering and organization. You can add any JSON-serializable fields that will help with retrieval. \n Reminder: Metadata is stored as a JSON field with no fixed schema, so you’ll want to name your fields consistently and apply a consistent schema, or your queries will return unexpected results. \n Key metadata considerations: \n \n Be strict with field naming - inconsistencies like ‘category’ vs ‘Category’ will affect queries \n Only include fields you plan to filter or sort by - extra fields add overhead \n Add timestamps (e.g., ‘createdAt’, ‘lastUpdated’) to track content freshness \n \n Best Practices \n \n Create indexes before bulk insertions \n Use batch operations for large insertions (the upsert method handles batching automatically) \n Only store metadata you’ll query against \n Match embedding dimensions to your model (e.g., 1536 for ) \n \n Examples \n For complete examples of different vector store implementations, see: \n \n Insert Embedding in PgVector \n Insert Embedding in Pinecone \n Insert Embedding in Qdrant \n Insert Embedding in Chroma \n Insert Embedding in Astra DB \n Insert Embedding in LibSQL \n Insert Embedding in Upstash \n Insert Embedding in Cloudflare Vectorize \n Basic RAG with Vector Storage \n Chunking and Embedding Retrieval",
            "image": "https://mastra.ai/api/og/docs?title=Storing%20Embeddings%20in%20A%20Vector%20Database%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/rag/retrieval",
            "title": "Retrieval, Semantic Search, Reranking | RAG | Mastra Docs",
            "url": "https://mastra.ai/docs/rag/retrieval",
            "author": "",
            "text": "Retrieval in RAG Systems \n After storing embeddings, you need to retrieve relevant chunks to answer user queries. \n Mastra provides flexible retrieval options with support for semantic search, filtering, and re-ranking. \n How Retrieval Works \n \n The user’s query is converted to an embedding using the same model used for document embeddings \n This embedding is compared to stored embeddings using vector similarity \n The most similar chunks are retrieved and can be optionally: \n \n \n Filtered by metadata \n Re-ranked for better relevance \n Processed through a knowledge graph \n \n Basic Retrieval \n The simplest approach is direct semantic search. This method uses vector similarity to find chunks that are semantically similar to the query: \n Results include both the text content and a similarity score: \n For an example of how to use the basic retrieval method, see the Retrieve Results example. \n Advanced Retrieval options \n Metadata Filtering \n Filter results based on metadata fields to narrow down the search space. This is useful when you have documents from different sources, time periods, or with specific attributes. Mastra provides a unified MongoDB-style query syntax that works across all supported vector stores. \n For detailed information about available operators and syntax, see the Metadata Filters Reference. \n Basic filtering examples: \n Common use cases for metadata filtering: \n \n Filter by document source or type \n Filter by date ranges \n Filter by specific categories or tags \n Filter by numerical ranges (e.g., price, rating) \n Combine multiple conditions for precise querying \n Filter by document attributes (e.g., language, author) \n \n For an example of how to use metadata filtering, see the Hybrid Vector Search example. \n Vector Query Tool \n Sometimes you want to give your agent the ability to query a vector database directly. The Vector Query Tool allows your agent to be in charge of retrieval decisions, combining semantic search with optional filtering and reranking based on the agent’s understanding of the user’s needs. \n When creating the tool, pay special attention to the tool’s name and description - these help the agent understand when and how to use the retrieval capabilities. For example, you might name it “SearchKnowledgeBase” and describe it as “Search through our documentation to find relevant information about X topic.” \n This is particularly useful when: \n \n Your agent needs to dynamically decide what information to retrieve \n The retrieval process requires complex decision-making \n You want the agent to combine multiple retrieval strategies based on context \n \n For detailed configuration options and advanced usage, see the Vector Query Tool Reference. \n Vector Store Prompts \n Vector store prompts define query patterns and filtering capabilities for each vector database implementation.\nWhen implementing filtering, these prompts are required in the agent’s instructions to specify valid operators and syntax for each vector store implementation. \n Re-ranking \n Initial vector similarity search can sometimes miss nuanced relevance. Re-ranking is a more computationally expensive process, but more accurate algorithm that improves results by: \n \n Considering word order and exact matches \n Applying more sophisticated relevance scoring \n Using a method called cross-attention between query and documents \n \n Here’s how to use re-ranking: \n The re-ranked results combine vector similarity with semantic understanding to improve retrieval quality. \n For more details about re-ranking, see the rerank() method. \n For an example of how to use the re-ranking method, see the Re-ranking Results example. \n Graph-based Retrieval \n For documents with complex relationships, graph-based retrieval can follow connections between chunks. This helps when: \n \n Information is spread across multiple documents \n Documents reference each other \n You need to traverse relationships to find complete answers \n \n Example setup: \n For more details about graph-based retrieval, see the GraphRAG class and the createGraphQueryTool() function. \n For an example of how to use the graph-based retrieval method, see the Graph-based Retrieval example. Vector Databases Creating Projects",
            "image": "https://mastra.ai/api/og/docs?title=Retrieval%2C%20Semantic%20Search%2C%20Reranking%20%7C%20RAG%20%7C%20Mastra%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/local-dev/creating-a-new-project",
            "title": "Creating a new Project | Mastra Local Development Docs",
            "url": "https://mastra.ai/docs/local-dev/creating-a-new-project",
            "author": "",
            "text": "Docs Local Dev Creating a New Project \n You can create a new project using the create-mastra package: \n You can also create a new project by using the mastra CLI directly: \n npm install -g mastra@latest \n mastra create \n Interactive Setup \n Running commands without arguments starts a CLI prompt for: \n \n Project name \n Component selection \n LLM provider configuration \n API key setup \n Example code inclusion \n \n Non-Interactive Setup \n To initialize mastra in non-interactive mode use the following command arguments: \n Arguments: \n --components Specify components: agents, tools, workflows \n --llm-provider LLM provider: openai, anthropic, groq, google, or cerebras \n --add-example Include example implementation \n --llm-api-key Provider API key \n --project-name Project name that will be used in package.json and as the project directory name \n Generated project structure: \n my-project/ \n ├── src/ \n │ └── mastra/ \n │ └── index.ts # Mastra entry point \n ├── package.json \n └── tsconfig.json Retrieval Add to an Existing Project",
            "image": "https://mastra.ai/api/og/docs?title=Creating%20a%20new%20Project%20|%20Mastra%20Local%20Development%20Docs&description=Create%20new%20Mastra%20projects%20or%20add%20Mastra%20to%20existing%20Node.js%20applications%20using%20the%20CLI",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/local-dev/creating-a-new-project",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/local-dev/add-to-existing-project",
            "title": "Adding to an Existing Project | Mastra Local Development Docs",
            "url": "https://mastra.ai/docs/local-dev/add-to-existing-project",
            "author": "",
            "text": "\n You can add Mastra to an existing project using the CLI: \n npm install -g mastra@latest \n mastra init \n Changes made to project: \n \n Creates src/mastra directory with entry point \n Adds required dependencies \n Configures TypeScript compiler options \n \n Interactive Setup \n Running commands without arguments starts a CLI prompt for: \n \n Component selection \n LLM provider configuration \n API key setup \n Example code inclusion \n \n Non-Interactive Setup \n To initialize mastra in non-interactive mode use the following command arguments: \n Arguments: \n --components Specify components: agents, tools, workflows \n --llm LLM provider: openai, anthropic, groq, google or cerebras \n --llm-api-key Provider API key \n --example Include example implementation \n --dir Directory for Mastra files (defaults to src/) \n For more details, refer to the mastra init CLI documentation. Creating a New Project Development Environment",
            "image": "https://mastra.ai/api/og/docs?title=Adding%20to%20an%20Existing%20Project%20|%20Mastra%20Local%20Development%20Docs&description=Add%20Mastra%20to%20your%20existing%20Node.js%20applications",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/local-dev/add-to-existing-project",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/local-dev/mastra-dev",
            "title": "Inspecting Agents with `mastra dev` | Mastra Local Dev Docs",
            "url": "https://mastra.ai/docs/local-dev/mastra-dev",
            "author": "",
            "text": "Inspecting agents and workflows with \n The command launches a development server that serves your Mastra application locally. \n REST API Endpoints \n spins up REST API endpoints for your agents and workflows, such as: \n \n \n \n \n \n \n \n By default, the server runs at http://localhost:4111, but you can change the port with the flag. \n Using the Client SDK \n The easiest way to interact with your local Mastra server is through our TypeScript/JavaScript Client SDK. Install it with: \n Then configure it to point to your local server: \n The client SDK provides type-safe wrappers for all API endpoints, making it much easier to develop and test your Mastra applications locally. \n UI Playground \n creates a UI with an agent chat interface, a workflow visualizer and a tool playground. \n OpenAPI Specification \n provides an OpenAPI spec at: \n \n \n \n Summary \n makes it easy to develop, debug, and iterate on your AI logic in a self-contained environment before deploying to production. \n \n Mastra Dev reference \n Client SDK documentation \n Creating Projects Integrations",
            "image": "https://mastra.ai/api/og/docs?title=Inspecting%20Agents%20with%20%60mastra%20dev%60%20%7C%20Mastra%20Local%20Dev%20Docs",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/deployment/overview",
            "title": "Deployment Overview",
            "url": "https://mastra.ai/docs/deployment/overview",
            "author": "",
            "text": "\n Mastra offers multiple deployment options to suit your application’s needs, from fully-managed solutions to self-hosted options. This guide will help you understand the available deployment paths and choose the right one for your project. \n Deployment Options \n Mastra Cloud \n Mastra Cloud is a deployment platform that connects to your GitHub repository, automatically deploys on code changes, and provides monitoring tools. It includes: \n \n GitHub repository integration \n Deployment on git push \n Agent testing interface \n Comprehensive logs and traces \n Custom domains for each project \n \n View Mastra Cloud documentation → \n With a Server \n You can deploy Mastra as a standard Node.js HTTP server, which gives you full control over your infrastructure and deployment environment. \n \n Custom API routes and middleware \n Configurable CORS and authentication \n Deploy to VMs, containers, or PaaS platforms \n Ideal for integrating with existing Node.js applications \n \n Server deployment guide → \n Serverless Platforms \n Mastra provides platform-specific deployers for popular serverless platforms, enabling you to deploy your application with minimal configuration. \n \n Deploy to Cloudflare Workers, Vercel, or Netlify \n Platform-specific optimizations \n Simplified deployment process \n Automatic scaling through the platform \n \n Serverless deployment guide → \n Client Configuration \n Once your Mastra application is deployed, you’ll need to configure your client to communicate with it. The Mastra Client SDK provides a simple and type-safe interface for interacting with your Mastra server. \n \n Type-safe API interactions \n Authentication and request handling \n Retries and error handling \n Support for streaming responses \n \n Client configuration guide → \n Choosing a Deployment Option \n Option Best For Key Benefits Mastra Cloud Teams wanting to ship quickly without infrastructure concerns Fully-managed, automatic scaling, built-in observability Server Deployment Teams needing maximum control and customization Full control, custom middleware, integrate with existing apps Serverless Platforms Teams already using Vercel, Netlify, or Cloudflare Platform integration, simplified deployment, automatic scaling Development Environment With a Server",
            "image": "https://mastra.ai/api/og/docs?title=Deployment%20Overview&description=Learn%20about%20different%20deployment%20options%20for%20your%20Mastra%20applications",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/deployment/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/deployment/server",
            "title": "Creating A Mastra Server",
            "url": "https://mastra.ai/docs/deployment/server",
            "author": "",
            "text": "\n While developing or when you deploy a Mastra application, it runs as an HTTP server that exposes your agents, workflows, and other functionality as API endpoints. This page explains how to configure and customize the server behavior. \n Server Architecture \n Mastra uses Hono  as its underlying HTTP server framework. When you build a Mastra application using mastra build, it generates a Hono-based HTTP server in the.mastra directory. \n The server provides: \n \n API endpoints for all registered agents \n API endpoints for all registered workflows \n Custom api route supports \n Custom middleware support \n Configuration of timeout \n Configuration of port \n Configuration of body limit \n \n See the Middleware and\n Custom API Routes pages for details on\nadding additional server behaviour. \n Server configuration \n You can configure server port and timeout in the Mastra instance. \n import { Mastra } from \" @mastra/core \"; \n \n export const mastra = new Mastra ({ \n server: { \n port: 3000, // Defaults to 4111 \n timeout: 10000, // Defaults to 30000 (30s) \n }, \n}); \n Custom CORS Config \n Mastra allows you to configure CORS (Cross-Origin Resource Sharing) settings for your server. \n import { Mastra } from ' @mastra/core '; \n \n export const mastra = new Mastra ({ \n server: { \n cors: { \n origin: [ ' https://example.com '], // Allow specific origins or '*' for all \n allowMethods: [ ' GET ', ' POST ', ' PUT ', ' DELETE ', ' OPTIONS '], \n allowHeaders: [ ' Content-Type ', ' Authorization '], \n credentials: false, \n } \n } \n}); \n Deployment \n Since Mastra builds to a standard Node.js server, you can deploy to any platform that runs Node.js applications: \n \n Cloud VMs (AWS EC2, DigitalOcean Droplets, GCP Compute Engine) \n Container platforms (Docker, Kubernetes) \n Platform as a Service (Heroku, Railway) \n Self-hosted servers \n \n Building \n Build the application: \n # Build from current directory \n mastra build \n \n # Or specify a directory \n mastra build --dir ./my-project \n The build process: \n \n Locates entry file ( src/mastra/index.ts or src/mastra/index.js) \n Creates.mastra output directory \n Bundles code using Rollup with tree shaking and source maps \n Generates Hono  HTTP server \n \n See mastra build for all options. \n Running the Server \n Start the HTTP server: \n node .mastra/output/index.mjs \n Enable Telemetry for build output \n Load instrumentation for the build output like so: \n node --import=./.mastra/output/instrumentation.mjs .mastra/output/index.mjs \n Serverless Deployment \n Mastra also supports serverless deployment on Cloudflare Workers, Vercel, and Netlify. \n See our Serverless Deployment guide for setup instructions. Overview Middleware",
            "image": "https://mastra.ai/api/og/docs?title=Creating%20A%20Mastra%20Server&description=Configure%20and%20customize%20the%20Mastra%20server%20with%20middleware%20and%20other%20options",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/deployment/server",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/deployment/deployment",
            "title": "Deployment",
            "url": "https://mastra.ai/docs/deployment/deployment",
            "author": "",
            "text": "Deploying Mastra Applications \n Mastra applications can be deployed in two ways: \n \n Direct Platform Deployment: Using platform-specific deployers for Cloudflare Workers, Vercel, or Netlify \n Universal Deployment: Using to generate a standard Node.js server that can run anywhere \n \n Prerequisites \n Before you begin, ensure you have: \n \n Node.js installed (version 18 or higher is recommended) \n If using a platform-specific deployer:\n \n An account with your chosen platform \n Required API keys or credentials \n \n \n \n Direct Platform Deployment \n Platform-specific deployers handle configuration and deployment for: \n \n Cloudflare Workers \n Vercel \n Netlify \n \n Installing Deployers \n Configuring Deployers \n Configure the deployer in your entry file: \n Deployer Configuration \n Cloudflare Deployer \n Vercel Deployer \n Netlify Deployer \n Universal Deployment \n Since Mastra builds to a standard Node.js server, you can deploy to any platform that runs Node.js applications: \n \n Cloud VMs (AWS EC2, DigitalOcean Droplets, GCP Compute Engine) \n Container platforms (Docker, Kubernetes) \n Platform as a Service (Heroku, Railway) \n Self-hosted servers \n \n Building \n Build the application: \n The build process: \n \n Locates entry file ( or ) \n Creates output directory \n Bundles code using Rollup with tree shaking and source maps \n Generates Hono HTTP server \n \n See for all options. \n Running the Server \n Start the HTTP server: \n Environment Variables \n Required variables: \n \n Platform deployer variables (if using platform deployers):\n \n Platform credentials \n \n \n Agent API keys:\n \n \n \n \n \n Server configuration (for universal deployment):\n \n: HTTP server port (default: 3000) \n: Server host (default: 0.0.0.0) \n \n \n \n Platform Documentation \n Platform deployment references: \n \n Cloudflare Workers \n Vercel \n Netlify \n Logging and Tracing Overview",
            "image": "https://mastra.ai/api/og/docs?title=Deployment",
            "extras": {
              "links": [
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples",
                "https://mastra.ai/showcase"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/deployment/client",
            "title": "MastraClient",
            "url": "https://mastra.ai/docs/deployment/client",
            "author": "",
            "text": "Mastra Client SDK \n The Mastra Client SDK provides a simple and type-safe interface for interacting with your Mastra Server from your client environment. \n Development Requirements \n To ensure smooth local development, make sure you have: \n \n Node.js 18.x or later installed \n TypeScript 4.7+ (if using TypeScript) \n A modern browser environment with Fetch API support \n Your local Mastra server running (typically on port 4111) \n \n Installation \n npm npm install @mastra/client-js@latest \n Initialize Mastra Client \n To get started you’ll need to initialize your MastraClient with necessary parameters: \n import { MastraClient } from \" @mastra/client-js \"; \n \n const client = new MastraClient ({ \n baseUrl: \" http://localhost:4111 \", // Default Mastra development server port \n}); \n Configuration Options \n You can customize the client with various options: \n const client = new MastraClient ({ \n // Required \n baseUrl: \" http://localhost:4111 \", \n \n // Optional configurations for development \n retries: 3, // Number of retry attempts \n backoffMs: 300, // Initial retry backoff time \n maxBackoffMs: 5000, // Maximum retry backoff time \n headers: { // Custom headers for development \n \" X-Development \": \" true \" \n } \n}); \n Example \n Once your MastraClient is initialized you can start making client calls via the type-safe\ninterface \n // Get a reference to your local agent \n const agent = client. getAgent ( \" dev-agent-id \"); \n \n // Generate responses \n const response = await agent. generate ({ \n messages: [ \n { \n role: \" user \", \n content: \" Hello, I'm testing the local development setup! \" \n } \n ] \n}); \n Available Features \n Mastra client exposes all resources served by the Mastra Server \n \n Agents: Create and manage AI agents, generate responses, and handle streaming interactions \n Memory: Manage conversation threads and message history \n Tools: Access and execute tools available to agents \n Workflows: Create and manage automated workflows \n Vectors: Handle vector operations for semantic search and similarity matching \n \n Best Practices \n \n Error Handling: Implement proper error handling for development scenarios \n Environment Variables: Use environment variables for configuration \n Debugging: Enable detailed logging when needed \n \n // Example with error handling and logging \n try { \n const agent = client. getAgent ( \" dev-agent-id \"); \n const response = await agent. generate ({ \n messages: [{ role: \" user \", content: \" Test message \" }] \n }); \n console. log ( \" Response: \", response); \n} catch (error) { \n console. error ( \" Development error: \", error); \n} Serverless Platforms Overview",
            "image": "https://mastra.ai/api/og/docs?title=MastraClient&description=Learn%20how%20to%20set%20up%20and%20use%20the%20Mastra%20Client%20SDK",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/deployment/client",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/mastra-cloud/overview",
            "title": "Mastra Cloud",
            "url": "https://mastra.ai/docs/mastra-cloud/overview",
            "author": "",
            "text": "\n Mastra Cloud is a deployment service that runs, manages, and monitors Mastra applications. It works with standard Mastra projects and handles deployment, scaling, and operational tasks. \n Core Functionality \n \n Atomic Deployments - Agents and workflows deploy as a single unit \n Project Organization - Group agents and workflows into projects with assigned URLs \n Environment Variables - Store configuration securely by environment \n Testing Console - Send messages to agents through a web interface \n Execution Tracing - Record agent interactions and tool calls \n Workflow Visualization - Display workflow steps and execution paths \n Logs - Standard logging output for debugging \n Platform Compatibility - Uses the same infrastructure as Cloudflare, Vercel, and Netlify deployers \n \n Dashboard Components \n The Mastra Cloud dashboard contains: \n \n Projects List - All projects in the account \n Project Details - Deployments, environment variables, and access URLs \n Deployment History - Record of deployments with timestamps and status \n Agent Inspector - Agent configuration view showing models, tools, and system prompts \n Testing Console - Interface for sending messages to agents \n Trace Explorer - Records of tool calls, parameters, and responses \n Workflow Viewer - Diagram of workflow steps and connections \n \n Technical Implementation \n Mastra Cloud runs on the same core code as the platform-specific deployers with these modifications: \n \n Edge Network Distribution - Geographically distributed execution \n Dynamic Resource Allocation - Adjusts compute resources based on traffic \n Mastra-specific Runtime - Runtime optimized for agent execution \n Standard Deployment API - Consistent deployment interface across environments \n Tracing Infrastructure - Records all agent and workflow execution steps \n \n Use Cases \n Common usage patterns: \n \n Deploying applications without managing infrastructure \n Maintaining staging and production environments \n Monitoring agent behavior across many requests \n Testing agent responses through a web interface \n Deploying to multiple regions \n \n Setup Process \n \n Configure a Mastra Cloud project \n Deploy code \n View execution traces \n The Mastra Client Setting Up a Project",
            "image": "https://mastra.ai/api/og/docs?title=Mastra%20Cloud&description=Deployment%20and%20monitoring%20service%20for%20Mastra%20applications",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/mastra-cloud/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/mastra-cloud/setting-up",
            "title": "Setting Up a Project",
            "url": "https://mastra.ai/docs/mastra-cloud/setting-up",
            "publishedDate": "2000-01-01T00:00:00.000Z",
            "author": "",
            "text": "Setting Up a Mastra Cloud Project \n This page describes the steps to set up a project on Mastra Cloud using GitHub integration. \n Prerequisites \n \n A Mastra Cloud account \n A GitHub account \n A GitHub repository containing a Mastra application \n \n Project Creation Process \n \n \n Sign in to Mastra Cloud \n \n Navigate to the Mastra Cloud dashboard at https://cloud.mastra.ai  \n Sign in with your account credentials \n \n \n \n Add a New Project \n \n From the “All Projects” view, click the “Add new” button in the top right \n This opens the GitHub repository import dialog \n \n \n \n \n Import Git Repository \n \n Search for repositories or select from the list of available GitHub repositories \n Click the “Import” button next to the repository you want to deploy \n \n \n \n Configure Deployment Details \nThe deployment configuration page includes: \n \n Repo Name: The GitHub repository name (read-only) \n Project Name: Customize the project name (defaults to repo name) \n Branch: Select the branch to deploy (dropdown, defaults to main) \n Project root: Set the root directory of your project (defaults to /) \n Mastra Directory: Specify where Mastra files are located (defaults to src/mastra) \n Build Command: Optional command to run during build process \n Store Settings: Configure data storage options \n Environment Variables: Add key-value pairs for configuration (e.g., API keys) \n \n \n \n Project Structure Requirements \n Mastra Cloud scans the GitHub repository for: \n \n Agents: Agent definitions (e.g., Weather Agent) with models and tools \n Workflows: Workflow step definitions (e.g., weather-workflow) \n Environment Variables: Required API keys and configuration variables \n \n The repository should contain a standard Mastra project structure for proper detection and deployment. \n Understanding the Dashboard \n After creating a project, the dashboard shows: \n Project Overview \n \n Created Date: When the project was created \n Domains: URLs for accessing your deployed application\n \n Format: https://[project-name].mastra.cloud \n Format: https://[random-id].mastra.cloud \n \n \n Status: Current deployment status (success or archived) \n Branch: The branch deployed (typically main) \n Environment Variables: Configured API keys and settings \n Workflows: List of detected workflows with step counts \n Agents: List of detected agents with models and tools \n Database Usage: Reads, writes, and storage statistics \n \n Deployments Section \n \n List of all deployments with:\n \n Deployment ID (based on commit hash) \n Status (success/archived) \n Branch \n Commit hash \n Timestamp \n \n \n \n Logs Section \n The Logs view displays: \n \n Timestamp for each log entry \n Log level (info, debug) \n Hostname \n Detailed log messages, including:\n \n API startup information \n Storage initialization \n Agent and workflow activity \n \n \n \n Navigation \n The sidebar provides access to: \n \n Overview: Project summary and statistics \n Deployments: Deployment history and details \n Logs: Application logs for debugging \n Agents: List and configuration of all agents \n Workflows: List and structure of all workflows \n Settings: Project configuration options \n \n Environment Variable Configuration \n Set environment variables through the dashboard: \n \n Navigate to your project in the dashboard \n Go to the “Environment Variables” section \n Add or edit variables (such as OPENAI_API_KEY) \n Save the configuration \n \n Environment variables are encrypted and made available to your application during deployment and execution. \n Testing Your Deployment \n After deployment, you can test your agents and workflows using: \n \n The custom domain assigned to your project: https://[project-name].mastra.cloud \n The dashboard interface for direct interaction with agents \n \n Next Steps \n After setting up your project, automatic deployments occur whenever you push to the main branch of your GitHub repository. See the deployment documentation for more details. Overview Deploying",
            "image": "https://mastra.ai/api/og/docs?title=Setting%20Up%20a%20Project&description=Configuration%20steps%20for%20Mastra%20Cloud%20projects",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/mastra-cloud/setting-up",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/mastra-cloud/deploying",
            "title": "Deploying to Mastra Cloud",
            "url": "https://mastra.ai/docs/mastra-cloud/deploying",
            "publishedDate": "2000-01-01T00:00:00.000Z",
            "author": "",
            "text": "\n This page describes the deployment process for Mastra applications to Mastra Cloud using GitHub integration. \n Prerequisites \n \n A GitHub account \n A GitHub repository containing a Mastra application \n Access to Mastra Cloud \n \n Deployment Process \n Mastra Cloud uses a Git-based deployment workflow similar to platforms like Vercel and Netlify: \n \n \n Import GitHub Repository \n \n From the Projects dashboard, click “Add new” \n Select the repository containing your Mastra application \n Click “Import” next to the desired repository \n \n \n \n Configure Deployment Settings \n \n Set the project name (defaults to repository name) \n Select branch to deploy (typically main) \n Configure the Mastra directory path (defaults to src/mastra) \n Add necessary environment variables (like API keys) \n \n \n \n Deploy from Git \n \n After initial configuration, deployments are triggered by pushes to the selected branch \n Mastra Cloud automatically builds and deploys your application \n Each deployment creates an atomic snapshot of your agents and workflows \n \n \n \n Automatic Deployments \n Mastra Cloud follows a Git-driven workflow: \n \n Make changes to your Mastra application locally \n Commit changes to the main branch \n Push to GitHub \n Mastra Cloud automatically detects the push and creates a new deployment \n Once the build completes, your application is live \n \n Deployment Domains \n Each project receives two URLs: \n \n \n Project-specific domain: https://[project-name].mastra.cloud \n \n Example: https://gray-acoustic-helicopter.mastra.cloud \n \n \n \n Deployment-specific domain: https://[deployment-id].mastra.cloud \n \n Example: https://young-loud-caravan-6156280f-ad56-4ec8-9701-6bb5271fd73d.mastra.cloud \n \n \n \n These URLs provide direct access to your deployed agents and workflows. \n Viewing Deployments \n \n The deployments section in the dashboard shows: \n \n Title: Deployment identifier (based on commit hash) \n Status: Current state (success or archived) \n Branch: The branch used (typically main) \n Commit: The Git commit hash \n Updated At: Timestamp of the deployment \n \n Each deployment represents an atomic snapshot of your Mastra application at a specific point in time. \n Interacting with Agents \n \n After deployment, interact with your agents: \n \n Navigate to your project in the dashboard \n Go to the Agents section \n Select an agent to view its details and interface \n Use the Chat tab to communicate with your agent \n View the agent’s configuration in the right panel:\n \n Model information (e.g., OpenAI) \n Available tools (e.g., getWeather) \n Complete system prompt \n \n \n Use suggested prompts (like “What capabilities do you have?”) or enter custom messages \n \n The interface shows the agent’s branch (typically “main”) and indicates whether conversation memory is enabled. \n Monitoring Logs \n The Logs section provides detailed information about your application: \n \n Time: When the log entry was created \n Level: Log level (info, debug) \n Hostname: Server identification \n Message: Detailed log information, including:\n \n API initialization \n Storage connections \n Agent and workflow activity \n \n \n \n These logs help debug and monitor your application’s behavior in the production environment. \n Workflows \n \n The Workflows section allows you to view and interact with your deployed workflows: \n \n View all workflows in your project \n Examine workflow structure and steps \n Access execution history and performance data \n \n Database Usage \n Mastra Cloud tracks database utilization metrics: \n \n Number of reads \n Number of writes \n Storage used (MB) \n \n These metrics appear in the project overview, helping you monitor resource consumption. \n Deployment Configuration \n Configure your deployment through the dashboard: \n \n Navigate to your project settings \n Set environment variables (like OPENAI_API_KEY) \n Configure project-specific settings \n \n Changes to configuration require a new deployment to take effect. \n Next Steps \n After deployment, trace and monitor execution using the observability tools. Setting Up a Project Observability",
            "image": "https://mastra.ai/api/og/docs?title=Deploying%20to%20Mastra%20Cloud&description=GitHub-based%20deployment%20process%20for%20Mastra%20applications",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/mastra-cloud/deploying",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/mastra-cloud/observability",
            "title": "Observability in Mastra Cloud",
            "url": "https://mastra.ai/docs/mastra-cloud/observability",
            "publishedDate": "2000-01-01T00:00:00.000Z",
            "author": "",
            "text": "\n Mastra Cloud records execution data for monitoring and debugging. It captures traces, logs, and runtime information from agents and workflows. \n Agent Interface \n The agent interface offers three main views, accessible via tabs: \n \n Chat: Interactive messaging interface to test your agent \n Traces: Detailed execution records \n Evaluation: Agent performance assessment \n \n \n Chat Interface \n The Chat tab provides: \n \n Interactive messaging with deployed agents \n System response to user queries \n Suggested prompt buttons (e.g., “What capabilities do you have?”) \n Message input area \n Branch indicator (e.g., “main”) \n Note about agent memory limitations \n \n Agent Configuration Panel \n The right sidebar displays agent details: \n \n Agent name and deployment identifier \n Model information (e.g., “OpenAI”) \n Tools available to the agent (e.g., “getWeather”) \n Complete system prompt text \n \n This panel provides visibility into how the agent is configured without needing to check the source code. \n Trace System \n Mastra Cloud records traces for agent and workflow interactions. \n Trace Explorer Interface \n \n The Trace Explorer interface shows: \n \n All agent and workflow interactions \n Specific trace details \n Input and output data \n Tool calls with parameters and results \n Workflow execution paths \n Filtering options by type, status, timestamp, and agent/workflow \n \n Trace Data Structure \n Each trace contains: \n \n Request Data: The request that initiated the agent or workflow \n Tool Call Records: Tool calls during execution with parameters \n Tool Response Data: The responses from tool calls \n Agent Response Data: The generated agent response \n Execution Timestamps: Timing information for each execution step \n Model Metadata: Information about model usage and tokens \n \n The trace view displays all API calls and results throughout execution. This data helps debug tool usage and agent logic flows. \n Agent Interaction Data \n Agent interaction traces include: \n \n User input text \n Agent processing steps \n Tool calls (e.g., weather API calls) \n Parameters and results for each tool call \n Final agent response text \n \n Dashboard Structure \n The Mastra Cloud dashboard contains: \n \n Project deployment history \n Environment variable configuration \n Agent configuration details (model, system prompt, tools) \n Workflow step visualization \n Deployment URLs \n Recent activity log \n \n Agent Testing \n Test your agents using the Chat interface: \n \n Navigate to the Agents section \n Select the agent you want to test \n Use the Chat tab to interact with your agent \n Send messages and view responses \n Use suggested prompts for common queries \n Switch to the Traces tab to view execution details \n \n Note that by default, agents do not remember conversation history across sessions. The interface indicates this with the message: “Agent will not remember previous messages. To enable memory for agent see image.” \n Workflow Monitoring \n \n Workflow monitoring shows: \n \n Diagram of workflow steps and connections \n Status for each workflow step \n Execution details for each step \n Execution trace records \n Multi-step process execution (e.g., weather lookup followed by activity planning) \n \n Workflow Execution \n \n When examining a specific workflow execution, you can see the detailed steps and their outputs. \n Logs \n \n The Logs section provides detailed information about your application: \n \n Time: When the log entry was created \n Level: Log level (info, debug) \n Hostname: Server identification \n Message: Detailed log information, including:\n \n API initialization \n Storage connections \n Agent and workflow activity \n \n \n \n Technical Features \n The observability system includes: \n \n API Endpoints: For programmatic access to trace data \n Structured Trace Format: JSON format for filtering and query operations \n Historical Data Storage: Retention of past execution records \n Deployment Version Links: Correlation between traces and deployment versions \n \n Debugging Patterns \n \n Compare trace data when testing agent behavior changes \n Use the chat interface to test edge case inputs \n View system prompts to understand agent behavior \n Examine tool call parameters and results \n Verify workflow execution step sequencing \n Identify execution bottlenecks in trace timing data \n Compare trace differences between agent versions \n \n Support Resources \n For technical assistance with observability: \n \n Review the Troubleshooting Documentation \n Contact technical support through the dashboard \n Join the Discord developer channel  \n Deploying Logging",
            "image": "https://mastra.ai/api/og/docs?title=Observability%20in%20Mastra%20Cloud&description=Monitoring%20and%20debugging%20tools%20for%20Mastra%20Cloud%20deployments",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/mastra-cloud/observability",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/observability/logging",
            "title": "Logging | Mastra Observability Documentation",
            "url": "https://mastra.ai/docs/observability/logging",
            "publishedDate": "",
            "author": "",
            "text": "\n In Mastra, logs can detail when certain functions run, what input data they receive, and how they respond. \n Basic Setup \n Here’s a minimal example that sets up a console logger at the INFO level. This will print out informational messages and above (i.e., DEBUG, INFO, WARN, ERROR) to the console. \n import { Mastra } from \" @mastra/core \"; \n import { createLogger } from \" @mastra/core/logger \"; \n \n export const mastra = new Mastra ({ \n // Other Mastra configuration... \n logger: createLogger ({ \n name: \" Mastra \", \n level: \" info \", \n }), \n}); \n In this configuration: \n \n name: \"Mastra\" specifies the name to group logs under. \n level: \"info\" sets the minimum severity of logs to record. \n \n Configuration \n \n For more details on the options you can pass to createLogger(), see the createLogger reference documentation. \n Once you have a Logger instance, you can call its methods (e.g.,.info(),.warn(),.error()) in the Logger instance reference documentation. \n If you want to send your logs to an external service for centralized collection, analysis, or storage, you can configure other logger types such as Upstash Redis. Consult the createLogger reference documentation for details on parameters like url, token, and key when using the UPSTASH logger type. \n Observability Tracing",
            "image": "https://mastra.ai/api/og/docs?title=Logging%20|%20Mastra%20Observability%20Documentation&description=Documentation%20on%20effective%20logging%20in%20Mastra,%20crucial%20for%20understanding%20application%20behavior%20and%20improving%20AI%20accuracy.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/observability/logging",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/observability/tracing",
            "title": "Tracing | Mastra Observability Documentation",
            "url": "https://mastra.ai/docs/observability/tracing",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "\n Mastra supports the OpenTelemetry Protocol (OTLP) for tracing and monitoring your application. When telemetry is enabled, Mastra automatically traces all core primitives including agent operations, LLM interactions, tool executions, integration calls, workflow runs, and database operations. Your telemetry data can then be exported to any OTEL collector. \n Basic Configuration \n Here’s a simple example of enabling telemetry: \n export const mastra = new Mastra ({ \n // ... other config \n telemetry: { \n serviceName: \" my-app \", \n enabled: true, \n sampling: { \n type: \" always_on \", \n }, \n export: { \n type: \" otlp \", \n endpoint: \" http://localhost:4318 \", // SigNoz local endpoint \n }, \n }, \n}); \n Configuration Options \n The telemetry config accepts these properties: \n type OtelConfig = { \n // Name to identify your service in traces (optional) \n serviceName?: string; \n \n // Enable/disable telemetry (defaults to true) \n enabled?: boolean; \n \n // Control how many traces are sampled \n sampling?: { \n type: \" ratio \" | \" always_on \" | \" always_off \" | \" parent_based \"; \n probability?: number; // For ratio sampling \n root?: { \n probability: number; // For parent_based sampling \n }; \n }; \n \n // Where to send telemetry data \n export?: { \n type: \" otlp \" | \" console \"; \n endpoint?: string; \n headers?: Record &lt; string, string &gt;; \n }; \n}; \n See the OtelConfig reference documentation for more details. \n Environment Variables \n You can configure the OTLP endpoint and headers through environment variables: \n.env OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 \n OTEL_EXPORTER_OTLP_HEADERS=x-api-key=your-api-key \n Then in your config: \n export const mastra = new Mastra ({ \n // ... other config \n telemetry: { \n serviceName: \" my-app \", \n enabled: true, \n export: { \n type: \" otlp \", \n // endpoint and headers will be picked up from env vars \n }, \n }, \n}); \n Example: SigNoz Integration \n Here’s what a traced agent interaction looks like in SigNoz : \n Other Supported Providers \n For a complete list of supported observability providers and their configuration details, see the Observability Providers reference. \n Next.js-specific Tracing steps \n If you’re using Next.js, you have three additional configuration steps: \n \n Enable the instrumentation hook in next.config.ts \n Configure Mastra telemetry settings \n Set up an OpenTelemetry exporter \n \n For implementation details, see the Next.js Tracing guide. Logging Overview",
            "image": "https://mastra.ai/api/og/docs?title=Tracing%20|%20Mastra%20Observability%20Documentation&description=Set%20up%20OpenTelemetry%20tracing%20for%20Mastra%20applications",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/observability/tracing",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/evals/overview",
            "title": "Overview",
            "url": "https://mastra.ai/docs/evals/overview",
            "publishedDate": "",
            "author": "",
            "text": "Testing your agents with evals \n While traditional software tests have clear pass/fail conditions, AI outputs are non-deterministic — they can vary with the same input. Evals help bridge this gap by providing quantifiable metrics for measuring agent quality. \n Evals are automated tests that evaluate Agents outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions. \n Evals can be run in the cloud, capturing real-time results. But evals can also be part of your CI/CD pipeline, allowing you to test and monitor your agents over time. \n Types of Evals \n There are different kinds of evals, each serving a specific purpose. Here are some common types: \n \n Textual Evals: Evaluate accuracy, reliability, and context understanding of agent responses \n Classification Evals: Measure accuracy in categorizing data based on predefined categories \n Tool Usage Evals: Assess how effectively an agent uses external tools or APIs \n Prompt Engineering Evals: Explore impact of different instructions and input formats \n \n Getting Started \n Evals need to be added to an agent. Here’s an example using the summarization, content similarity, and tone consistency metrics: \n src/mastra/agents/index.ts import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n import { SummarizationMetric } from \" @mastra/evals/llm \"; \n import { \n ContentSimilarityMetric, \n ToneConsistencyMetric, \n} from \" @mastra/evals/nlp \"; \n \n const model = openai ( \" gpt-4o \"); \n \n export const myAgent = new Agent ({ \n name: \" ContentWriter \", \n instructions: \" You are a content writer that creates accurate summaries \", \n model, \n evals: { \n summarization: new SummarizationMetric (model), \n contentSimilarity: new ContentSimilarityMetric (), \n tone: new ToneConsistencyMetric (), \n }, \n}); \n You can view eval results in the Mastra dashboard when using mastra dev. \n Beyond Automated Testing \n While automated evals are valuable, high-performing AI teams often combine them with: \n \n A/B Testing: Compare different versions with real users \n Human Review: Regular review of production data and traces \n Continuous Monitoring: Track eval metrics over time to detect regressions \n \n Understanding Eval Results \n Each eval metric measures a specific aspect of your agent’s output. Here’s how to interpret and improve your results: \n Understanding Scores \n For any metric: \n \n Check the metric documentation to understand the scoring process \n Look for patterns in when scores change \n Compare scores across different inputs and contexts \n Track changes over time to spot trends \n \n Improving Results \n When scores aren’t meeting your targets: \n \n Check your instructions - Are they clear? Try making them more specific \n Look at your context - Is it giving the agent what it needs? \n Simplify your prompts - Break complex tasks into smaller steps \n Add guardrails - Include specific rules for tricky cases \n \n Maintaining Quality \n Once you’re hitting your targets: \n \n Monitor stability - Do scores remain consistent? \n Document what works - Keep notes on successful approaches \n Test edge cases - Add examples that cover unusual scenarios \n Fine-tune - Look for ways to improve efficiency \n \n See Textual Evals for more info on what evals can do. \n For more info on how to create your own evals, see the Custom Evals guide. \n For running evals in your CI pipeline, see the Running in CI guide. Tracing Textual Evals",
            "image": "https://mastra.ai/api/og/docs?title=Overview&description=Understanding%20how%20to%20evaluate%20and%20measure%20AI%20agent%20quality%20using%20Mastra%20evals.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/evals/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/evals/textual-evals",
            "title": "Textual Evals",
            "url": "https://mastra.ai/docs/evals/textual-evals",
            "publishedDate": "",
            "author": "",
            "text": "\n Textual evals use an LLM-as-judge methodology to evaluate agent outputs. This approach leverages language models to assess various aspects of text quality, similar to how a teaching assistant might grade assignments using a rubric. \n Each eval focuses on specific quality aspects and returns a score between 0 and 1, providing quantifiable metrics for non-deterministic AI outputs. \n Mastra provides several eval metrics for assessing Agent outputs. Mastra is not limited to these metrics, and you can also define your own evals. \n Why Use Textual Evals? \n Textual evals help ensure your agent: \n \n Produces accurate and reliable responses \n Uses context effectively \n Follows output requirements \n Maintains consistent quality over time \n \n Available Metrics \n Accuracy and Reliability \n These metrics evaluate how correct, truthful, and complete your agent’s answers are: \n \n hallucination: Detects facts or claims not present in provided context \n faithfulness: Measures how accurately responses represent provided context \n content-similarity: Evaluates consistency of information across different phrasings \n completeness: Checks if responses include all necessary information \n answer-relevancy: Assesses how well responses address the original query \n textual-difference: Measures textual differences between strings \n \n Understanding Context \n These metrics evaluate how well your agent uses provided context: \n \n context-position: Analyzes where context appears in responses \n context-precision: Evaluates whether context chunks are grouped logically \n context-relevancy: Measures use of appropriate context pieces \n contextual-recall: Assesses completeness of context usage \n \n Output Quality \n These metrics evaluate adherence to format and style requirements: \n \n tone: Measures consistency in formality, complexity, and style \n toxicity: Detects harmful or inappropriate content \n bias: Detects potential biases in the output \n prompt-alignment: Checks adherence to explicit instructions like length restrictions, formatting requirements, or other constraints \n summarization: Evaluates information retention and conciseness \n keyword-coverage: Assesses technical terminology usage \n Overview Custom Evals",
            "image": "https://mastra.ai/api/og/docs?title=Textual%20Evals&description=Understand%20how%20Mastra%20uses%20LLM-as-judge%20methodology%20to%20evaluate%20text%20quality.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/evals/textual-evals",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/evals/custom-eval",
            "title": "Create your own Eval",
            "url": "https://mastra.ai/docs/evals/custom-eval",
            "publishedDate": "2005-03-03T00:00:00.000Z",
            "author": "",
            "text": "\n Creating your own eval is as easy as creating a new function. You simply create a class that extends the Metric class and implement the measure method. \n Basic example \n For a simple example of creating a custom metric that checks if the output contains certain words, see our Word Inclusion example. \n Creating a custom LLM-Judge \n A custom LLM judge helps evaluate specific aspects of your AI’s responses. Think of it like having an expert reviewer for your particular use case: \n \n Medical Q&amp;A → Judge checks for medical accuracy and safety \n Customer Service → Judge evaluates tone and helpfulness \n Code Generation → Judge verifies code correctness and style \n \n For a practical example, see how we evaluate Chef Michel’s recipes for gluten content in our Gluten Checker example. Textual Evals Running in CI",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/docs/evals/custom-eval",
                "https://mastra.ai/",
                "https://mastra.ai/blog",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/evals/running-in-ci",
            "title": "Running in CI",
            "url": "https://mastra.ai/docs/evals/running-in-ci",
            "publishedDate": "",
            "author": "",
            "text": "Running Evals in CI \n Running evals in your CI pipeline helps bridge this gap by providing quantifiable metrics for measuring agent quality over time. \n Setting Up CI Integration \n We support any testing framework that supports ESM modules. For example, you can use Vitest , Jest  or Mocha  to run evals in your CI/CD pipeline. \n src/mastra/agents/index.test.ts import { describe, it, expect } from ' vitest '; \n import { evaluate } from \" @mastra/evals \"; \n import { ToneConsistencyMetric } from \" @mastra/evals/nlp \"; \n import { myAgent } from './index '; \n \n describe ( ' My Agent ', () =&gt; { \n it ( ' should validate tone consistency ', async () =&gt; { \n const metric = new ToneConsistencyMetric (); \n const result = await evaluate (myAgent, ' Hello, world! ', metric) \n \n expect (result.score). toBe ( 1); \n }); \n}); \n You will need to configure a testSetup and globalSetup script for your testing framework to capture the eval results. It allows us to show these results in your mastra dashboard. \n Framework Configuration \n Vitest Setup \n Add these files to your project to run evals in your CI/CD pipeline: \n import { globalSetup } from ' @mastra/evals '; \n \n export default function setup () { \n globalSetup () \n} \n import { beforeAll } from ' vitest '; \n import { attachListeners } from ' @mastra/evals '; \n \n beforeAll ( async () =&gt; { \n await attachListeners (); \n}); \n import { defineConfig } from ' vitest/config ' \n \n export default defineConfig ({ \n test: { \n globalSetup: './globalSetup.ts ', \n setupFiles: [ './testSetup.ts '], \n }, \n}) \n Storage Configuration \n To store eval results in Mastra Storage and capture results in the Mastra dashboard: \n import { beforeAll } from ' vitest '; \n import { attachListeners } from ' @mastra/evals '; \n import { mastra } from './your-mastra-setup '; \n \n beforeAll ( async () =&gt; { \n // Store evals in Mastra Storage (requires storage to be enabled) \n await attachListeners (mastra); \n}); \n With file storage, evals persist and can be queried later. With memory storage, evals are isolated to the test process. Custom Evals With Vercel AI SDK",
            "image": "https://mastra.ai/api/og/docs?title=Running%20in%20CI&description=Learn%20how%20to%20run%20Mastra%20evals%20in%20your%20CI/CD%20pipeline%20to%20monitor%20agent%20quality%20over%20time.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/evals/running-in-ci",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/frameworks/ai-sdk",
            "title": "Using with Vercel AI SDK",
            "url": "https://mastra.ai/docs/frameworks/ai-sdk",
            "author": "",
            "text": "\n Mastra leverages AI SDK’s model routing (a unified interface on top of OpenAI, Anthropic, etc), structured output, and tool calling. \n We explain this in greater detail in this blog post  \n Mastra + AI SDK \n Mastra acts as a layer on top of AI SDK to help teams productionize their proof-of-concepts quickly and easily. \n Model routing \n When creating agents in Mastra, you can specify any AI SDK-supported model: \n import { openai } from \" @ai-sdk/openai \"; \n import { Agent } from \" @mastra/core/agent \"; \n \n const agent = new Agent ({ \n name: \" WeatherAgent \", \n instructions: \" Instructions for the agent... \", \n model: openai ( \" gpt-4-turbo \"), // Model comes directly from AI SDK \n}); \n \n const result = await agent. generate ( \" What is the weather like? \"); \n AI SDK Hooks \n Mastra is compatible with AI SDK’s hooks for seamless frontend integration: \n useChat \n The useChat hook enables real-time chat interactions in your frontend application \n \n Works with agent data streams i.e..toDataStreamResponse() \n The useChat api defaults to /api/chat \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for data streams,\ni.e. no structured output is defined. \n \n import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages); \n \n return stream. toDataStreamResponse (); \n} \n import { useChat } from ' @ai-sdk/react '; \n \n export function ChatComponent () { \n const { messages, input, handleInputChange, handleSubmit } = useChat ({ \n api: ' /path-to-your-agent-stream-api-endpoint ' \n }); \n \n return ( \n &lt; div &gt; \n {messages. map ( m =&gt; ( \n &lt; div key = { m. id} &gt; \n {m. role}: { m.content} \n &lt;/ div &gt; \n ))} \n &lt; form onSubmit = {handleSubmit} &gt; \n &lt; input \n value = {input} \n onChange = {handleInputChange} \n placeholder = \" Say something... \" \n /&gt; \n &lt;/ form &gt; \n &lt;/ div &gt; \n ); \n} \n \n Gotcha: When using useChat with agent memory functionality, make sure to check out the Agent Memory section for important implementation details. \n \n useCompletion \n For single-turn completions, use the useCompletion hook: \n \n Works with agent data streams i.e..toDataStreamResponse() \n The useCompletion api defaults to /api/completion \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for data streams,\ni.e. no structured output is defined. \n \n app/api/completion/route.ts import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages); \n \n return stream. toDataStreamResponse (); \n} \n import { useCompletion } from \" @ai-sdk/react \"; \n \n export function CompletionComponent () { \n const { \n completion, \n input, \n handleInputChange, \n handleSubmit, \n } = useCompletion ({ \n api: ' /path-to-your-agent-stream-api-endpoint ' \n }); \n \n return ( \n &lt; div &gt; \n &lt; form onSubmit = {handleSubmit} &gt; \n &lt; input \n value = {input} \n onChange = {handleInputChange} \n placeholder = \" Enter a prompt... \" \n /&gt; \n &lt;/ form &gt; \n &lt; p &gt;Completion result: {completion}&lt; / p &gt; \n &lt;/ div &gt; \n ); \n} \n useObject \n For consuming text streams that represent JSON objects and parsing them into a complete object based on a schema. \n \n Works with agent text streams i.e..toTextStreamResponse() \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for text streams,\ni.e. structured output is defined. \n \n app/api/use-object/route.ts import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages, { \n output: z. object ({ \n weather: z. string (), \n }), \n }); \n \n return stream. toTextStreamResponse (); \n} \n import { experimental_useObject as useObject } from ' @ai-sdk/react '; \n \n export default function Page () { \n const { object, submit } = useObject ({ \n api: ' /api/use-object ', \n schema: z. object ({ \n weather: z. string (), \n }), \n }); \n \n return ( \n &lt; div &gt; \n &lt; button onClick = { () =&gt; submit ( ' example input ')} &gt; Generate &lt;/ button &gt; \n {object?. content &amp;&amp; &lt; p &gt; {object. content} &lt;/ p &gt;} \n &lt;/ div &gt; \n ); \n} \n Tool Calling \n AI SDK Tool Format \n Mastra supports tools created using the AI SDK format, so you can use\nthem directly with Mastra agents. See our tools doc on Vercel AI SDK Tool Format\n for more details. \n Client-side tool calling \n Mastra leverages AI SDK’s tool calling, so what applies in AI SDK applies here still.\n Agent Tools in Mastra are 100% percent compatible with AI SDK tools. \n Mastra tools also expose an optional execute async function. It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process. \n One way to then leverage client-side tool calling is to use the @ai-sdk/react useChat hook’s onToolCall property for\nclient-side tool execution \n Custom DataStream \n In certain scenarios you need to write custom data, message annotations to an agent’s dataStream.\nThis can be useful for: \n \n Streaming additional data to the client \n Passing progress info back to the client in real time \n \n Mastra integrates well with AI SDK to make this possible \n CreateDataStream \n The createDataStream function allows you to stream additional data to the client \n import { createDataStream } from \" ai \" \n import { Agent } from ' @mastra/core/agent '; \n \n export const weatherAgent = new Agent ({ \n name: ' Weather Agent ', \n instructions: ` \n You are a helpful weather assistant that provides accurate weather information. \n \n Your primary function is to help users get weather details for specific locations. When responding: \n - Always ask for a location if none is provided \n - If the location name isn't in English, please translate it \n - If giving a location with multiple parts (e.g. \"New York, NY\"), use the most relevant part (e.g. \"New York\") \n - Include relevant details like humidity, wind conditions, and precipitation \n - Keep responses concise but informative \n \n Use the weatherTool to fetch current weather data. \n `, \n model: openai ( ' gpt-4o '), \n tools: { weatherTool }, \n }); \n \n const stream = createDataStream ({ \n async execute (dataStream) { \n // Write data \n dataStream. writeData ({ value: ' Hello ' }); \n \n // Write annotation \n dataStream. writeMessageAnnotation ({ type: ' status ', value: ' processing ' }); \n \n //mastra agent stream \n const agentStream = await weatherAgent. stream ( ' What is the weather ') \n \n // Merge agent stream \n agentStream. mergeIntoDataStream (dataStream); \n }, \n onError: error =&gt; ` Custom error: ${ error.message}`, \n }); \n \n CreateDataStreamResponse \n The createDataStreamResponse function creates a Response object that streams data to the client \n import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n //mastra agent stream \n const agentStream = await myAgent. stream (messages); \n \n const response = createDataStreamResponse ({ \n status: 200, \n statusText: ' OK ', \n headers: { \n ' Custom-Header ': ' value ', \n }, \n async execute (dataStream) { \n // Write data \n dataStream. writeData ({ value: ' Hello ' }); \n \n // Write annotation \n dataStream. writeMessageAnnotation ({ type: ' status ', value: ' processing ' }); \n \n // Merge agent stream \n agentStream. mergeIntoDataStream (dataStream); \n }, \n onError: error =&gt; ` Custom error: ${ error.message}`, \n }); \n \n return response \n}",
            "image": "https://mastra.ai/api/og/docs?title=Using%20with%20Vercel%20AI%20SDK&description=Learn%20how%20Mastra%20leverages%20the%20Vercel%20AI%20SDK%20library%20and%20how%20you%20can%20leverage%20it%20further%20with%20Mastra",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/frameworks/ai-sdk",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/frameworks/next-js",
            "title": "Getting started with Mastra and NextJS | Mastra Guides",
            "url": "https://mastra.ai/docs/frameworks/next-js",
            "publishedDate": "",
            "author": "",
            "text": "Integrate Mastra in your Next.js project \n There are two main ways to integrate Mastra with your Next.js application: as a separate backend service or directly integrated into your Next.js app. \n 1. Separate Backend Integration \n Best for larger projects where you want to: \n \n Scale your AI backend independently \n Keep clear separation of concerns \n Have more deployment flexibility \n \n Create Mastra Backend Create a new Mastra project using our CLI: npx For detailed setup instructions, see our installation guide. Install MastraClient npm npm install @mastra/client-js@latest Use MastraClient Create a client instance and use it in your Next.js application: import { MastraClient } from ' @mastra/client-js '; \n \n // Initialize the client \n export const mastraClient = new MastraClient ({ \n baseUrl: process.env.NEXT_PUBLIC_MASTRA_API_URL || ' http://localhost:4111 ', \n}); Example usage in your React component: app/components/SimpleWeather.tsx ' use client ' \n \n import { mastraClient } from ' @/lib/mastra ' \n \n export function SimpleWeather () { \n async function handleSubmit (formData: FormData) { \n const city = formData. get ( ' city ') \n const agent = mastraClient. getAgent ( ' weatherAgent ') \n \n try { \n const response = await agent. generate ({ \n messages: [{ role: ' user ', content: ` What's the weather like in ${ city}? ` }], \n }) \n // Handle the response \n console. log (response.text) \n } catch (error) { \n console. error ( ' Error: ', error) \n } \n } \n \n return ( \n &lt; form action = {handleSubmit} &gt; \n &lt; input name = \" city \" placeholder = \" Enter city name \" /&gt; \n &lt; button type = \" submit \" &gt; Get Weather &lt;/ button &gt; \n &lt;/ form &gt; \n ) \n} Deployment When you’re ready to deploy, you can use any of our platform-specific deployers (Vercel, Netlify, Cloudflare) or deploy to any Node.js hosting platform. Check our deployment guide for detailed instructions. \n 2. Direct Integration \n Better for smaller projects or prototypes. This approach bundles Mastra directly with your Next.js application. \n Initialize Mastra in your Next.js Root First, navigate to your Next.js project root and initialize Mastra: Then run the initialization command: npm This will set up Mastra in your Next.js project. For more details about init and other configuration options, see our mastra init reference. Configure Next.js Add to your next.config.js: /** @ type { import('next').NextConfig} */ \n const nextConfig = { \n serverExternalPackages: [ \" @mastra/* \"], \n // ... your other Next.js config \n} \n \n module. exports = nextConfig Server Actions Example ' use server ' \n \n import { mastra } from ' @/mastra ' \n \n export async function getWeatherInfo (city: string) { \n const agent = mastra. getAgent ( ' weatherAgent ') \n \n const result = await agent. generate ( ` What's the weather like in ${ city}? `) \n \n return result \n} Use it in your component: app/components/Weather.tsx ' use client ' \n \n import { getWeatherInfo } from '../actions ' \n \n export function Weather () { \n async function handleSubmit (formData: FormData) { \n const city = formData. get ( ' city ') as string \n const result = await getWeatherInfo (city) \n // Handle the result \n console. log (result) \n } \n \n return ( \n &lt; form action = {handleSubmit} &gt; \n &lt; input name = \" city \" placeholder = \" Enter city name \" /&gt; \n &lt; button type = \" submit \" &gt; Get Weather &lt;/ button &gt; \n &lt;/ form &gt; \n ) \n} API Routes Example import { mastra } from ' @/mastra ' \n import { NextResponse } from ' next/server ' \n \n export async function POST (req: Request) { \n const { city } = await req. json () \n const agent = mastra. getAgent ( ' weatherAgent ') \n \n const result = await agent. stream ( ` What's the weather like in ${ city}? `) \n \n return result. toDataStreamResponse () \n} Deployment When using direct integration, your Mastra instance will be deployed alongside your Next.js application. Ensure you: \n Set up environment variables for your LLM API keys in your deployment platform \n Implement proper error handling for production use \n Monitor your AI agent’s performance and costs \n \n Observability \n Mastra provides built-in observability features to help you monitor, debug, and optimize your AI operations. This includes: \n \n Tracing of AI operations and their performance \n Logging of prompts, completions, and errors \n Integration with observability platforms like Langfuse and LangSmith \n \n For detailed setup instructions and configuration options specific to Next.js local development, see our Next.js Observability Configuration Guide. With Vercel AI SDK With CopilotKit",
            "image": "https://mastra.ai/api/og/docs?title=Getting%20started%20with%20Mastra%20and%20NextJS%20|%20Mastra%20Guides&description=Guide%20on%20integrating%20Mastra%20with%20NextJS.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/frameworks/next-js",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/storage/overview",
            "title": "Storage in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/storage/overview",
            "publishedDate": "",
            "author": "",
            "text": "MastraStorage \n MastraStorage provides a unified interface for managing: \n \n Suspended Workflows: the serialized state of suspended workflows (so they can be resumed later) \n Memory: threads and messages per resourceId in your application \n Traces: OpenTelemetry traces from all components of Mastra \n Eval Datasets: scores and scoring reasons from eval runs \n \n \n Mastra provides different storage providers, but you can treat them as interchangeable. Eg, you could use libsql in development but postgres in production, and your code will work the same both ways. \n Configuration \n Mastra can be configured with a default storage option: \n import { Mastra } from \" @mastra/core/mastra \"; \n import { LibSQLStore } from \" @mastra/libsql \"; \n \n const mastra = new Mastra ({ \n storage: new LibSQLStore ({ \n url: \" file:./mastra.db \", \n }), \n}); \n Data Schema \n Messages Stores conversation messages and their metadata. Each message belongs to a thread and contains the actual content along with metadata about the sender role and message type. \n Storage Providers \n Mastra supports the following providers: \n \n For local development, check out LibSQL Storage \n For production, check out PostgreSQL Storage \n For serverless deployments, check out Upstash Storage \n With CopilotKit Overview",
            "image": "https://mastra.ai/api/og/docs?title=Storage%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20Mastra%27s%20storage%20system%20and%20data%20persistence%20capabilities.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/storage/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/voice/overview",
            "title": "Voice in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/voice/overview",
            "author": "",
            "text": "\n Mastra’s Voice system provides a unified interface for voice interactions, enabling text-to-speech (TTS), speech-to-text (STT), and real-time speech-to-speech (STS) capabilities in your applications. \n Adding Voice to Agents \n To learn how to integrate voice capabilities into your agents, check out the Adding Voice to Agents documentation. This section covers how to use both single and multiple voice providers, as well as real-time interactions. \n import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n \n // Initialize OpenAI voice for TTS \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n You can then use the following voice capabilities: \n Text to Speech (TTS) \n Turn your agent’s responses into natural-sounding speech using Mastra’s TTS capabilities.\nChoose from multiple providers like OpenAI, ElevenLabs, and more. \n For detailed configuration options and advanced features, check out our Text-to-Speech guide. \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { playAudio } from \" @mastra/node-audio \"; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n \n const { text } = await voiceAgent. generate ( ' What color is the sky? '); \n \n // Convert text to speech to an Audio Stream \n const audioStream = await voiceAgent.voice. speak (text, { \n speaker: \" default \", // Optional: specify a speaker \n responseFormat: \" wav \", // Optional: specify a response format \n}); \n \n playAudio (audioStream); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Speech to Text (STT) \n Transcribe spoken content using various providers like OpenAI, ElevenLabs, and more. For detailed configuration options and more, check out Speech to Text. \n You can download a sample audio file from here . \n \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { createReadStream } from ' fs '; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIVoice (), \n}); \n \n // Use an audio file from a URL \n const audioStream = await createReadStream ( \"./how_can_i_help_you.mp3 \"); \n \n // Convert audio to text \n const transcript = await voiceAgent.voice. listen (audioStream); \n console. log ( ` User said: ${ transcript}`); \n \n // Generate a response based on the transcript \n const { text } = await voiceAgent. generate (transcript); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Speech to Speech (STS) \n Create conversational experiences with speech-to-speech capabilities. The unified API enables real-time voice interactions between users and AI agents.\nFor detailed configuration options and advanced features, check out Speech to Speech. \n OpenAI import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { playAudio, getMicrophoneStream } from ' @mastra/node-audio '; \n import { OpenAIRealtimeVoice } from \" @mastra/voice-openai-realtime \"; \n \n const voiceAgent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice: new OpenAIRealtimeVoice (), \n}); \n \n // Listen for agent audio responses \n voiceAgent.voice. on ( ' speaker ', ({ audio }) =&gt; { \n playAudio (audio); \n}); \n \n // Initiate the conversation \n await voiceAgent.voice. speak ( ' How can I help you today? '); \n \n // Send continuous audio from the microphone \n const micStream = getMicrophoneStream (); \n await voiceAgent.voice. send (micStream); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Voice Configuration \n Each voice provider can be configured with different models and options. Below are the detailed configuration options for all supported providers: \n OpenAI // OpenAI Voice Configuration \n const voice = new OpenAIVoice ({ \n speechModel: { \n name: \" gpt-3.5-turbo \", // Example model name \n apiKey: process.env.OPENAI_API_KEY, \n language: \" en-US \", // Language code \n voiceType: \" neural \", // Type of voice model \n }, \n listeningModel: { \n name: \" whisper-1 \", // Example model name \n apiKey: process.env.OPENAI_API_KEY, \n language: \" en-US \", // Language code \n format: \" wav \", // Audio format \n }, \n speaker: \" alloy \", // Example speaker name \n}); Visit the OpenAI Voice Reference for more information on the OpenAI voice provider. \n Using Multiple Voice Providers \n This example demonstrates how to create and use two different voice providers in Mastra: OpenAI for speech-to-text (STT) and PlayAI for text-to-speech (TTS). \n Start by creating instances of the voice providers with any necessary configuration. \n import { OpenAIVoice } from \" @mastra/voice-openai \"; \n import { PlayAIVoice } from \" @mastra/voice-playai \"; \n import { CompositeVoice } from \" @mastra/core/voice \"; \n import { playAudio, getMicrophoneStream } from \" @mastra/node-audio \"; \n \n // Initialize OpenAI voice for STT \n const input = new OpenAIVoice ({ \n listeningModel: { \n name: \" whisper-1 \", \n apiKey: process.env.OPENAI_API_KEY, \n }, \n}); \n \n // Initialize PlayAI voice for TTS \n const output = new PlayAIVoice ({ \n speechModel: { \n name: \" playai-voice \", \n apiKey: process.env.PLAYAI_API_KEY, \n }, \n}); \n \n // Combine the providers using CompositeVoice \n const voice = new CompositeVoice ({ \n input, \n output, \n}); \n \n // Implement voice interactions using the combined voice provider \n const audioStream = getMicrophoneStream (); // Assume this function gets audio input \n const transcript = await voice. listen (audioStream); \n \n // Log the transcribed text \n console. log ( \" Transcribed text: \", transcript); \n \n // Convert text to speech \n const responseAudio = await voice. speak ( ` You said: ${ transcript}`, { \n speaker: \" default \", // Optional: specify a speaker, \n responseFormat: \" wav \", // Optional: specify a response format \n}); \n \n // Play the audio response \n playAudio (responseAudio); \n For more information on the CompositeVoice, refer to the CompositeVoice Reference. \n More Resources \n \n CompositeVoice \n MastraVoice \n OpenAI Voice \n Azure Voice \n Google Voice \n Deepgram Voice \n PlayAI Voice \n Voice Examples \n",
            "image": "https://mastra.ai/api/og/docs?title=Voice%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20voice%20capabilities%20in%20Mastra,%20including%20text-to-speech,%20speech-to-text,%20and%20real-time%20speech-to-speech%20interactions.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/voice/overview",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/voice/text-to-speech",
            "title": "Text-to-Speech (TTS) in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/voice/text-to-speech",
            "publishedDate": "",
            "author": "",
            "text": "\n Text-to-Speech (TTS) in Mastra offers a unified API for synthesizing spoken audio from text using various providers.\nBy incorporating TTS into your applications, you can enhance user experience with natural voice interactions, improve accessibility for users with visual impairments, and create more engaging multimodal interfaces. \n TTS is a core component of any voice application. Combined with STT (Speech-to-Text), it forms the foundation of voice interaction systems. Newer models support STS ( Speech-to-Speech) which can be used for real-time interactions but come at high cost ($). \n Configuration \n To use TTS in Mastra, you need to provide a speechModel when initializing the voice provider. This includes parameters such as: \n \n name: The specific TTS model to use. \n apiKey: Your API key for authentication. \n Provider-specific options: Additional options that may be required or supported by the specific voice provider. \n \n The speaker option allows you to select different voices for speech synthesis. Each provider offers a variety of voice options with distinct characteristics for Voice diversity, Quality, Voice personality, and Multilingual support \n Note: All of these parameters are optional. You can use the default settings provided by the voice provider, which will depend on the specific provider you are using. \n const voice = new OpenAIVoice ({ \n speechModel: { \n name: \" tts-1-hd \", \n apiKey: process.env.OPENAI_API_KEY \n }, \n speaker: \" alloy \", \n}); \n \n // If using default settings the configuration can be simplified to: \n const voice = new OpenAIVoice (); \n Available Providers \n Mastra supports a wide range of Text-to-Speech providers, each with their own unique capabilities and voice options. You can choose the provider that best suits your application’s needs: \n \n OpenAI - High-quality voices with natural intonation and expression \n Azure - Microsoft’s speech service with a wide range of voices and languages \n ElevenLabs - Ultra-realistic voices with emotion and fine-grained control \n PlayAI - Specialized in natural-sounding voices with various styles \n Google - Google’s speech synthesis with multilingual support \n Cloudflare - Edge-optimized speech synthesis for low-latency applications \n Deepgram - AI-powered speech technology with high accuracy \n Speechify - Text-to-speech optimized for readability and accessibility \n Sarvam - Specialized in Indic languages and accents \n Murf - Studio-quality voice overs with customizable parameters \n \n Each provider is implemented as a separate package that you can install as needed: \n pnpm add @mastra/voice-openai # Example for OpenAI \n Using the Speak Method \n The primary method for TTS is the speak() method, which converts text to speech. This method can accept options that allows you to specify the speaker and other provider-specific options. Here’s how to use it: \n import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from ' @mastra/voice-openai '; \n \n const voice = new OpenAIVoice (); \n \n const agent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that can help users with their tasks. \", \n model: openai ( \" gpt-4o \"), \n voice, \n}); \n \n const { text } = await agent. generate ( ' What color is the sky? '); \n \n // Convert text to speech to an Audio Stream \n const readableStream = await voice. speak (text, { \n speaker: \" default \", // Optional: specify a speaker \n properties: { \n speed: 1.0, // Optional: adjust speech speed \n pitch: \" default \", // Optional: specify pitch if supported \n }, \n}); \n Check out the Adding Voice to Agents documentation to learn how to use TTS in an agent. Overview Speech to Text",
            "image": "https://mastra.ai/api/og/docs?title=Text-to-Speech%20(TTS)%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20Text-to-Speech%20capabilities%20in%20Mastra,%20including%20configuration,%20usage,%20and%20integration%20with%20voice%20providers.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/voice/text-to-speech",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/voice/speech-to-text",
            "title": "Speech-to-Text (STT) in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/voice/speech-to-text",
            "publishedDate": "",
            "author": "",
            "text": "\n Speech-to-Text (STT) in Mastra provides a standardized interface for converting audio input into text across multiple service providers.\nSTT helps create voice-enabled applications that can respond to human speech, enabling hands-free interaction, accessibility for users with disabilities, and more natural human-computer interfaces. \n Configuration \n To use STT in Mastra, you need to provide a listeningModel when initializing the voice provider. This includes parameters such as: \n \n name: The specific STT model to use. \n apiKey: Your API key for authentication. \n Provider-specific options: Additional options that may be required or supported by the specific voice provider. \n \n Note: All of these parameters are optional. You can use the default settings provided by the voice provider, which will depend on the specific provider you are using. \n const voice = new OpenAIVoice ({ \n listeningModel: { \n name: \" whisper-1 \", \n apiKey: process.env.OPENAI_API_KEY, \n }, \n}); \n \n // If using default settings the configuration can be simplified to: \n const voice = new OpenAIVoice (); \n Available Providers \n Mastra supports several Speech-to-Text providers, each with their own capabilities and strengths: \n \n OpenAI - High-accuracy transcription with Whisper models \n Azure - Microsoft’s speech recognition with enterprise-grade reliability \n ElevenLabs - Advanced speech recognition with support for multiple languages \n Google - Google’s speech recognition with extensive language support \n Cloudflare - Edge-optimized speech recognition for low-latency applications \n Deepgram - AI-powered speech recognition with high accuracy for various accents \n Sarvam - Specialized in Indic languages and accents \n \n Each provider is implemented as a separate package that you can install as needed: \n pnpm add @mastra/voice-openai # Example for OpenAI \n Using the Listen Method \n The primary method for STT is the listen() method, which converts spoken audio into text. Here’s how to use it: \n import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from ' @mastra/voice-openai '; \n import { getMicrophoneStream } from \" @mastra/node-audio \"; \n \n const voice = new OpenAIVoice (); \n \n const agent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that provides recommendations based on user input. \", \n model: openai ( \" gpt-4o \"), \n voice, \n}); \n \n const audioStream = getMicrophoneStream (); // Assume this function gets audio input \n \n const transcript = await agent.voice. listen (audioStream, { \n filetype: \" m4a \", // Optional: specify the audio file type \n}); \n \n console. log ( ` User said: ${ transcript}`); \n \n const { text } = await agent. generate ( ` Based on what the user said, provide them a recommendation: ${ transcript}`); \n \n console. log ( ` Recommendation: ${ text}`); \n Check out the Adding Voice to Agents documentation to learn how to use STT in an agent. Text to Speech Speech to Speech new",
            "image": "https://mastra.ai/api/og/docs?title=Speech-to-Text%20(STT)%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20Speech-to-Text%20capabilities%20in%20Mastra,%20including%20configuration,%20usage,%20and%20integration%20with%20voice%20providers.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/voice/speech-to-text",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/voice/speech-to-speech",
            "title": "Speech-to-Speech Capabilities in Mastra | Mastra Docs",
            "url": "https://mastra.ai/docs/voice/speech-to-speech",
            "publishedDate": "",
            "author": "",
            "text": "\n Introduction \n Speech-to-Speech (STS) in Mastra provides a standardized interface for real-time interactions across multiple providers. \nSTS enables continuous bidirectional audio communication through listening to events from Realtime models. Unlike separate TTS and STT operations, STS maintains an open connection that processes speech continuously in both directions. \n Configuration \n \n chatModel: Configuration for the realtime model.\n \n apiKey: Your OpenAI API key. Falls back to the OPENAI_API_KEY environment variable. \n model: The model ID to use for real-time voice interactions (e.g., gpt-4o-mini-realtime). \n options: Additional options for the realtime client, such as session configuration. \n \n \n speaker: The default voice ID for speech synthesis. This allows you to specify which voice to use for the speech output. \n \n const voice = new OpenAIRealtimeVoice ({ \n chatModel: { \n apiKey: ' your-openai-api-key ', \n model: ' gpt-4o-mini-realtime ', \n options: { \n sessionConfig: { \n turn_detection: { \n type: ' server_vad ', \n threshold: 0.6, \n silence_duration_ms: 1200, \n }, \n }, \n }, \n }, \n speaker: ' alloy ', // Default voice \n}); \n \n // If using default settings the configuration can be simplified to: \n const voice = new OpenAIRealtimeVoice (); \n Using STS \n import { Agent } from \" @mastra/core/agent \"; \n import { OpenAIRealtimeVoice } from \" @mastra/voice-openai-realtime \"; \n import { playAudio, getMicrophoneStream } from \" @mastra/node-audio \"; \n \n const agent = new Agent ({ \n name: ' Agent ', \n instructions: ` You are a helpful assistant with real-time voice capabilities. `, \n model: openai ( ' gpt-4o '), \n voice: new OpenAIRealtimeVoice (), \n}); \n \n // Connect to the voice service \n await agent.voice. connect (); \n \n // Listen for agent audio responses \n agent.voice. on ( ' speaker ', ({ audio }) =&gt; { \n playAudio (audio); \n}); \n \n // Initiate the conversation \n await agent.voice. speak ( ' How can I help you today? '); \n \n // Send continuous audio from the microphone \n const micStream = getMicrophoneStream (); \n await agent.voice. send (micStream); \n For integrating Speech-to-Speech capabilities with agents, refer to the Adding Voice to Agents documentation. Speech to Text Licensing",
            "image": "https://mastra.ai/api/og/docs?title=Speech-to-Speech%20Capabilities%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20speech-to-speech%20capabilities%20in%20Mastra,%20including%20real-time%20interactions%20and%20event-driven%20architecture.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/voice/speech-to-speech",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/community/licensing",
            "title": "Licensing",
            "url": "https://mastra.ai/docs/community/licensing",
            "author": "",
            "text": "License \n Elastic License 2.0 (ELv2) \n Mastra is licensed under the Elastic License 2.0 (ELv2), a modern license designed to balance open-source principles with sustainable business practices. \n What is Elastic License 2.0? \n The Elastic License 2.0 is a source-available license that grants users broad rights to use, modify, and distribute the software while including specific limitations to protect the project’s sustainability. It allows: \n \n Free use for most purposes \n Viewing, modifying, and redistributing the source code \n Creating and distributing derivative works \n Commercial use within your organization \n \n The primary limitation is that you cannot provide Mastra as a hosted or managed service that offers users access to the substantial functionality of the software. \n Why We Chose Elastic License 2.0 \n We selected the Elastic License 2.0 for several important reasons: \n \n \n Sustainability: It enables us to maintain a healthy balance between openness and the ability to sustain long-term development. \n \n \n Innovation Protection: It ensures we can continue investing in innovation without concerns about our work being repackaged as competing services. \n \n \n Community Focus: It maintains the spirit of open source by allowing users to view, modify, and learn from our code while protecting our ability to support the community. \n \n \n Business Clarity: It provides clear guidelines for how Mastra can be used in commercial contexts. \n \n \n Building Your Business with Mastra \n Despite the licensing restrictions, there are numerous ways to build successful businesses using Mastra: \n Allowed Business Models \n \n Building Applications: Create and sell applications built with Mastra \n Offering Consulting Services: Provide expertise, implementation, and customization services \n Developing Custom Solutions: Build bespoke AI solutions for clients using Mastra \n Creating Add-ons and Extensions: Develop and sell complementary tools that extend Mastra’s functionality \n Training and Education: Offer courses and educational materials about using Mastra effectively \n \n Examples of Compliant Usage \n \n A company builds an AI-powered customer service application using Mastra and sells it to clients \n A consulting firm offers implementation and customization services for Mastra \n A developer creates specialized agents and tools with Mastra and licenses them to other businesses \n A startup builds a vertical-specific solution (e.g., healthcare AI assistant) powered by Mastra \n \n What to Avoid \n The main restriction is that you cannot offer Mastra itself as a hosted service where users access its core functionality. This means: \n \n Don’t create a SaaS platform that is essentially Mastra with minimal modifications \n Don’t offer a managed Mastra service where customers are primarily paying to use Mastra’s features \n \n Questions About Licensing? \n If you have specific questions about how the Elastic License 2.0 applies to your use case, please contact us  on Discord for clarification. We’re committed to supporting legitimate business use cases while protecting the sustainability of the project. Overview Discord",
            "image": "https://mastra.ai/api/og/docs?title=Licensing&description=Mastra%20License",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/docs/community/licensing",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/docs/community/discord",
            "title": "Discord Community and Bot | Documentation | Mastra",
            "url": "https://mastra.ai/docs/community/discord",
            "author": "",
            "text": "\n The Discord server has over 1000 members and serves as the main discussion forum for Mastra. The Mastra team monitors Discord during North American and European business hours, with community members active across other time zones. Join the Discord server . \n Discord MCP Bot \n In addition to community members, we have an (experimental!) Discord bot that can also help answer questions. It uses Model Context Protocol (MCP). You can ask it a question with /ask (either in public channels or DMs) and clear history (in DMs only) with /cleardm. Licensing",
            "image": "https://mastra.ai/api/og/docs?title=Discord%20Community%20and%20Bot%20|%20Documentation%20|%20Mastra&description=Information%20about%20the%20Mastra%20Discord%20community%20and%20MCP%20bot.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/docs/community/discord",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/system-prompt",
            "title": "Example: Agents with a System Prompt | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/system-prompt",
            "publishedDate": "",
            "author": "",
            "text": "Giving an Agent a System Prompt \n When building AI agents, you often need to give them specific instructions and capabilities to handle specialized tasks effectively. System prompts allow you to define an agent’s personality, knowledge domain, and behavioral guidelines. This example shows how to create an AI agent with custom instructions and integrate it with a dedicated tool for retrieving verified information. \n import { openai } from \" @ai-sdk/openai \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { createTool } from \" @mastra/core/tools \"; \n \n import { z } from \" zod \"; \n \n const instructions = ` You are a helpful cat expert assistant. When discussing cats, you should always include an interesting cat fact. \n \n Your main responsibilities: \n 1. Answer questions about cats \n 2. Use the catFact tool to provide verified cat facts \n 3. Incorporate the cat facts naturally into your responses \n \n Always use the catFact tool at least once in your responses to ensure accuracy. `; \n \n const getCatFact = async () =&gt; { \n const { fact } = ( await fetch ( \" https://catfact.ninja/fact \"). then ((res) =&gt; \n res. json (), \n )) as { \n fact: string; \n }; \n \n return fact; \n}; \n \n const catFact = createTool ({ \n id: \" Get cat facts \", \n inputSchema: z. object ({}), \n description: \" Fetches cat facts \", \n execute: async () =&gt; { \n console. log ( \" using tool to fetch cat fact \"); \n return { \n catFact: await getCatFact (), \n }; \n }, \n}); \n \n const catOne = new Agent ({ \n name: \" cat-one \", \n instructions: instructions, \n model: openai ( \" gpt-4o-mini \"), \n tools: { \n catFact, \n }, \n}); \n \n const result = await catOne. generate ( \" Tell me a cat fact \"); \n \n console. log (result.text); \n \n Overview Agentic Workflows",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Agents%20with%20a%20System%20Prompt%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20creating%20an%20AI%20agent%20in%20Mastra%20with%20a%20system%20prompt%20to%20define%20its%20personality%20and%20capabilities.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/system-prompt",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/agentic-workflows",
            "title": "Example: Calling Agentic Workflows | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/agentic-workflows",
            "publishedDate": "",
            "author": "",
            "text": "When building AI applications, you often need to coordinate multiple steps that depend on each other’s outputs. This example shows how to create an AI workflow that fetches weather data and uses it to suggest activities, demonstrating how to integrate external APIs with LLM-powered planning. import { Mastra } from \" @mastra/core \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n name: ' Weather Agent ', \n instructions: ` \n You are a local activities and travel expert who excels at weather-based planning. Analyze the weather data and provide practical activity recommendations. \n For each day in the forecast, structure your response exactly as follows: \n 📅 [Day, Month Date, Year] \n ═══════════════════════════ \n 🌡️ WEATHER SUMMARY \n • Conditions: [brief description] \n • Temperature: [X°C/Y°F to A°C/B°F] \n • Precipitation: [X% chance] \n 🌅 MORNING ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n 🌞 AFTERNOON ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n 🏠 INDOOR ALTERNATIVES \n • [Activity Name] - [Brief description including specific venue] \n Ideal for: [weather condition that would trigger this alternative] \n ⚠️ SPECIAL CONSIDERATIONS \n • [Any relevant weather warnings, UV index, wind conditions, etc.] \n Guidelines: \n - Suggest 2-3 time-specific outdoor activities per day \n - Include 1-2 indoor backup options \n - For precipitation &gt;50%, lead with indoor activities \n - All activities must be specific to the location \n - Include specific venues, trails, or locations \n - Consider activity intensity based on temperature \n - Keep descriptions concise but informative \n Maintain this exact formatting for consistency, using the emoji and section headers as shown. \n `, \n model: openai ( ' gpt-4o-mini '), \n}); \n \n const fetchWeather = new Step ({ \n id: \" fetch-weather \", \n description: \" Fetches weather forecast for a given city \", \n inputSchema: z. object ({ \n city: z. string (). describe ( \" The city to get the weather for \"), \n }), \n execute: async ({ context }) =&gt; { \n const triggerData = context?. getStepResult &lt;{ \n city: string; \n }&gt;( \" trigger \"); \n \n if (! triggerData) { \n throw new Error ( \" Trigger data not found \"); \n } \n \n const geocodingUrl = ` https://geocoding-api.open-meteo.com/v1/search?name= ${ encodeURIComponent (triggerData.city)} &amp;count=1 `; \n const geocodingResponse = await fetch (geocodingUrl); \n const geocodingData = await geocodingResponse. json (); \n \n if (! geocodingData.results?.[ 0]) { \n throw new Error ( ` Location ' ${ triggerData.city} ' not found `); \n } \n \n const { latitude, longitude, name } = geocodingData.results[ 0]; \n \n const weatherUrl = ` https://api.open-meteo.com/v1/forecast?latitude= ${ latitude} &amp;longitude= ${ longitude} &amp;daily=temperature_2m_max,temperature_2m_min,precipitation_probability_mean,weathercode&amp;timezone=auto `; \n const response = await fetch (weatherUrl); \n const data = await response. json (); \n \n const forecast = data.daily.time. map ((date: string, index: number) =&gt; ({ \n date, \n maxTemp: data.daily.temperature_2m_max[index], \n minTemp: data.daily.temperature_2m_min[index], \n precipitationChance: data.daily.precipitation_probability_mean[index], \n condition: getWeatherCondition (data.daily.weathercode[index]), \n location: name, \n })); \n \n return forecast; \n }, \n}); \n \n const forecastSchema = z. array ( \n z. object ({ \n date: z. string (), \n maxTemp: z. number (), \n minTemp: z. number (), \n precipitationChance: z. number (), \n condition: z. string (), \n location: z. string (), \n }), \n); \n \n const planActivities = new Step ({ \n id: \" plan-activities \", \n description: \" Suggests activities based on weather conditions \", \n inputSchema: forecastSchema, \n execute: async ({ context, mastra }) =&gt; { \n const forecast = \n context?. getStepResult &lt;z. infer &lt; typeof forecastSchema&gt;&gt;( \n \" fetch-weather \", \n ); \n \n if (! forecast) { \n throw new Error ( \" Forecast data not found \"); \n } \n \n const prompt = ` Based on the following weather forecast for ${ forecast[ 0].location}, suggest appropriate activities: \n ${ JSON. stringify (forecast, null, 2)} \n `; \n \n const response = await agent. stream ([ \n { \n role: \" user \", \n content: prompt, \n }, \n ]); \n \n let activitiesText = ''; \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk); \n activitiesText += chunk; \n } \n \n return { \n activities: activitiesText, \n }; \n }, \n}); \n \n function getWeatherCondition (code: number): string { \n const conditions: Record &lt; number, string &gt; = { \n 0: \" Clear sky \", \n 1: \" Mainly clear \", \n 2: \" Partly cloudy \", \n 3: \" Overcast \", \n 45: \" Foggy \", \n 48: \" Depositing rime fog \", \n 51: \" Light drizzle \", \n 53: \" Moderate drizzle \", \n 55: \" Dense drizzle \", \n 61: \" Slight rain \", \n 63: \" Moderate rain \", \n 65: \" Heavy rain \", \n 71: \" Slight snow fall \", \n 73: \" Moderate snow fall \", \n 75: \" Heavy snow fall \", \n 95: \" Thunderstorm \", \n }; \n return conditions[code] || \" Unknown \"; \n} \n \n const weatherWorkflow = new Workflow ({ \n name: \" weather-workflow \", \n triggerSchema: z. object ({ \n city: z. string (). describe ( \" The city to get the weather for \"), \n }), \n}) \n . step (fetchWeather) \n . then (planActivities); \n \n weatherWorkflow. commit (); \n \n const mastra = new Mastra ({ \n workflows: { \n weatherWorkflow, \n }, \n}); \n \n async function main () { \n const { start } = mastra. getWorkflow ( \" weatherWorkflow \"). createRun (); \n \n const result = await start ({ \n triggerData: { \n city: \" London \", \n }, \n }); \n \n console. log ( \" \\n \\n \"); \n console. log (result); \n} \n \n main ();",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Calling%20Agentic%20Workflows%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20creating%20AI%20workflows%20in%20Mastra,%20demonstrating%20integration%20of%20external%20APIs%20with%20LLM-powered%20planning.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/agentic-workflows",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/using-a-tool",
            "title": "Example: Giving an Agent a Tool | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/using-a-tool",
            "publishedDate": "",
            "author": "",
            "text": "\n When building AI agents, you often need to integrate external data sources or functionality to enhance their capabilities. This example shows how to create an AI agent that uses a dedicated weather tool to provide accurate weather information for specific locations. \n import { Mastra } from \" @mastra/core \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { createTool } from \" @mastra/core/tools \"; \n import { openai } from \" @ai-sdk/openai \"; \n import { z } from \" zod \"; \n \n interface WeatherResponse { \n current: { \n time: string; \n temperature_2m: number; \n apparent_temperature: number; \n relative_humidity_2m: number; \n wind_speed_10m: number; \n wind_gusts_10m: number; \n weather_code: number; \n }; \n} \n \n const weatherTool = createTool ({ \n id: \" get-weather \", \n description: \" Get current weather for a location \", \n inputSchema: z. object ({ \n location: z. string (). describe ( \" City name \"), \n }), \n outputSchema: z. object ({ \n temperature: z. number (), \n feelsLike: z. number (), \n humidity: z. number (), \n windSpeed: z. number (), \n windGust: z. number (), \n conditions: z. string (), \n location: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n return await getWeather (context.location); \n }, \n}); \n \n const getWeather = async (location: string) =&gt; { \n const geocodingUrl = ` https://geocoding-api.open-meteo.com/v1/search?name= ${ encodeURIComponent (location)} &amp;count=1 `; \n const geocodingResponse = await fetch (geocodingUrl); \n const geocodingData = await geocodingResponse. json (); \n \n if (! geocodingData.results?.[ 0]) { \n throw new Error ( ` Location ' ${ location} ' not found `); \n } \n \n const { latitude, longitude, name } = geocodingData.results[ 0]; \n \n const weatherUrl = ` https://api.open-meteo.com/v1/forecast?latitude= ${ latitude} &amp;longitude= ${ longitude} &amp;current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code `; \n \n const response = await fetch (weatherUrl); \n const data: WeatherResponse = await response. json (); \n \n return { \n temperature: data.current.temperature_2m, \n feelsLike: data.current.apparent_temperature, \n humidity: data.current.relative_humidity_2m, \n windSpeed: data.current.wind_speed_10m, \n windGust: data.current.wind_gusts_10m, \n conditions: getWeatherCondition (data.current.weather_code), \n location: name, \n }; \n}; \n \n function getWeatherCondition (code: number): string { \n const conditions: Record &lt; number, string &gt; = { \n 0: \" Clear sky \", \n 1: \" Mainly clear \", \n 2: \" Partly cloudy \", \n 3: \" Overcast \", \n 45: \" Foggy \", \n 48: \" Depositing rime fog \", \n 51: \" Light drizzle \", \n 53: \" Moderate drizzle \", \n 55: \" Dense drizzle \", \n 56: \" Light freezing drizzle \", \n 57: \" Dense freezing drizzle \", \n 61: \" Slight rain \", \n 63: \" Moderate rain \", \n 65: \" Heavy rain \", \n 66: \" Light freezing rain \", \n 67: \" Heavy freezing rain \", \n 71: \" Slight snow fall \", \n 73: \" Moderate snow fall \", \n 75: \" Heavy snow fall \", \n 77: \" Snow grains \", \n 80: \" Slight rain showers \", \n 81: \" Moderate rain showers \", \n 82: \" Violent rain showers \", \n 85: \" Slight snow showers \", \n 86: \" Heavy snow showers \", \n 95: \" Thunderstorm \", \n 96: \" Thunderstorm with slight hail \", \n 99: \" Thunderstorm with heavy hail \", \n }; \n return conditions[code] || \" Unknown \"; \n} \n \n const weatherAgent = new Agent ({ \n name: \" Weather Agent \", \n instructions: ` You are a helpful weather assistant that provides accurate weather information. \n Your primary function is to help users get weather details for specific locations. When responding: \n - Always ask for a location if none is provided \n - If the location name isn’t in English, please translate it \n - Include relevant details like humidity, wind conditions, and precipitation \n - Keep responses concise but informative \n Use the weatherTool to fetch current weather data. `, \n model: openai ( \" gpt-4o-mini \"), \n tools: { weatherTool }, \n}); \n \n const mastra = new Mastra ({ \n agents: { weatherAgent }, \n}); \n \n async function main () { \n const agent = await mastra. getAgent ( \" weatherAgent \"); \n const result = await agent. generate ( \" What is the weather in London? \"); \n console. log (result.text); \n} \n \n main (); \n \n Agentic Workflows Hierarchical Multi-Agent System",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Giving%20an%20Agent%20a%20Tool%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20creating%20an%20AI%20agent%20in%20Mastra%20that%20uses%20a%20dedicated%20tool%20to%20provide%20weather%20information.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/using-a-tool",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/hierarchical-multi-agent",
            "title": "Example: Hierarchical Multi-Agent System | Agents | Mastra",
            "url": "https://mastra.ai/examples/agents/hierarchical-multi-agent",
            "publishedDate": "2024-10-22T00:00:00.000Z",
            "author": "",
            "text": "\n This example demonstrates how to create a hierarchical multi-agent system where agents interact through tool functions, with one agent coordinating the work of others. \n The system consists of three agents: \n \n A Publisher agent (supervisor) that orchestrates the process \n A Copywriter agent that writes the initial content \n An Editor agent that refines the content \n \n First, define the Copywriter agent and its tool: \n import { openai } from \" @ai-sdk/openai \"; \n import { anthropic } from \" @ai-sdk/anthropic \"; \n \n const copywriterAgent = new Agent ({ \n name: \" Copywriter \", \n instructions: \" You are a copywriter agent that writes blog post copy. \", \n model: anthropic ( \" claude-3-5-sonnet-20241022 \"), \n}); \n \n const copywriterTool = createTool ({ \n id: \" copywriter-agent \", \n description: \" Calls the copywriter agent to write blog post copy. \", \n inputSchema: z. object ({ \n topic: z. string (). describe ( \" Blog post topic \"), \n }), \n outputSchema: z. object ({ \n copy: z. string (). describe ( \" Blog post copy \"), \n }), \n execute: async ({ context }) =&gt; { \n const result = await copywriterAgent. generate ( \n ` Create a blog post about ${ context.topic}`, \n ); \n return { copy: result.text }; \n }, \n}); \n Next, define the Editor agent and its tool: \n const editorAgent = new Agent ({ \n name: \" Editor \", \n instructions: \" You are an editor agent that edits blog post copy. \", \n model: openai ( \" gpt-4o-mini \"), \n}); \n \n const editorTool = createTool ({ \n id: \" editor-agent \", \n description: \" Calls the editor agent to edit blog post copy. \", \n inputSchema: z. object ({ \n copy: z. string (). describe ( \" Blog post copy \"), \n }), \n outputSchema: z. object ({ \n copy: z. string (). describe ( \" Edited blog post copy \"), \n }), \n execute: async ({ context }) =&gt; { \n const result = await editorAgent. generate ( \n ` Edit the following blog post only returning the edited copy: ${ context.copy}`, \n ); \n return { copy: result.text }; \n }, \n}); \n Finally, create the Publisher agent that coordinates the others: \n const publisherAgent = new Agent ({ \n name: \" publisherAgent \", \n instructions: \n \" You are a publisher agent that first calls the copywriter agent to write blog post copy about a specific topic and then calls the editor agent to edit the copy. Just return the final edited copy. \", \n model: anthropic ( \" claude-3-5-sonnet-20241022 \"), \n tools: { copywriterTool, editorTool }, \n}); \n \n const mastra = new Mastra ({ \n agents: { publisherAgent }, \n}); \n To use the entire system: \n async function main () { \n const agent = mastra. getAgent ( \" publisherAgent \"); \n const result = await agent. generate ( \n \" Write a blog post about React JavaScript frameworks. Only return the final edited copy. \", \n ); \n console. log (result.text); \n} \n \n main (); \n \n Using a Tool Multi-Agent Workflow",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Hierarchical%20Multi-Agent%20System%20|%20Agents%20|%20Mastra&description=Example%20of%20creating%20a%20hierarchical%20multi-agent%20system%20using%20Mastra,%20where%20agents%20interact%20through%20tool%20functions.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/hierarchical-multi-agent",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/multi-agent-workflow",
            "title": "Example: Multi-Agent Workflow | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/multi-agent-workflow",
            "publishedDate": "2024-10-22T00:00:00.000Z",
            "author": "",
            "text": "\n This example demonstrates how to create an agentic workflow with work product being passed between multiple agents with a worker agent and a supervisor agent. \n In this example, we create a sequential workflow that calls two agents in order: \n \n A Copywriter agent that writes the initial blog post \n An Editor agent that refines the content \n \n First, import the required dependencies: \n import { openai } from \" @ai-sdk/openai \"; \n import { anthropic } from \" @ai-sdk/anthropic \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n Create the copywriter agent that will generate the initial blog post: \n const copywriterAgent = new Agent ({ \n name: \" Copywriter \", \n instructions: \" You are a copywriter agent that writes blog post copy. \", \n model: anthropic ( \" claude-3-5-sonnet-20241022 \"), \n}); \n Define the copywriter step that executes the agent and handles the response: \n const copywriterStep = new Step ({ \n id: \" copywriterStep \", \n execute: async ({ context }) =&gt; { \n if (! context?.triggerData?.topic) { \n throw new Error ( \" Topic not found in trigger data \"); \n } \n const result = await copywriterAgent. generate ( \n ` Create a blog post about ${ context.triggerData.topic}`, \n ); \n console. log ( \" copywriter result \", result.text); \n return { \n copy: result.text, \n }; \n }, \n}); \n Set up the editor agent to refine the copywriter’s content: \n const editorAgent = new Agent ({ \n name: \" Editor \", \n instructions: \" You are an editor agent that edits blog post copy. \", \n model: openai ( \" gpt-4o-mini \"), \n}); \n Create the editor step that processes the copywriter’s output: \n const editorStep = new Step ({ \n id: \" editorStep \", \n execute: async ({ context }) =&gt; { \n const copy = context?. getStepResult &lt;{ copy: number }&gt;( \" copywriterStep \")?.copy; \n \n const result = await editorAgent. generate ( \n ` Edit the following blog post only returning the edited copy: ${ copy}`, \n ); \n console. log ( \" editor result \", result.text); \n return { \n copy: result.text, \n }; \n }, \n}); \n Configure the workflow and execute the steps: \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n topic: z. string (), \n }), \n}); \n \n // Run steps sequentially. \n myWorkflow. step (copywriterStep). then (editorStep). commit (); \n \n const { runId, start } = myWorkflow. createRun (); \n \n const res = await start ({ \n triggerData: { topic: \" React JavaScript frameworks \" }, \n}); \n console. log ( \" Results: \", res.results); \n \n Hierarchical Multi-Agent System Bird Checker",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Multi-Agent%20Workflow%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20creating%20an%20agentic%20workflow%20in%20Mastra,%20where%20work%20product%20is%20passed%20between%20multiple%20agents.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/multi-agent-workflow",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/bird-checker",
            "title": "Example: Categorizing Birds | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/bird-checker",
            "publishedDate": "2024-03-07T00:00:00.000Z",
            "author": "",
            "text": "Example: Categorizing Birds with an AI Agent \n We will get a random image from Unsplash  that matches a selected query and uses a Mastra AI Agent to determine if it is a bird or not. \n import { anthropic } from \" @ai-sdk/anthropic \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { z } from \" zod \"; \n \n export type Image = { \n alt_description: string; \n urls: { \n regular: string; \n raw: string; \n }; \n user: { \n first_name: string; \n links: { \n html: string; \n }; \n }; \n}; \n \n export type ImageResponse &lt; T, K &gt; = \n | { \n ok: true; \n data: T; \n } \n | { \n ok: false; \n error: K; \n }; \n \n const getRandomImage = async ({ \n query, \n}: { \n query: string; \n}): Promise &lt; ImageResponse &lt; Image, string &gt;&gt; =&gt; { \n const page = Math. floor (Math. random () * 20); \n const order_by = Math. random () &lt; 0.5 ? \" relevant \" : \" latest \"; \n try { \n const res = await fetch ( \n ` https://api.unsplash.com/search/photos?query= ${ query} &amp;page= ${ page} &amp;order_by= ${ order_by}`, \n { \n method: \" GET \", \n headers: { \n Authorization: ` Client-ID ${ process.env.UNSPLASH_ACCESS_KEY}`, \n \" Accept-Version \": \" v1 \", \n }, \n cache: \" no-store \", \n }, \n ); \n \n if (! res.ok) { \n return { \n ok: false, \n error: \" Failed to fetch image \", \n }; \n } \n \n const data = ( await res. json ()) as { \n results: Array &lt; Image &gt;; \n }; \n const randomNo = Math. floor (Math. random () * data.results. length); \n \n return { \n ok: true, \n data: data.results[randomNo] as Image, \n }; \n } catch (err) { \n return { \n ok: false, \n error: \" Error fetching image \", \n }; \n } \n}; \n \n const instructions = ` \n You can view an image and figure out if it is a bird or not. \n You can also figure out the species of the bird and where the picture was taken. \n `; \n \n export const birdCheckerAgent = new Agent ({ \n name: \" Bird checker \", \n instructions, \n model: anthropic ( \" claude-3-haiku-20240307 \"), \n}); \n \n const queries: string [] = [ \" wildlife \", \" feathers \", \" flying \", \" birds \"]; \n const randomQuery = queries[Math. floor (Math. random () * queries.length)]; \n \n // Get the image url from Unsplash with random type \n const imageResponse = await getRandomImage ({ query: randomQuery }); \n \n if (! imageResponse.ok) { \n console. log ( \" Error fetching image \", imageResponse.error); \n process. exit ( 1); \n} \n \n console. log ( \" Image URL: \", imageResponse.data.urls.regular); \n const response = await birdCheckerAgent. generate ( \n [ \n { \n role: \" user \", \n content: [ \n { \n type: \" image \", \n image: new URL (imageResponse.data.urls.regular), \n }, \n { \n type: \" text \", \n text: \" view this image and let me know if it's a bird or not, and the scientific name of the bird without any explanation. Also summarize the location for this picture in one or two short sentences understandable by a high school student \", \n }, \n ], \n }, \n ], \n { \n output: z. object ({ \n bird: z. boolean (), \n species: z. string (), \n location: z. string (), \n }), \n }, \n); \n \n console. log (response.object); \n \n Multi-Agent Workflow Give your Agent a voice",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Categorizing%20Birds%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20using%20a%20Mastra%20AI%20Agent%20to%20determine%20if%20an%20image%20from%20Unsplash%20depicts%20a%20bird.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/bird-checker",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/adding-voice-capabilities",
            "title": "Example: Adding Voice Capabilities | Agents | Mastra",
            "url": "https://mastra.ai/examples/agents/adding-voice-capabilities",
            "publishedDate": "",
            "author": "",
            "text": "Giving your Agent a Voice \n This example demonstrates how to add voice capabilities to Mastra agents, enabling them to speak and listen using different voice providers. We’ll create two agents with different voice configurations and show how they can interact using speech. \n The example showcases: \n \n Using CompositeVoice to combine different providers for speaking and listening \n Using a single provider for both capabilities \n Basic voice interactions between agents \n \n First, let’s import the required dependencies and set up our agents: \n // Import required dependencies \n import { openai } from ' @ai-sdk/openai '; \n import { Agent } from ' @mastra/core/agent '; \n import { CompositeVoice } from ' @mastra/core/voice '; \n import { OpenAIVoice } from ' @mastra/voice-openai '; \n import { createReadStream, createWriteStream } from ' fs '; \n import { PlayAIVoice } from ' @mastra/voice-playai '; \n import path from ' path '; \n \n // Initialize Agent 1 with both listening and speaking capabilities \n const agent1 = new Agent ({ \n name: ' Agent1 ', \n instructions: ` You are an agent with both STT and TTS capabilities. `, \n model: openai ( ' gpt-4o '), \n voice: new CompositeVoice ({ \n input: new OpenAIVoice (), // For converting speech to text \n output: new PlayAIVoice (), // For converting text to speech \n }), \n}); \n \n // Initialize Agent 2 with just OpenAI for both listening and speaking capabilities \n const agent2 = new Agent ({ \n name: ' Agent2 ', \n instructions: ` You are an agent with both STT and TTS capabilities. `, \n model: openai ( ' gpt-4o '), \n voice: new OpenAIVoice (), \n}); \n In this setup: \n \n Agent1 uses a CompositeVoice that combines OpenAI for speech-to-text and PlayAI for text-to-speech \n Agent2 uses OpenAI’s voice capabilities for both functions \n \n Now let’s demonstrate a basic interaction between the agents: \n // Step 1: Agent 1 speaks a question and saves it to a file \n const audio1 = await agent1.voice. speak ( ' What is the meaning of life in one sentence? '); \n await saveAudioToFile (audio1, ' agent1-question.mp3 '); \n \n // Step 2: Agent 2 listens to Agent 1's question \n const audioFilePath = path. join (process. cwd (), ' agent1-question.mp3 '); \n const audioStream = createReadStream (audioFilePath); \n const audio2 = await agent2.voice. listen (audioStream); \n const text = await convertToText (audio2); \n \n // Step 3: Agent 2 generates and speaks a response \n const agent2Response = await agent2. generate (text); \n const agent2ResponseAudio = await agent2.voice. speak (agent2Response.text); \n await saveAudioToFile (agent2ResponseAudio, ' agent2-response.mp3 '); \n Here’s what’s happening in the interaction: \n \n Agent1 converts text to speech using PlayAI and saves it to a file (we save the audio so you can hear the interaction) \n Agent2 listens to the audio file using OpenAI’s speech-to-text \n Agent2 generates a response and converts it to speech \n \n The example includes helper functions for handling audio files: \n /** \n * Saves an audio stream to a file \n */ \n async function saveAudioToFile (audio: NodeJS. ReadableStream, filename: string): Promise &lt; void &gt; { \n const filePath = path. join (process. cwd (), filename); \n const writer = createWriteStream (filePath); \n audio. pipe (writer); \n return new Promise &lt; void &gt;((resolve, reject) =&gt; { \n writer. on ( ' finish ', resolve); \n writer. on ( ' error ', reject); \n }); \n} \n \n /** \n * Converts either a string or a readable stream to text \n */ \n async function convertToText (input: string | NodeJS. ReadableStream): Promise &lt; string &gt; { \n if ( typeof input === ' string ') { \n return input; \n } \n \n const chunks: Buffer [] = []; \n return new Promise &lt; string &gt;((resolve, reject) =&gt; { \n input. on ( ' data ', chunk =&gt; chunks. push (Buffer. from (chunk))); \n input. on ( ' error ', err =&gt; reject (err)); \n input. on ( ' end ', () =&gt; resolve (Buffer. concat (chunks). toString ( ' utf-8 '))); \n }); \n} \n Key Points \n \n The voice property in the Agent configuration accepts any implementation of MastraVoice \n CompositeVoice allows using different providers for speaking and listening \n Audio can be handled as streams, making it efficient for real-time processing \n Voice capabilities can be combined with the agent’s natural language processing \n \n \n Bird Checker Deploying an MCPServer",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Adding%20Voice%20Capabilities%20|%20Agents%20|%20Mastra&description=Example%20of%20adding%20voice%20capabilities%20to%20Mastra%20agents,%20enabling%20them%20to%20speak%20and%20listen%20using%20different%20voice%20providers.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/adding-voice-capabilities",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/agents/deploying-mcp-server",
            "title": "Example: Deploying an MCPServer | Agents | Mastra Docs",
            "url": "https://mastra.ai/examples/agents/deploying-mcp-server",
            "publishedDate": "",
            "author": "",
            "text": "\n This example guides you through setting up a basic Mastra MCPServer using the stdio transport, building it, and preparing it for deployment, such as publishing to NPM. \n Install Dependencies \n Install the necessary packages: \n pnpm add @mastra/mcp @mastra/core tsup \n Set up MCP Server \n \n \n Create a file for your stdio server, for example, /src/mastra/stdio.ts. \n \n \n Add the following code to the file. Remember to import your actual Mastra tools and name the server appropriately. \n #!/usr/bin/env node \n import { MCPServer } from \" @mastra/mcp \"; \n import { weatherTool } from \"./tools \"; \n \n const server = new MCPServer ({ \n name: \" my-mcp-server \", \n version: \" 1.0.0 \", \n tools: { weatherTool }, \n}); \n \n server. startStdio (). catch ((error) =&gt; { \n console. error ( \" Error running MCP server: \", error); \n process. exit ( 1); \n}); \n \n \n Update your package.json to include the bin entry pointing to your built server file and a script to build the server. \n \n \n { \n \" bin \": \" dist/stdio.js \", \n \" scripts \": { \n \" build:mcp \": \" tsup src/mastra/stdio.ts --format esm --no-splitting --dts &amp;&amp; chmod +x dist/stdio.js \" \n } \n} \n \n \n Run the build command: \n This will compile your server code and make the output file executable. \n \n \n Deploying to NPM \n To make your MCP server available for others (or yourself) to use via npx or as a dependency, you can publish it to NPM. \n \n \n Ensure you have an NPM account and are logged in ( npm login). \n \n \n Make sure your package name in package.json is unique and available. \n \n \n Run the publish command from your project root after building: \n npm publish --access public \n For more details on publishing packages, refer to the NPM documentation . \n \n \n Use the Deployed MCP Server \n Once published, your MCP server can be used by an MCPClient by specifying the command to run your package. You can also use any other MCP client like Claude desktop, Cursor, or Windsurf. \n import { MCPClient } from \" @mastra/mcp \"; \n \n const mcp = new MCPClient ({ \n servers: { \n // Give this MCP server instance a name \n yourServerName: { \n command: \" npx \", \n args: [ \" -y \", \" @your-org-name/your-package-name@latest \"], // Replace with your package name \n }, \n }, \n}); \n \n // You can then get tools or toolsets from this configuration to use in your agent \n const tools = await mcp. getTools (); \n const toolsets = await mcp. getToolsets (); \n Note: If you published without an organization scope, the args might just be [\"-y\", \"your-package-name@latest\"]. \n \n Give your Agent a voice Creating a Workflow",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Deploying%20an%20MCPServer%20|%20Agents%20|%20Mastra%20Docs&description=Example%20of%20setting%20up,%20building,%20and%20deploying%20a%20Mastra%20MCPServer%20using%20the%20stdio%20transport%20and%20publishing%20it%20to%20NPM.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/agents/deploying-mcp-server",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/creating-a-workflow",
            "title": "Example: Creating a Workflow | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/creating-a-workflow",
            "publishedDate": "",
            "author": "",
            "text": "\n A workflow allows you to define and execute sequences of operations in a structured path. This example shows a workflow with a single step. \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n input: z. number (), \n }), \n}); \n \n const stepOne = new Step ({ \n id: \" stepOne \", \n inputSchema: z. object ({ \n value: z. number (), \n }), \n outputSchema: z. object ({ \n doubledValue: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const doubledValue = context?.triggerData?.input * 2; \n return { doubledValue }; \n }, \n}); \n \n myWorkflow. step (stepOne). commit (); \n \n const { runId, start } = myWorkflow. createRun (); \n \n const res = await start ({ \n triggerData: { input: 90 }, \n}); \n \n console. log (res.results); \n \n Deploying an MCPServer Sequential Steps",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Creating%20a%20Workflow%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20define%20and%20execute%20a%20simple%20workflow%20with%20a%20single%20step.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/creating-a-workflow",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/sequential-steps",
            "title": "Example: Sequential Steps | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/sequential-steps",
            "publishedDate": "",
            "author": "",
            "text": "Workflow with Sequential Steps \n Workflow can be chained to run one after another in a specific sequence. \n Control Flow Diagram \n This example shows how to chain workflow steps by using the then method demonstrating how to pass data between sequential steps and execute them in order. \n Here’s the control flow diagram: \n Creating the Steps \n Let’s start by creating the steps and initializing the workflow. \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n \n const stepOne = new Step ({ \n id: \" stepOne \", \n execute: async ({ context }) =&gt; ({ \n doubledValue: context.triggerData.inputValue * 2, \n }), \n}); \n \n const stepTwo = new Step ({ \n id: \" stepTwo \", \n execute: async ({ context }) =&gt; { \n if (context.steps.stepOne.status!== \" success \") { \n return { incrementedValue: 0 } \n } \n \n return { incrementedValue: context.steps.stepOne.output.doubledValue + 1 } \n }, \n}); \n \n const stepThree = new Step ({ \n id: \" stepThree \", \n execute: async ({ context }) =&gt; { \n if (context.steps.stepTwo.status!== \" success \") { \n return { tripledValue: 0 } \n } \n \n return { tripledValue: context.steps.stepTwo.output.incrementedValue * 3 } \n }, \n}); \n \n // Build the workflow \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n Chaining the Steps and Executing the Workflow \n Now let’s chain the steps together. \n // sequential steps \n myWorkflow. step (stepOne). then (stepTwo). then (stepThree); \n \n myWorkflow. commit (); \n \n const { start } = myWorkflow. createRun (); \n \n const res = await start ({ triggerData: { inputValue: 90 } }); \n \n Creating a Workflow Parallel Steps",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Sequential%20Steps%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20chain%20workflow%20steps%20in%20a%20specific%20sequence,%20passing%20data%20between%20them.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/sequential-steps",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/parallel-steps",
            "title": "Example: Parallel Execution | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/parallel-steps",
            "publishedDate": "",
            "author": "",
            "text": "Parallel Execution with Steps \n When building AI applications, you often need to process multiple independent tasks simultaneously to improve efficiency. \n Control Flow Diagram \n This example shows how to structure a workflow that executes steps in parallel, with each branch handling its own data flow and dependencies. \n Here’s the control flow diagram: \n Creating the Steps \n Let’s start by creating the steps and initializing the workflow. \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n \n const stepOne = new Step ({ \n id: \" stepOne \", \n execute: async ({ context }) =&gt; ({ \n doubledValue: context.triggerData.inputValue * 2, \n }), \n}); \n \n const stepTwo = new Step ({ \n id: \" stepTwo \", \n execute: async ({ context }) =&gt; { \n if (context.steps.stepOne.status!== \" success \") { \n return { incrementedValue: 0 } \n } \n \n return { incrementedValue: context.steps.stepOne.output.doubledValue + 1 } \n }, \n}); \n \n const stepThree = new Step ({ \n id: \" stepThree \", \n execute: async ({ context }) =&gt; ({ \n tripledValue: context.triggerData.inputValue * 3, \n }), \n}); \n \n const stepFour = new Step ({ \n id: \" stepFour \", \n execute: async ({ context }) =&gt; { \n if (context.steps.stepThree.status!== \" success \") { \n return { isEven: false } \n } \n \n return { isEven: context.steps.stepThree.output.tripledValue % 2 === 0 } \n }, \n}); \n \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n Chaining and Parallelizing Steps \n Now we can add the steps to the workflow. Note the.then() method is used to chain the steps, but the.step() method is used to add the steps to the workflow. \n myWorkflow \n . step (stepOne) \n . then (stepTwo) // chain one \n . step (stepThree) \n . then (stepFour) // chain two \n . commit (); \n \n const { start } = myWorkflow. createRun (); \n \n const result = await start ({ triggerData: { inputValue: 3 } }); \n \n Sequential Steps Branching Paths",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Parallel%20Execution%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20execute%20multiple%20independent%20tasks%20in%20parallel%20within%20a%20workflow.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/parallel-steps",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/branching-paths",
            "title": "Example: Branching Paths | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/branching-paths",
            "publishedDate": "",
            "author": "",
            "text": "\n When processing data, you often need to take different actions based on intermediate results. This example shows how to create a workflow that splits into separate paths, where each path executes different steps based on the output of a previous step. \n Control Flow Diagram \n This example shows how to create a workflow that splits into separate paths, where each path executes different steps based on the output of a previous step. \n Here’s the control flow diagram: \n Creating the Steps \n Let’s start by creating the steps and initializing the workflow. \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \" \n \n const stepOne = new Step ({ \n id: \" stepOne \", \n execute: async ({ context }) =&gt; ({ \n doubledValue: context.triggerData.inputValue * 2 \n }) \n}); \n \n const stepTwo = new Step ({ \n id: \" stepTwo \", \n execute: async ({ context }) =&gt; { \n const stepOneResult = context. getStepResult &lt;{ doubledValue: number }&gt;( \" stepOne \"); \n if (! stepOneResult) { \n return { isDivisibleByFive: false } \n } \n \n return { isDivisibleByFive: stepOneResult.doubledValue % 5 === 0 } \n } \n}); \n \n \n const stepThree = new Step ({ \n id: \" stepThree \", \n execute: async ({ context }) =&gt; { \n const stepOneResult = context. getStepResult &lt;{ doubledValue: number }&gt;( \" stepOne \"); \n if (! stepOneResult) { \n return { incrementedValue: 0 } \n } \n \n return { incrementedValue: stepOneResult.doubledValue + 1 } \n } \n}); \n \n const stepFour = new Step ({ \n id: \" stepFour \", \n execute: async ({ context }) =&gt; { \n const stepThreeResult = context. getStepResult &lt;{ incrementedValue: number }&gt;( \" stepThree \"); \n if (! stepThreeResult) { \n return { isDivisibleByThree: false } \n } \n \n return { isDivisibleByThree: stepThreeResult.incrementedValue % 3 === 0 } \n } \n}); \n \n // New step that depends on both branches \n const finalStep = new Step ({ \n id: \" finalStep \", \n execute: async ({ context }) =&gt; { \n // Get results from both branches using getStepResult \n const stepTwoResult = context. getStepResult &lt;{ isDivisibleByFive: boolean }&gt;( \" stepTwo \"); \n const stepFourResult = context. getStepResult &lt;{ isDivisibleByThree: boolean }&gt;( \" stepFour \"); \n \n const isDivisibleByFive = stepTwoResult?.isDivisibleByFive || false; \n const isDivisibleByThree = stepFourResult?.isDivisibleByThree || false; \n \n return { \n summary: ` The number ${ context.triggerData.inputValue} when doubled ${ isDivisibleByFive ? ' is ' : ' is not '} divisible by 5, and when doubled and incremented ${ isDivisibleByThree ? ' is ' : ' is not '} divisible by 3. `, \n isDivisibleByFive, \n isDivisibleByThree \n } \n } \n}); \n \n // Build the workflow \n const myWorkflow = new Workflow ({ \n name: \" my-workflow \", \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n Branching Paths and Chaining Steps \n Now let’s configure the workflow with branching paths and merge them using the compound.after([]) syntax. \n // Create two parallel branches \n myWorkflow \n // First branch \n . step (stepOne) \n . then (stepTwo) \n \n // Second branch \n . after (stepOne) \n . step (stepThree) \n . then (stepFour) \n \n // Merge both branches using compound after syntax \n . after ([stepTwo, stepFour]) \n . step (finalStep) \n . commit (); \n \n const { start } = myWorkflow. createRun (); \n \n const result = await start ({ triggerData: { inputValue: 3 } }); \n console. log (result.steps.finalStep.output.summary); \n // Output: \"The number 3 when doubled is not divisible by 5, and when doubled and incremented is divisible by 3.\" \n Advanced Branching and Merging \n You can create more complex workflows with multiple branches and merge points: \n const complexWorkflow = new Workflow ({ \n name: \" complex-workflow \", \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n \n // Create multiple branches with different merge points \n complexWorkflow \n // Main step \n . step (stepOne) \n \n // First branch \n . then (stepTwo) \n \n // Second branch \n . after (stepOne) \n . step (stepThree) \n . then (stepFour) \n \n // Third branch (another path from stepOne) \n . after (stepOne) \n . step ( new Step ({ \n id: \" alternativePath \", \n execute: async ({ context }) =&gt; { \n const stepOneResult = context. getStepResult &lt;{ doubledValue: number }&gt;( \" stepOne \"); \n return { \n result: (stepOneResult?.doubledValue || 0) * 3 \n } \n } \n })) \n \n // Merge first and second branches \n . after ([stepTwo, stepFour]) \n . step ( new Step ({ \n id: \" partialMerge \", \n execute: async ({ context }) =&gt; { \n const stepTwoResult = context. getStepResult &lt;{ isDivisibleByFive: boolean }&gt;( \" stepTwo \"); \n const stepFourResult = context. getStepResult &lt;{ isDivisibleByThree: boolean }&gt;( \" stepFour \"); \n \n return { \n intermediateResult: \" Processed first two branches \", \n branchResults: { \n branch1: stepTwoResult?.isDivisibleByFive, \n branch2: stepFourResult?.isDivisibleByThree \n } \n } \n } \n })) \n \n // Final merge of all branches \n . after ([ \" partialMerge \", \" alternativePath \"]) \n . step ( new Step ({ \n id: \" finalMerge \", \n execute: async ({ context }) =&gt; { \n const partialMergeResult = context. getStepResult &lt;{ \n intermediateResult: string, \n branchResults: { branch1: boolean, branch2: boolean } \n }&gt;( \" partialMerge \"); \n \n const alternativePathResult = context. getStepResult &lt;{ result: number }&gt;( \" alternativePath \"); \n \n return { \n finalResult: \" All branches processed \", \n combinedData: { \n fromPartialMerge: partialMergeResult?.branchResults, \n fromAlternativePath: alternativePathResult?.result \n } \n } \n } \n })) \n . commit (); \n \n Parallel Steps Conditional Branching",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Branching%20Paths%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20create%20workflows%20with%20branching%20paths%20based%20on%20intermediate%20results.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/branching-paths",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/conditional-branching",
            "title": "Example: Conditional Branching (experimental) | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/conditional-branching",
            "publishedDate": "",
            "author": "",
            "text": "Workflow with Conditional Branching (experimental) \n Workflows often need to follow different paths based on conditions. This example demonstrates how to use if and else to create conditional branches in your workflows. \n Basic If/Else Example \n This example shows a simple workflow that takes different paths based on a numeric value: \n import { Mastra } from ' @mastra/core '; \n import { Step, Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n \n \n // Step that provides the initial value \n const startStep = new Step ({ \n id: ' start ', \n outputSchema: z. object ({ \n value: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n // Get the value from the trigger data \n const value = context.triggerData.inputValue; \n return { value }; \n }, \n}); \n \n // Step that handles high values \n const highValueStep = new Step ({ \n id: ' highValue ', \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n const value = context. getStepResult &lt;{ value: number }&gt;( ' start ')?.value; \n return { result: ` High value processed: ${ value}` }; \n }, \n}); \n \n // Step that handles low values \n const lowValueStep = new Step ({ \n id: ' lowValue ', \n outputSchema: z. object ({ \n result: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n const value = context. getStepResult &lt;{ value: number }&gt;( ' start ')?.value; \n return { result: ` Low value processed: ${ value}` }; \n }, \n}); \n \n // Final step that summarizes the result \n const finalStep = new Step ({ \n id: ' final ', \n outputSchema: z. object ({ \n summary: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n // Get the result from whichever branch executed \n const highResult = context. getStepResult &lt;{ result: string }&gt;( ' highValue ')?.result; \n const lowResult = context. getStepResult &lt;{ result: string }&gt;( ' lowValue ')?.result; \n \n const result = highResult || lowResult; \n return { summary: ` Processing complete: ${ result}` }; \n }, \n}); \n \n // Build the workflow with conditional branching \n const conditionalWorkflow = new Workflow ({ \n name: ' conditional-workflow ', \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n \n conditionalWorkflow \n . step (startStep) \n . if ( async ({ context }) =&gt; { \n const value = context. getStepResult &lt;{ value: number }&gt;( ' start ')?.value ?? 0; \n return value &gt;= 10; // Condition: value is 10 or greater \n }) \n . then (highValueStep) \n . then (finalStep) \n . else () \n . then (lowValueStep) \n . then (finalStep) // Both branches converge on the final step \n . commit (); \n \n // Register the workflow \n const mastra = new Mastra ({ \n workflows: { conditionalWorkflow }, \n}); \n \n // Example usage \n async function runWorkflow (inputValue: number) { \n const workflow = mastra. getWorkflow ( ' conditionalWorkflow '); \n const { start } = workflow. createRun (); \n \n const result = await start ({ \n triggerData: { inputValue }, \n }); \n \n console. log ( ' Workflow result: ', result.results); \n return result; \n} \n \n // Run with a high value (follows the \"if\" branch) \n const result1 = await runWorkflow ( 15); \n // Run with a low value (follows the \"else\" branch) \n const result2 = await runWorkflow ( 5); \n \n console. log ( ' Result 1: ', result1); \n console. log ( ' Result 2: ', result2); \n \n Using Reference-Based Conditions \n You can also use reference-based conditions with comparison operators: \n // Using reference-based conditions instead of functions \n conditionalWorkflow \n . step (startStep) \n . if ({ \n ref: { step: startStep, path: ' value ' }, \n query: { $gte: 10 }, // Condition: value is 10 or greater \n }) \n . then (highValueStep) \n . then (finalStep) \n . else () \n . then (lowValueStep) \n . then (finalStep) \n . commit (); \n \n Branching Paths Calling an Agent",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Conditional%20Branching%20(experimental)%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20create%20conditional%20branches%20in%20workflows%20using%20if/else%20statements.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/conditional-branching",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/calling-agent",
            "title": "Example: Calling an Agent from a Workflow | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/calling-agent",
            "publishedDate": "",
            "author": "",
            "text": "\n This example demonstrates how to create a workflow that calls an AI agent to process messages and generate responses, and execute it within a workflow step. \n import { openai } from \" @ai-sdk/openai \"; \n import { Mastra } from \" @mastra/core \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n \n const penguin = new Agent ({ \n name: \" agent skipper \", \n instructions: ` You are skipper from penguin of madagascar, reply as that `, \n model: openai ( \" gpt-4o-mini \"), \n}); \n \n const newWorkflow = new Workflow ({ \n name: \" pass message to the workflow \", \n triggerSchema: z. object ({ \n message: z. string (), \n }), \n}); \n \n const replyAsSkipper = new Step ({ \n id: \" reply \", \n outputSchema: z. object ({ \n reply: z. string (), \n }), \n execute: async ({ context, mastra }) =&gt; { \n const skipper = mastra?. getAgent ( ' penguin '); \n \n const res = await skipper?. generate ( \n context?.triggerData?.message, \n ); \n return { reply: res?.text || \"\" }; \n }, \n}); \n \n newWorkflow. step (replyAsSkipper); \n newWorkflow. commit (); \n \n const mastra = new Mastra ({ \n agents: { penguin }, \n workflows: { newWorkflow }, \n}); \n \n const { runId, start } = await mastra. getWorkflow ( \" newWorkflow \"). createRun (); \n \n const runResult = await start ({ \n triggerData: { message: \" Give me a run down of the mission to save private \" }, \n}); \n \n console. log (runResult.results); \n \n Conditional Branching Using a Tool as a Step",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Calling%20an%20Agent%20from%20a%20Workflow%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20call%20an%20AI%20agent%20from%20within%20a%20workflow%20step.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/calling-agent",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/using-a-tool-as-a-step",
            "title": "Example: Using a Tool as a Step | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/using-a-tool-as-a-step",
            "publishedDate": "",
            "author": "",
            "text": "Tool as a Workflow step \n This example demonstrates how to create and integrate a custom tool as a workflow step, showing how to define input/output schemas and implement the tool’s execution logic. \n import { createTool } from ' @mastra/core/tools '; \n import { Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n \n const crawlWebpage = createTool ({ \n id: ' Crawl Webpage ', \n description: ' Crawls a webpage and extracts the text content ', \n inputSchema: z. object ({ \n url: z. string (). url (), \n }), \n outputSchema: z. object ({ \n rawText: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n const response = await fetch (context.triggerData.url); \n const text = await response. text (); \n return { rawText: ' This is the text content of the webpage: ' + text }; \n }, \n}); \n \n const contentWorkflow = new Workflow ({ name: ' content-review ' }); \n \n contentWorkflow. step (crawlWebpage). commit (); \n \n const { start } = contentWorkflow. createRun (); \n \n const res = await start ({ triggerData: { url: ' https://example.com '} }); \n \n console. log (res.results); \n \n Calling an Agent Cyclical Dependencies",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Using%20a%20Tool%20as%20a%20Step%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20integrate%20a%20custom%20tool%20as%20a%20step%20in%20a%20workflow.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/using-a-tool-as-a-step",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/cyclical-dependencies",
            "title": "Example: Cyclical Dependencies | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/cyclical-dependencies",
            "publishedDate": "",
            "author": "",
            "text": "Workflow with Cyclical dependencies \n Workflows support cyclical dependencies where steps can loop back based on conditions. The example below shows how to use conditional logic to create loops and handle repeated execution. \n import { Workflow, Step } from ' @mastra/core '; \n import { z } from ' zod '; \n \n async function main () { \n const doubleValue = new Step ({ \n id: ' doubleValue ', \n description: ' Doubles the input value ', \n inputSchema: z. object ({ \n inputValue: z. number (), \n }), \n outputSchema: z. object ({ \n doubledValue: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const doubledValue = context.inputValue * 2; \n return { doubledValue }; \n }, \n }); \n \n const incrementByOne = new Step ({ \n id: ' incrementByOne ', \n description: ' Adds 1 to the input value ', \n outputSchema: z. object ({ \n incrementedValue: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const valueToIncrement = context?. getStepResult &lt;{ firstValue: number }&gt;( ' trigger ')?.firstValue; \n if (! valueToIncrement) throw new Error ( ' No value to increment provided '); \n const incrementedValue = valueToIncrement + 1; \n return { incrementedValue }; \n }, \n }); \n \n const cyclicalWorkflow = new Workflow ({ \n name: ' cyclical-workflow ', \n triggerSchema: z. object ({ \n firstValue: z. number (), \n }), \n }); \n \n cyclicalWorkflow \n . step (doubleValue, { \n variables: { \n inputValue: { \n step: ' trigger ', \n path: ' firstValue ', \n }, \n }, \n }) \n . then (incrementByOne) \n . after (doubleValue) \n . step (doubleValue, { \n variables: { \n inputValue: { \n step: doubleValue, \n path: ' doubledValue ', \n }, \n }, \n }) \n . commit (); \n \n const { runId, start } = cyclicalWorkflow. createRun (); \n \n console. log ( ' Run ', runId); \n \n const res = await start ({ triggerData: { firstValue: 6 } }); \n \n console. log (res.results); \n} \n \n main (); \n \n Using a Tool as a Step Workflow Variables",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Cyclical%20Dependencies%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20create%20workflows%20with%20cyclical%20dependencies%20and%20conditional%20loops.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/cyclical-dependencies",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/workflow-variables",
            "title": "Data Mapping with Workflow Variables | Mastra Examples",
            "url": "https://mastra.ai/examples/workflows/workflow-variables",
            "publishedDate": "",
            "author": "",
            "text": "This example demonstrates how to use workflow variables to map data between steps in a Mastra workflow. src/mastra/workflows/user-registration.ts import { Step, Workflow } from \" @mastra/core/workflows \"; \n import { z } from \" zod \"; \n \n // Define our schemas for better type safety \n const userInputSchema = z. object ({ \n email: z. string (). email (), \n name: z. string (), \n age: z. number (). min ( 18), \n}); \n \n const validatedDataSchema = z. object ({ \n isValid: z. boolean (), \n validatedData: z. object ({ \n email: z. string (), \n name: z. string (), \n age: z. number (), \n }), \n}); \n \n const formattedDataSchema = z. object ({ \n userId: z. string (), \n formattedData: z. object ({ \n email: z. string (), \n displayName: z. string (), \n ageGroup: z. string (), \n }), \n}); \n \n const profileSchema = z. object ({ \n profile: z. object ({ \n id: z. string (), \n email: z. string (), \n displayName: z. string (), \n ageGroup: z. string (), \n createdAt: z. string (), \n }), \n}); \n \n // Define the workflow \n const registrationWorkflow = new Workflow ({ \n name: \" user-registration \", \n triggerSchema: userInputSchema, \n}); \n \n // Step 1: Validate user input \n const validateInput = new Step ({ \n id: \" validateInput \", \n inputSchema: userInputSchema, \n outputSchema: validatedDataSchema, \n execute: async ({ context }) =&gt; { \n const { email, name, age } = context; \n \n // Simple validation logic \n const isValid = email. includes ( ' @ ') &amp;&amp; name.length &gt; 0 &amp;&amp; age &gt;= 18; \n \n return { \n isValid, \n validatedData: { \n email: email. toLowerCase (). trim (), \n name, \n age, \n }, \n }; \n }, \n}); \n \n // Step 2: Format user data \n const formatUserData = new Step ({ \n id: \" formatUserData \", \n inputSchema: z. object ({ \n validatedData: z. object ({ \n email: z. string (), \n name: z. string (), \n age: z. number (), \n }), \n }), \n outputSchema: formattedDataSchema, \n execute: async ({ context }) =&gt; { \n const { validatedData } = context; \n \n // Generate a simple user ID \n const userId = ` user_ ${ Math. floor (Math. random () * 10000)}`; \n \n // Format the data \n const ageGroup = validatedData.age &lt; 30 ? \" young-adult \" : \" adult \"; \n \n return { \n userId, \n formattedData: { \n email: validatedData.email, \n displayName: validatedData.name, \n ageGroup, \n }, \n }; \n }, \n}); \n \n // Step 3: Create user profile \n const createUserProfile = new Step ({ \n id: \" createUserProfile \", \n inputSchema: z. object ({ \n userId: z. string (), \n formattedData: z. object ({ \n email: z. string (), \n displayName: z. string (), \n ageGroup: z. string (), \n }), \n }), \n outputSchema: profileSchema, \n execute: async ({ context }) =&gt; { \n const { userId, formattedData } = context; \n \n // In a real app, you would save to a database here \n \n return { \n profile: { \n id: userId, \n ... formattedData, \n createdAt: new Date (). toISOString (), \n }, \n }; \n }, \n}); \n \n // Build the workflow with variable mappings \n registrationWorkflow \n // First step gets data from the trigger \n . step (validateInput, { \n variables: { \n email: { step: ' trigger ', path: ' email ' }, \n name: { step: ' trigger ', path: ' name ' }, \n age: { step: ' trigger ', path: ' age ' }, \n } \n }) \n // Format user data with validated data from previous step \n . then (formatUserData, { \n variables: { \n validatedData: { step: validateInput, path: ' validatedData ' }, \n }, \n when: { \n ref: { step: validateInput, path: ' isValid ' }, \n query: { $eq: true }, \n }, \n }) \n // Create profile with data from the format step \n . then (createUserProfile, { \n variables: { \n userId: { step: formatUserData, path: ' userId ' }, \n formattedData: { step: formatUserData, path: ' formattedData ' }, \n }, \n }) \n . commit (); \n \n export default registrationWorkflow; curl --location ' http://localhost:4111/api/workflows/user-registration/start-async ' \\ \n --header ' Content-Type: application/json ' \\ \n --data ' { \n \"email\": \"user@example.com\", \n \"name\": \"John Doe\", \n \"age\": 25 \n } '",
            "image": "https://mastra.ai/api/og/docs?title=Data%20Mapping%20with%20Workflow%20Variables%20|%20Mastra%20Examples&description=Learn%20how%20to%20use%20workflow%20variables%20to%20map%20data%20between%20steps%20in%20Mastra%20workflows.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/workflow-variables",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/human-in-the-loop",
            "title": "Example: Human in the Loop | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/human-in-the-loop",
            "publishedDate": "",
            "author": "",
            "text": "Human in the Loop Workflow \n Human-in-the-loop workflows allow you to pause execution at specific points to collect user input, make decisions, or perform actions that require human judgment. This example demonstrates how to create a workflow with human intervention points. \n How It Works \n \n A workflow step can suspend execution using the suspend() function, optionally passing a payload with context for the human decision maker. \n When the workflow is resumed, the human input is passed in the context parameter of the resume() call. \n This input becomes available in the step’s execution context as context.inputData, which is typed according to the step’s inputSchema. \n The step can then continue execution based on the human input. \n \n This pattern allows for safe, type-checked human intervention in automated workflows. \n Interactive Terminal Example Using Inquirer \n This example demonstrates how to use the Inquirer  library to collect user input directly from the terminal when a workflow is suspended, creating a truly interactive human-in-the-loop experience. \n import { Mastra } from ' @mastra/core '; \n import { Step, Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n import { confirm, input, select } from ' @inquirer/prompts '; \n \n // Step 1: Generate product recommendations \n const generateRecommendations = new Step ({ \n id: ' generateRecommendations ', \n outputSchema: z. object ({ \n customerName: z. string (), \n recommendations: z. array ( \n z. object ({ \n productId: z. string (), \n productName: z. string (), \n price: z. number (), \n description: z. string (), \n }), \n ), \n }), \n execute: async ({ context }) =&gt; { \n const customerName = context.triggerData.customerName; \n \n // In a real application, you might call an API or ML model here \n // For this example, we'll return mock data \n return { \n customerName, \n recommendations: [ \n { \n productId: ' prod-001 ', \n productName: ' Premium Widget ', \n price: 99.99, \n description: ' Our best-selling premium widget with advanced features ', \n }, \n { \n productId: ' prod-002 ', \n productName: ' Basic Widget ', \n price: 49.99, \n description: ' Affordable entry-level widget for beginners ', \n }, \n { \n productId: ' prod-003 ', \n productName: ' Widget Pro Plus ', \n price: 149.99, \n description: ' Professional-grade widget with extended warranty ', \n }, \n ], \n }; \n }, \n}); \n // Step 2: Get human approval and customization for the recommendations \n const reviewRecommendations = new Step ({ \n id: ' reviewRecommendations ', \n inputSchema: z. object ({ \n approvedProducts: z. array (z. string ()), \n customerNote: z. string (). optional (), \n offerDiscount: z. boolean (). optional (), \n }), \n outputSchema: z. object ({ \n finalRecommendations: z. array ( \n z. object ({ \n productId: z. string (), \n productName: z. string (), \n price: z. number (), \n }), \n ), \n customerNote: z. string (). optional (), \n offerDiscount: z. boolean (), \n }), \n execute: async ({ context, suspend }) =&gt; { \n const { customerName, recommendations } = context. getStepResult (generateRecommendations) || { \n customerName: '', \n recommendations: [], \n }; \n \n // Check if we have input from a resumed workflow \n const reviewInput = { \n approvedProducts: context.inputData?.approvedProducts || [], \n customerNote: context.inputData?.customerNote, \n offerDiscount: context.inputData?.offerDiscount, \n }; \n \n // If we don't have agent input yet, suspend for human review \n if (! reviewInput.approvedProducts.length) { \n console. log ( ` Generating recommendations for customer: ${ customerName}`); \n await suspend ({ \n customerName, \n recommendations, \n message: ' Please review these product recommendations before sending to the customer ', \n }); \n \n // Placeholder return (won't be reached due to suspend) \n return { \n finalRecommendations: [], \n customerNote: '', \n offerDiscount: false, \n }; \n } \n \n // Process the agent's product selections \n const finalRecommendations = recommendations \n . filter (product =&gt; reviewInput.approvedProducts. includes (product.productId)) \n . map (product =&gt; ({ \n productId: product.productId, \n productName: product.productName, \n price: product.price, \n })); \n \n return { \n finalRecommendations, \n customerNote: reviewInput.customerNote || '', \n offerDiscount: reviewInput.offerDiscount || false, \n }; \n }, \n}); \n // Step 3: Send the recommendations to the customer \n const sendRecommendations = new Step ({ \n id: ' sendRecommendations ', \n outputSchema: z. object ({ \n emailSent: z. boolean (), \n emailContent: z. string (), \n }), \n execute: async ({ context }) =&gt; { \n const { customerName } = context. getStepResult (generateRecommendations) || { customerName: '' }; \n const { finalRecommendations, customerNote, offerDiscount } = context. getStepResult (reviewRecommendations) || { \n finalRecommendations: [], \n customerNote: '', \n offerDiscount: false, \n }; \n \n // Generate email content based on the recommendations \n let emailContent = ` Dear ${ customerName},\\n\\nBased on your preferences, we recommend:\\n\\n `; \n \n finalRecommendations. forEach (product =&gt; { \n emailContent += ` - ${ product.productName}: $ ${ product.price. toFixed ( 2)} \\n `; \n }); \n \n if (offerDiscount) { \n emailContent += ' \\nAs a valued customer, use code SAVE10 for 10% off your next purchase!\\n '; \n } \n \n if (customerNote) { \n emailContent += ` \\nPersonal note: ${ customerNote} \\n `; \n } \n \n emailContent += ' \\nThank you for your business,\\nThe Sales Team '; \n \n // In a real application, you would send this email \n console. log ( ' Email content generated: ', emailContent); \n \n return { \n emailSent: true, \n emailContent, \n }; \n }, \n}); \n \n // Build the workflow \n const recommendationWorkflow = new Workflow ({ \n name: ' product-recommendation-workflow ', \n triggerSchema: z. object ({ \n customerName: z. string (), \n }), \n}); \n \n recommendationWorkflow \n. step (generateRecommendations) \n. then (reviewRecommendations) \n. then (sendRecommendations) \n. commit (); \n \n // Register the workflow \n const mastra = new Mastra ({ \n workflows: { recommendationWorkflow }, \n}); \n // Example of using the workflow with Inquirer prompts \n async function runRecommendationWorkflow () { \n const registeredWorkflow = mastra. getWorkflow ( ' recommendationWorkflow '); \n const run = registeredWorkflow. createRun (); \n \n console. log ( ' Starting product recommendation workflow... '); \n const result = await run. start ({ \n triggerData: { \n customerName: ' Jane Smith ', \n }, \n }); \n \n const isReviewStepSuspended = result.activePaths. get ( ' reviewRecommendations ')?.status === ' suspended '; \n \n // Check if workflow is suspended for human review \n if (isReviewStepSuspended) { \n const { customerName, recommendations, message } = result.activePaths. get ( ' reviewRecommendations ')?.suspendPayload; \n \n console. log ( ' \\n=================================== '); \n console. log (message); \n console. log ( ` Customer: ${ customerName}`); \n console. log ( ' ===================================\\n '); \n \n // Use Inquirer to collect input from the sales agent in the terminal \n console. log ( ' Available product recommendations: '); \n recommendations. forEach ((product, index) =&gt; { \n console. log ( `${ index + 1}. ${ product.productName} - $ ${ product.price. toFixed ( 2)}`); \n console. log ( ` ${ product.description} \\n `); \n }); \n \n // Let the agent select which products to recommend \n const approvedProducts = await checkbox ({ \n message: ' Select products to recommend to the customer: ', \n choices: recommendations. map (product =&gt; ({ \n name: `${ product.productName} ($ ${ product.price. toFixed ( 2)}) `, \n value: product.productId, \n })), \n }); \n \n // Let the agent add a personal note \n const includeNote = await confirm ({ \n message: ' Would you like to add a personal note? ', \n default: false, \n }); \n \n let customerNote = ''; \n if (includeNote) { \n customerNote = await input ({ \n message: ' Enter your personalized note for the customer: ', \n }); \n } \n \n // Ask if a discount should be offered \n const offerDiscount = await confirm ({ \n message: ' Offer a 10% discount to this customer? ', \n default: false, \n }); \n \n console. log ( ' \\nSubmitting your review... '); \n \n // Resume the workflow with the agent's input \n const resumeResult = await run. resume ({ \n stepId: ' reviewRecommendations ', \n context: { \n approvedProducts, \n customerNote, \n offerDiscount, \n }, \n }); \n \n console. log ( ' \\n=================================== '); \n console. log ( ' Workflow completed! '); \n console. log ( ' Email content: '); \n console. log ( ' ===================================\\n '); \n console. log (resumeResult?.results?.sendRecommendations || ' No email content generated '); \n \n return resumeResult; \n } \n \n return result; \n} \n \n // Invoke the workflow with interactive terminal input \n runRecommendationWorkflow (). catch (console.error); \n Advanced Example with Multiple User Inputs \n This example demonstrates a more complex workflow that requires multiple human intervention points, such as in a content moderation system. \n import { Mastra } from ' @mastra/core '; \n import { Step, Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n import { select, input } from ' @inquirer/prompts '; \n \n // Step 1: Receive and analyze content \n const analyzeContent = new Step ({ \n id: ' analyzeContent ', \n outputSchema: z. object ({ \n content: z. string (), \n aiAnalysisScore: z. number (), \n flaggedCategories: z. array (z. string ()). optional (), \n }), \n execute: async ({ context }) =&gt; { \n const content = context.triggerData.content; \n \n // Simulate AI analysis \n const aiAnalysisScore = simulateContentAnalysis (content); \n const flaggedCategories = aiAnalysisScore &lt; 0.7 \n ? [ ' potentially inappropriate ', ' needs review '] \n : []; \n \n return { \n content, \n aiAnalysisScore, \n flaggedCategories, \n }; \n }, \n}); \n // Step 2: Moderate content that needs review \n const moderateContent = new Step ({ \n id: ' moderateContent ', \n // Define the schema for human input that will be provided when resuming \n inputSchema: z. object ({ \n moderatorDecision: z. enum ([ ' approve ', ' reject ', ' modify ']). optional (), \n moderatorNotes: z. string (). optional (), \n modifiedContent: z. string (). optional (), \n }), \n outputSchema: z. object ({ \n moderationResult: z. enum ([ ' approved ', ' rejected ', ' modified ']), \n moderatedContent: z. string (), \n notes: z. string (). optional (), \n }), \n // @ts-ignore \n execute: async ({ context, suspend }) =&gt; { \n const analysisResult = context. getStepResult (analyzeContent); \n // Access the input provided when resuming the workflow \n const moderatorInput = { \n decision: context.inputData?.moderatorDecision, \n notes: context.inputData?.moderatorNotes, \n modifiedContent: context.inputData?.modifiedContent, \n }; \n \n // If the AI analysis score is high enough, auto-approve \n if (analysisResult?.aiAnalysisScore &gt; 0.9 &amp;&amp; ! analysisResult?.flaggedCategories?.length) { \n return { \n moderationResult: ' approved ', \n moderatedContent: analysisResult.content, \n notes: ' Auto-approved by system ', \n }; \n } \n \n // If we don't have moderator input yet, suspend for human review \n if (! moderatorInput.decision) { \n await suspend ({ \n content: analysisResult?.content, \n aiScore: analysisResult?.aiAnalysisScore, \n flaggedCategories: analysisResult?.flaggedCategories, \n message: ' Please review this content and make a moderation decision ', \n }); \n \n // Placeholder return \n return { \n moderationResult: ' approved ', \n moderatedContent: '', \n }; \n } \n \n // Process the moderator's decision \n switch (moderatorInput.decision) { \n case ' approve ': \n return { \n moderationResult: ' approved ', \n moderatedContent: analysisResult?.content || '', \n notes: moderatorInput.notes || ' Approved by moderator ', \n }; \n \n case ' reject ': \n return { \n moderationResult: ' rejected ', \n moderatedContent: '', \n notes: moderatorInput.notes || ' Rejected by moderator ', \n }; \n \n case ' modify ': \n return { \n moderationResult: ' modified ', \n moderatedContent: moderatorInput.modifiedContent || analysisResult?.content || '', \n notes: moderatorInput.notes || ' Modified by moderator ', \n }; \n \n default: \n return { \n moderationResult: ' rejected ', \n moderatedContent: '', \n notes: ' Invalid moderator decision ', \n }; \n } \n }, \n}); \n // Step 3: Apply moderation actions \n const applyModeration = new Step ({ \n id: ' applyModeration ', \n outputSchema: z. object ({ \n finalStatus: z. string (), \n content: z. string (). optional (), \n auditLog: z. object ({ \n originalContent: z. string (), \n moderationResult: z. string (), \n aiScore: z. number (), \n timestamp: z. string (), \n }), \n }), \n execute: async ({ context }) =&gt; { \n const analysisResult = context. getStepResult (analyzeContent); \n const moderationResult = context. getStepResult (moderateContent); \n \n // Create audit log \n const auditLog = { \n originalContent: analysisResult?.content || '', \n moderationResult: moderationResult?.moderationResult || ' unknown ', \n aiScore: analysisResult?.aiAnalysisScore || 0, \n timestamp: new Date (). toISOString (), \n }; \n \n // Apply moderation action \n switch (moderationResult?.moderationResult) { \n case ' approved ': \n return { \n finalStatus: ' Content published ', \n content: moderationResult.moderatedContent, \n auditLog, \n }; \n \n case ' modified ': \n return { \n finalStatus: ' Content modified and published ', \n content: moderationResult.moderatedContent, \n auditLog, \n }; \n \n case ' rejected ': \n return { \n finalStatus: ' Content rejected ', \n auditLog, \n }; \n \n default: \n return { \n finalStatus: ' Error in moderation process ', \n auditLog, \n }; \n } \n }, \n}); \n // Build the workflow \n const contentModerationWorkflow = new Workflow ({ \n name: ' content-moderation-workflow ', \n triggerSchema: z. object ({ \n content: z. string (), \n }), \n}); \n \n contentModerationWorkflow \n . step (analyzeContent) \n . then (moderateContent) \n . then (applyModeration) \n . commit (); \n \n // Register the workflow \n const mastra = new Mastra ({ \n workflows: { contentModerationWorkflow }, \n}); \n \n // Example of using the workflow with Inquirer prompts \n async function runModerationDemo () { \n const registeredWorkflow = mastra. getWorkflow ( ' contentModerationWorkflow '); \n const run = registeredWorkflow. createRun (); \n \n // Start the workflow with content that needs review \n console. log ( ' Starting content moderation workflow... '); \n const result = await run. start ({ \n triggerData: { \n content: ' This is some user-generated content that requires moderation. ' \n } \n }); \n \n const isReviewStepSuspended = result.activePaths. get ( ' moderateContent ')?.status === ' suspended '; \n \n // Check if workflow is suspended \n if (isReviewStepSuspended) { \n const { content, aiScore, flaggedCategories, message } = result.activePaths. get ( ' moderateContent ')?.suspendPayload; \n \n console. log ( ' \\n=================================== '); \n console. log (message); \n console. log ( ' ===================================\\n '); \n \n console. log ( ' Content to review: '); \n console. log (content); \n console. log ( ` \\nAI Analysis Score: ${ aiScore}`); \n console. log ( ` Flagged Categories: ${ flaggedCategories?. join ( ', ') || ' None '} \\n `); \n \n // Collect moderator decision using Inquirer \n const moderatorDecision = await select ({ \n message: ' Select your moderation decision: ', \n choices: [ \n { name: ' Approve content as is ', value: ' approve ' }, \n { name: ' Reject content completely ', value: ' reject ' }, \n { name: ' Modify content before publishing ', value: ' modify ' } \n ], \n }); \n \n // Collect additional information based on decision \n let moderatorNotes = ''; \n let modifiedContent = ''; \n \n moderatorNotes = await input ({ \n message: ' Enter any notes about your decision: ', \n }); \n \n if (moderatorDecision === ' modify ') { \n modifiedContent = await input ({ \n message: ' Enter the modified content: ', \n default: content, \n }); \n } \n \n console. log ( ' \\nSubmitting your moderation decision... '); \n \n // Resume the workflow with the moderator's input \n const resumeResult = await run. resume ({ \n stepId: ' moderateContent ', \n context: { \n moderatorDecision, \n moderatorNotes, \n modifiedContent, \n }, \n }); \n \n if (resumeResult?.results?.applyModeration?.status === ' success ') { \n console. log ( ' \\n=================================== '); \n console. log ( ` Moderation complete: ${ resumeResult?.results?.applyModeration?.output.finalStatus}`); \n console. log ( ' ===================================\\n '); \n \n if (resumeResult?.results?.applyModeration?.output.content) { \n console. log ( ' Published content: '); \n console. log (resumeResult.results.applyModeration.output.content); \n } \n } \n \n return resumeResult; \n } \n \n console. log ( ' Workflow completed without requiring human intervention: ', result.results); \n return result; \n} \n \n // Helper function for AI content analysis simulation \n function simulateContentAnalysis (content: string): number { \n // In a real application, this would call an AI service \n // For the example, we're returning a random score \n return Math. random (); \n} \n \n // Invoke the demo function \n runModerationDemo (). catch (console.error); \n Key Concepts \n \n \n Suspension Points - Use the suspend() function within a step’s execute to pause workflow execution. \n \n \n Suspension Payload - Pass relevant data when suspending to provide context for human decision-making: \n \n \n await suspend ({ \n messageForHuman: ' Please review this data ', \n data: someImportantData \n }); \n \n Checking Workflow Status - After starting a workflow, check the returned status to see if it’s suspended: \n \n const result = await workflow. start ({ triggerData }); \n if (result.status === ' suspended ' &amp;&amp; result.suspendedStepId === ' stepId ') { \n // Process suspension \n console. log ( ' Workflow is waiting for input: ', result.suspendPayload); \n } \n \n Interactive Terminal Input - Use libraries like Inquirer to create interactive prompts: \n \n import { select, input, confirm } from ' @inquirer/prompts '; \n \n // When the workflow is suspended \n if (result.status === ' suspended ') { \n // Display information from the suspend payload \n console. log (result.suspendPayload.message); \n \n // Collect user input interactively \n const decision = await select ({ \n message: ' What would you like to do? ', \n choices: [ \n { name: ' Approve ', value: ' approve ' }, \n { name: ' Reject ', value: ' reject ' } \n ] \n }); \n \n // Resume the workflow with the collected input \n await run. resume ({ \n stepId: result.suspendedStepId, \n context: { decision } \n }); \n } \n \n Resuming Workflow - Use the resume() method to continue workflow execution with human input: \n \n const resumeResult = await run. resume ({ \n stepId: ' suspendedStepId ', \n context: { \n // This data is passed to the suspended step as context.inputData \n // and must conform to the step's inputSchema \n userDecision: ' approve ' \n }, \n }); \n \n Input Schema for Human Data - Define an input schema on steps that might be resumed with human input to ensure type safety: \n \n const myStep = new Step ({ \n id: ' myStep ', \n inputSchema: z. object ({ \n // This schema validates the data passed in resume's context \n // and makes it available as context.inputData \n userDecision: z. enum ([ ' approve ', ' reject ']), \n userComments: z. string (). optional (), \n }), \n execute: async ({ context, suspend }) =&gt; { \n // Check if we have user input from a previous suspension \n if (context.inputData?.userDecision) { \n // Process the user's decision \n return { result: ` User decided: ${ context.inputData.userDecision}` }; \n } \n \n // If no input, suspend for human decision \n await suspend (); \n } \n }); \n Human-in-the-loop workflows are powerful for building systems that blend automation with human judgment, such as: \n \n Content moderation systems \n Approval workflows \n Supervised AI systems \n Customer service automation with escalation \n \n \n",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Human%20in%20the%20Loop%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20create%20workflows%20with%20human%20intervention%20points.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/human-in-the-loop",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows/suspend-and-resume",
            "title": "Example: Suspend and Resume | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows/suspend-and-resume",
            "publishedDate": "",
            "author": "",
            "text": "Workflow with Suspend and Resume \n Workflow steps can be suspended and resumed at any point in the workflow execution. This example demonstrates how to suspend a workflow step and resume it later. \n Basic Example \n import { Mastra } from ' @mastra/core '; \n import { Step, Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n \n const stepOne = new Step ({ \n id: ' stepOne ', \n outputSchema: z. object ({ \n doubledValue: z. number (), \n }), \n execute: async ({ context }) =&gt; { \n const doubledValue = context.triggerData.inputValue * 2; \n return { doubledValue }; \n }, \n}); \n const stepTwo = new Step ({ \n id: ' stepTwo ', \n outputSchema: z. object ({ \n incrementedValue: z. number (), \n }), \n execute: async ({ context, suspend }) =&gt; { \n \n const secondValue = context.inputData?.secondValue?? 0; \n const doubledValue = context. getStepResult (stepOne)?.doubledValue?? 0; \n \n const incrementedValue = doubledValue + secondValue; \n \n if (incrementedValue &lt; 100) { \n await suspend (); \n return { incrementedValue: 0 }; \n } \n return { incrementedValue }; \n }, \n}); \n \n // Build the workflow \n const myWorkflow = new Workflow ({ \n name: ' my-workflow ', \n triggerSchema: z. object ({ \n inputValue: z. number (), \n }), \n}); \n \n // run workflows in parallel \n myWorkflow \n . step (stepOne) \n . then (stepTwo) \n . commit (); \n // Register the workflow \n export const mastra = new Mastra ({ \n workflows: { registeredWorkflow: myWorkflow }, \n}) \n \n // Get registered workflow from Mastra \n const registeredWorkflow = mastra. getWorkflow ( ' registeredWorkflow '); \n const { runId, start } = registeredWorkflow. createRun (); \n \n // Start watching the workflow before executing it \n myWorkflow. watch ( async ({ context, activePaths }) =&gt; { \n for ( const _path of activePaths) { \n const stepTwoStatus = context.steps?.stepTwo?.status; \n if (stepTwoStatus === ' suspended ') { \n console. log ( \" Workflow suspended, resuming with new value \"); \n \n // Resume the workflow with new context \n await myWorkflow. resume ({ \n runId, \n stepId: ' stepTwo ', \n context: { secondValue: 100 }, \n }); \n } \n } \n}) \n \n // Start the workflow execution \n await start ({ triggerData: { inputValue: 45 } }); \n Advanced Example with Multiple Suspension Points Using async/await pattern and suspend payloads \n This example demonstrates a more complex workflow with multiple suspension points using the async/await pattern. It simulates a content generation workflow that requires human intervention at different stages. \n import { Mastra } from ' @mastra/core '; \n import { Step, Workflow } from ' @mastra/core/workflows '; \n import { z } from ' zod '; \n \n // Step 1: Get user input \n const getUserInput = new Step ({ \n id: ' getUserInput ', \n execute: async ({ context }) =&gt; { \n // In a real application, this might come from a form or API \n return { userInput: context.triggerData.input }; \n }, \n outputSchema: z. object ({ userInput: z. string () }), \n}); \n // Step 2: Generate content with AI (may suspend for human guidance) \n const promptAgent = new Step ({ \n id: ' promptAgent ', \n inputSchema: z. object ({ \n guidance: z. string (), \n }), \n execute: async ({ context, suspend }) =&gt; { \n const userInput = context. getStepResult (getUserInput)?.userInput; \n console. log ( ` Generating content based on: ${ userInput}`); \n \n const guidance = context.inputData?.guidance; \n \n // Simulate AI generating content \n const initialDraft = generateInitialDraft (userInput); \n \n // If confidence is high, return the generated content directly \n if (initialDraft.confidenceScore &gt; 0.7) { \n return { modelOutput: initialDraft.content }; \n } \n \n console. log ( ' Low confidence in generated content, suspending for human guidance ', {guidance}); \n \n // If confidence is low, suspend for human guidance \n if (! guidance) { \n // only suspend if no guidance is provided \n await suspend (); \n return undefined; \n } \n \n // This code runs after resume with human guidance \n console. log ( ' Resumed with human guidance '); \n \n // Use the human guidance to improve the output \n return { \n modelOutput: enhanceWithGuidance (initialDraft.content, guidance), \n }; \n }, \n outputSchema: z. object ({ modelOutput: z. string () }). optional (), \n}); \n // Step 3: Evaluate the content quality \n const evaluateTone = new Step ({ \n id: ' evaluateToneConsistency ', \n execute: async ({ context }) =&gt; { \n const content = context. getStepResult (promptAgent)?.modelOutput; \n \n // Simulate evaluation \n return { \n toneScore: { score: calculateToneScore (content) }, \n completenessScore: { score: calculateCompletenessScore (content) }, \n }; \n }, \n outputSchema: z. object ({ \n toneScore: z. any (), \n completenessScore: z. any (), \n }), \n}); \n // Step 4: Improve response if needed (may suspend) \n const improveResponse = new Step ({ \n id: ' improveResponse ', \n inputSchema: z. object ({ \n improvedContent: z. string (), \n resumeAttempts: z. number (), \n }), \n execute: async ({ context, suspend }) =&gt; { \n const content = context. getStepResult (promptAgent)?.modelOutput; \n const toneScore = \n context. getStepResult (evaluateTone)?.toneScore.score?? 0; \n const completenessScore = \n context. getStepResult (evaluateTone)?.completenessScore.score?? 0; \n \n const improvedContent = context.inputData.improvedContent; \n const resumeAttempts = context.inputData.resumeAttempts?? 0; \n \n // If scores are above threshold, make minor improvements \n if (toneScore &gt; 0.8 &amp;&amp; completenessScore &gt; 0.8) { \n return { improvedOutput: makeMinorImprovements (content) }; \n } \n \n console. log ( ' Content quality below threshold, suspending for human intervention ', {improvedContent, resumeAttempts}); \n \n if (! improvedContent) { \n // Suspend with payload containing content and resume attempts \n await suspend ({ \n content, \n scores: { tone: toneScore, completeness: completenessScore }, \n needsImprovement: toneScore &lt; 0.8 ? ' tone ' : ' completeness ', \n resumeAttempts: resumeAttempts + 1, \n }); \n return { improvedOutput: content?? '' }; \n } \n \n console. log ( ' Resumed with human improvements ', improvedContent); \n return { improvedOutput: improvedContent?? content?? '' }; \n }, \n outputSchema: z. object ({ improvedOutput: z. string () }). optional (), \n}); \n // Step 5: Final evaluation \n const evaluateImproved = new Step ({ \n id: ' evaluateImprovedResponse ', \n execute: async ({ context }) =&gt; { \n const improvedContent = context. getStepResult (improveResponse)?.improvedOutput; \n \n // Simulate final evaluation \n return { \n toneScore: { score: calculateToneScore (improvedContent) }, \n completenessScore: { score: calculateCompletenessScore (improvedContent) }, \n }; \n }, \n outputSchema: z. object ({ \n toneScore: z. any (), \n completenessScore: z. any (), \n }), \n}); \n \n // Build the workflow \n const contentWorkflow = new Workflow ({ \n name: ' content-generation-workflow ', \n triggerSchema: z. object ({ input: z. string () }), \n}); \n \n contentWorkflow \n . step (getUserInput) \n . then (promptAgent) \n . then (evaluateTone) \n . then (improveResponse) \n . then (evaluateImproved) \n . commit (); \n // Register the workflow \n const mastra = new Mastra ({ \n workflows: { contentWorkflow }, \n}); \n \n // Helper functions (simulated) \n function generateInitialDraft (input: string = '') { \n // Simulate AI generating content \n return { \n content: ` Generated content based on: ${ input}`, \n confidenceScore: 0.6, // Simulate low confidence to trigger suspension \n }; \n} \n \n function enhanceWithGuidance (content: string = '', guidance: string = '') { \n return `${ content} (Enhanced with guidance: ${ guidance}) `; \n} \n \n function makeMinorImprovements (content: string = '') { \n return `${ content} (with minor improvements) `; \n} \n \n function calculateToneScore (_: string = '') { \n return 0.7; // Simulate a score that will trigger suspension \n} \n \n function calculateCompletenessScore (_: string = '') { \n return 0.9; \n} \n \n // Usage example \n async function runWorkflow () { \n const workflow = mastra. getWorkflow ( ' contentWorkflow '); \n const { runId, start } = workflow. createRun (); \n \n let finalResult: any; \n \n // Start the workflow \n const initialResult = await start ({ \n triggerData: { input: ' Create content about sustainable energy ' }, \n }); \n \n console. log ( ' Initial workflow state: ', initialResult.results); \n \n const promptAgentStepResult = initialResult.activePaths. get ( ' promptAgent '); \n \n // Check if promptAgent step is suspended \n if (promptAgentStepResult?.status === ' suspended ') { \n console. log ( ' Workflow suspended at promptAgent step '); \n console. log ( ' Suspension payload: ', promptAgentStepResult?.suspendPayload); \n \n // Resume with human guidance \n const resumeResult1 = await workflow. resume ({ \n runId, \n stepId: ' promptAgent ', \n context: { \n guidance: ' Focus more on solar and wind energy technologies ', \n }, \n }); \n \n console. log ( ' Workflow resumed and continued to next steps '); \n \n let improveResponseResumeAttempts = 0; \n let improveResponseStatus = resumeResult1?.activePaths. get ( ' improveResponse ')?.status; \n \n // Check if improveResponse step is suspended \n while (improveResponseStatus === ' suspended ') { \n console. log ( ' Workflow suspended at improveResponse step '); \n console. log ( ' Suspension payload: ', resumeResult1?.activePaths. get ( ' improveResponse ')?.suspendPayload); \n \n const improvedContent = \n improveResponseResumeAttempts &lt; 3 \n ? undefined \n : ' Completely revised content about sustainable energy focusing on solar and wind technologies '; \n \n // Resume with human improvements \n finalResult = await workflow. resume ({ \n runId, \n stepId: ' improveResponse ', \n context: { \n improvedContent, \n resumeAttempts: improveResponseResumeAttempts, \n }, \n }); \n \n improveResponseResumeAttempts = \n finalResult?.activePaths. get ( ' improveResponse ')?.suspendPayload?.resumeAttempts ?? 0; \n improveResponseStatus = finalResult?.activePaths. get ( ' improveResponse ')?.status; \n \n console. log ( ' Improved response result: ', finalResult?.results); \n } \n } \n return finalResult; \n} \n \n // Run the workflow \n const result = await runWorkflow (); \n console. log ( ' Workflow completed '); \n console. log ( ' Final workflow result: ', result); \n \n",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Suspend%20and%20Resume%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20suspend%20and%20resume%20workflow%20steps%20during%20execution.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows/suspend-and-resume",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows_vNext/conditional-branching",
            "title": "Example: Conditional Branching | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows_vNext/conditional-branching",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "Workflows often need to follow different paths based on some condition.\nThis example demonstrates how to use the branch construct to create conditional flows within your workflows. Define a planning agent which leverages an LLM call to plan activities given a location and corresponding weather conditions. import { Agent } from ' @mastra/core/agent ' \n import { openai } from ' @ai-sdk/openai ' \n \n const llm = openai ( ' gpt-4o ') \n \n const planningAgent = new Agent ({ \n name: ' planningAgent ', \n model: llm, \n instructions: ` \n You are a local activities and travel expert who excels at weather-based planning. Analyze the weather data and provide practical activity recommendations. \n \n 📅 [Day, Month Date, Year] \n ═══════════════════════════ \n \n 🌡️ WEATHER SUMMARY \n • Conditions: [brief description] \n • Temperature: [X°C/Y°F to A°C/B°F] \n • Precipitation: [X% chance] \n \n 🌅 MORNING ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🌞 AFTERNOON ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🏠 INDOOR ALTERNATIVES \n • [Activity Name] - [Brief description including specific venue] \n Ideal for: [weather condition that would trigger this alternative] \n \n ⚠️ SPECIAL CONSIDERATIONS \n • [Any relevant weather warnings, UV index, wind conditions, etc.] \n \n Guidelines: \n - Suggest 2-3 time-specific outdoor activities per day \n - Include 1-2 indoor backup options \n - For precipitation &gt;50%, lead with indoor activities \n - All activities must be specific to the location \n - Include specific venues, trails, or locations \n - Consider activity intensity based on temperature \n - Keep descriptions concise but informative \n \n Maintain this exact formatting for consistency, using the emoji and section headers as shown. \n `, \n}) \n \n export { planningAgent } Define the weather workflow with 3 steps: one to fetch the weather via a network call, one to plan activities, and another to plan only indoor activities.\nBoth using the planning agent. workflows/conditional-workflow.ts import { z } from ' zod ' \n import { createStep, createWorkflow } from './vNext ' \n \n \n function getWeatherCondition (code: number): string { \n const conditions: Record &lt; number, string &gt; = { \n 0: ' Clear sky ', \n 1: ' Mainly clear ', \n 2: ' Partly cloudy ', \n 3: ' Overcast ', \n 45: ' Foggy ', \n 48: ' Depositing rime fog ', \n 51: ' Light drizzle ', \n 53: ' Moderate drizzle ', \n 55: ' Dense drizzle ', \n 61: ' Slight rain ', \n 63: ' Moderate rain ', \n 65: ' Heavy rain ', \n 71: ' Slight snow fall ', \n 73: ' Moderate snow fall ', \n 75: ' Heavy snow fall ', \n 95: ' Thunderstorm ', \n } \n return conditions[code] || ' Unknown ' \n} \n \n const forecastSchema = z. object ({ \n date: z. string (), \n maxTemp: z. number (), \n minTemp: z. number (), \n precipitationChance: z. number (), \n condition: z. string (), \n location: z. string (), \n}) \n \n // Fetch weather step \n const fetchWeather = createStep ({ \n id: ' fetch-weather ', \n description: ' Fetches weather forecast for a given city ', \n inputSchema: z. object ({ \n city: z. string (), \n }), \n outputSchema: forecastSchema, \n execute: async ({ inputData }) =&gt; { \n if (! inputData) { \n throw new Error ( ' Trigger data not found ') \n } \n \n const geocodingUrl = ` https://geocoding-api.open-meteo.com/v1/search?name= ${ encodeURIComponent (inputData.city)} &amp;count=1 ` \n const geocodingResponse = await fetch (geocodingUrl) \n const geocodingData = ( await geocodingResponse. json ()) as { \n results: { latitude: number; longitude: number; name: string }[] \n } \n \n if (! geocodingData.results?.[ 0]) { \n throw new Error ( ` Location ' ${ inputData.city} ' not found `) \n } \n \n const { latitude, longitude, name } = geocodingData.results[ 0] \n \n const weatherUrl = ` https://api.open-meteo.com/v1/forecast?latitude= ${ latitude} &amp;longitude= ${ longitude} &amp;current=precipitation,weathercode&amp;timezone=auto,&amp;hourly=precipitation_probability,temperature_2m ` \n const response = await fetch (weatherUrl) \n const data = ( await response. json ()) as { \n current: { \n time: string \n precipitation: number \n weathercode: number \n } \n hourly: { \n precipitation_probability: number [] \n temperature_2m: number [] \n } \n } \n \n const forecast = { \n date: new Date (). toISOString (), \n maxTemp: Math. max (... data.hourly.temperature_2m), \n minTemp: Math. min (... data.hourly.temperature_2m), \n condition: getWeatherCondition (data.current.weathercode), \n location: name, \n precipitationChance: data.hourly.precipitation_probability. reduce ( \n (acc, curr) =&gt; Math. max (acc, curr), \n 0 \n ), \n } \n \n return forecast \n }, \n}) \n \n // Plan activities indorrs or outdoors \n const planActivities = createStep ({ \n id: ' plan-activities ', \n description: ' Suggests activities based on weather conditions ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n console. log ( ' planActivities ') \n const forecast = inputData \n \n if (! forecast) { \n throw new Error ( ' Forecast data not found ') \n } \n \n const prompt = ` Based on the following weather forecast for ${ forecast.location}, suggest appropriate activities: \n ${ JSON. stringify (forecast, null, 2)} \n ` \n \n const agent = mastra?. getAgent ( ' planningAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk) \n activitiesText += chunk \n } \n \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n // Plan indoor activities only \n const planIndoorActivities = createStep ({ \n id: ' plan-indoor-activities ', \n description: ' Suggests indoor activities based on weather conditions ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n console. log ( ' planIndoorActivities ') \n const forecast = inputData \n \n if (! forecast) { \n throw new Error ( ' Forecast data not found ') \n } \n \n const prompt = ` In case it rains, plan indoor activities for ${ forecast.location} on ${ forecast.date}` \n \n const agent = mastra?. getAgent ( ' planningAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk) \n activitiesText += chunk \n } \n \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n const weatherWorkflow = createWorkflow ({ \n id: ' weather-workflow-step2-if-else ', \n inputSchema: z. object ({ \n city: z. string (). describe ( ' The city to get the weather for '), \n }), \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n}) \n . then (fetchWeather) \n . branch ([ \n [ \n async ({ inputData }) =&gt; { \n return inputData?.precipitationChance &gt; 50 \n }, \n planIndoorActivities, \n ], \n [ \n async ({ inputData }) =&gt; { \n return inputData?.precipitationChance &lt;= 50 \n }, \n planActivities, \n ], \n ]) \n \n weatherWorkflow. commit () \n \n export { weatherWorkflow } import { Mastra } from ' @mastra/core/mastra ' \n import { createLogger } from ' @mastra/core/logger ' \n import { weatherWorkflow } from './workflows ' \n import { planningAgent } from './agents ' \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n weatherWorkflow, \n }, \n agents: { \n planningAgent, \n }, \n logger: createLogger ({ \n name: ' Mastra ', \n level: ' info ', \n }), \n}) \n \n export { mastra } Register the agents and workflow with the mastra instance.\nThis is critical for enabling access to the agents within the workflow. import { Mastra } from ' @mastra/core/mastra ' \n import { createLogger } from ' @mastra/core/logger ' \n import { weatherWorkflow } from './workflows ' \n import { planningAgent } from './agents ' \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n weatherWorkflow, \n }, \n agents: { \n planningAgent, \n }, \n logger: createLogger ({ \n name: ' Mastra ', \n level: ' info ', \n }), \n}) \n \n export { mastra } Here, we’ll get the weather workflow from the mastra instance, then create a run and execute the created run with the required inputData. import { mastra } from \"./ \" \n \n const workflow = mastra. vnext_getWorkflow ( ' weatherWorkflow ') \n const run = workflow. createRun () \n \n const result = await run. start ({ inputData: { city: ' New York ' } }) \n console. dir (result, { depth: null })",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Conditional%20Branching%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20create%20conditional%20branches%20in%20workflows%20using%20the%20`branch`%20statement%20.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows_vNext/conditional-branching",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows_vNext/parallel-steps",
            "title": "Example: Parallel Execution | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows_vNext/parallel-steps",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "When building AI applications, you often need to process multiple independent tasks simultaneously to improve efficiency.\nWe make this functionality a core part of workflows through the.parallel method. Define a planning agent which leverages an LLM call to plan activities given a location and corresponding weather conditions. import { Agent } from ' @mastra/core/agent ' \n import { openai } from ' @ai-sdk/openai ' \n \n const llm = openai ( ' gpt-4o ') \n \n const planningAgent = new Agent ({ \n name: ' planningAgent ', \n model: llm, \n instructions: ` \n You are a local activities and travel expert who excels at weather-based planning. Analyze the weather data and provide practical activity recommendations. \n \n 📅 [Day, Month Date, Year] \n ═══════════════════════════ \n \n 🌡️ WEATHER SUMMARY \n • Conditions: [brief description] \n • Temperature: [X°C/Y°F to A°C/B°F] \n • Precipitation: [X% chance] \n \n 🌅 MORNING ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🌞 AFTERNOON ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🏠 INDOOR ALTERNATIVES \n • [Activity Name] - [Brief description including specific venue] \n Ideal for: [weather condition that would trigger this alternative] \n \n ⚠️ SPECIAL CONSIDERATIONS \n • [Any relevant weather warnings, UV index, wind conditions, etc.] \n \n Guidelines: \n - Suggest 2-3 time-specific outdoor activities per day \n - Include 1-2 indoor backup options \n - For precipitation &gt;50%, lead with indoor activities \n - All activities must be specific to the location \n - Include specific venues, trails, or locations \n - Consider activity intensity based on temperature \n - Keep descriptions concise but informative \n \n Maintain this exact formatting for consistency, using the emoji and section headers as shown. \n `, \n}) \n \n export { planningAgent } Define a synthesize agent which takes planned indoor and outdoor activities and provides a full report on the day. agents/synthesize-agent.ts import { Agent } from ' @mastra/core/agent ' \n import { openai } from ' @ai-sdk/openai ' \n \n const llm = openai ( ' gpt-4o ') \n \n const synthesizeAgent = new Agent ({ \n name: ' synthesizeAgent ', \n model: llm, \n instructions: ` \n You are given two different blocks of text, one about indoor activities and one about outdoor activities. \n Make this into a full report about the day and the possibilities depending on whether it rains or not. \n `, \n}) \n \n export { synthesizeAgent } Here, we’ll define a workflow which orchestrates a parallel -&gt; sequential flow between the planning steps and the synthesize step. workflows/parallel-workflow.ts import { Step, Workflow } from ' @mastra/core/workflows ' \n import { z } from ' zod ' \n import { activityPlannerAgent } from '../agents ' \n import { createStep, createWorkflow } from ' @mastra/core/workflows/vNext ' \n \n const forecastSchema = z. object ({ \n date: z. string (), \n maxTemp: z. number (), \n minTemp: z. number (), \n precipitationChance: z. number (), \n condition: z. string (), \n location: z. string (), \n}) \n \n const fetchWeather = createStep ({ \n id: ' fetch-weather ', \n description: ' Fetches weather forecast for a given city ', \n inputSchema: z. object ({ \n city: z. string (), \n }), \n outputSchema: forecastSchema, \n execute: async ({ inputData }) =&gt; { \n if (! inputData) { \n throw new Error ( ' Trigger data not found ') \n } \n \n const geocodingUrl = ` https://geocoding-api.open-meteo.com/v1/search?name= ${ encodeURIComponent (inputData.city)} &amp;count=1 ` \n const geocodingResponse = await fetch (geocodingUrl) \n const geocodingData = ( await geocodingResponse. json ()) as { \n results: { latitude: number; longitude: number; name: string }[] \n } \n \n if (! geocodingData.results?.[ 0]) { \n throw new Error ( ` Location ' ${ inputData.city} ' not found `) \n } \n \n const { latitude, longitude, name } = geocodingData.results[ 0] \n \n const weatherUrl = ` https://api.open-meteo.com/v1/forecast?latitude= ${ latitude} &amp;longitude= ${ longitude} &amp;current=precipitation,weathercode&amp;timezone=auto,&amp;hourly=precipitation_probability,temperature_2m ` \n const response = await fetch (weatherUrl) \n const data = ( await response. json ()) as { \n current: { \n time: string \n precipitation: number \n weathercode: number \n } \n hourly: { \n precipitation_probability: number [] \n temperature_2m: number [] \n } \n } \n \n const forecast = { \n date: new Date (). toISOString (), \n maxTemp: Math. max (... data.hourly.temperature_2m), \n minTemp: Math. min (... data.hourly.temperature_2m), \n condition: getWeatherCondition (data.current.weathercode), \n location: name, \n precipitationChance: data.hourly.precipitation_probability. reduce ( \n (acc, curr) =&gt; Math. max (acc, curr), \n 0 \n ), \n } \n \n return forecast \n }, \n}) \n \n const planActivities = createStep ({ \n id: ' plan-activities ', \n description: ' Suggests activities based on weather conditions ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n console. log ( ' mastra ', mastra) \n console. log ( ' planActivities ', inputData) \n const forecast = inputData \n \n if (! forecast) { \n throw new Error ( ' Forecast data not found ') \n } \n \n const prompt = ` Based on the following weather forecast for ${ forecast.location}, suggest appropriate activities: \n ${ JSON. stringify (forecast, null, 2)} \n ` \n \n const agent = mastra?. getAgent ( ' planningAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk) \n activitiesText += chunk \n } \n \n console. log ( ' planActivities ', activitiesText) \n \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n function getWeatherCondition (code: number): string { \n const conditions: Record &lt; number, string &gt; = { \n 0: ' Clear sky ', \n 1: ' Mainly clear ', \n 2: ' Partly cloudy ', \n 3: ' Overcast ', \n 45: ' Foggy ', \n 48: ' Depositing rime fog ', \n 51: ' Light drizzle ', \n 53: ' Moderate drizzle ', \n 55: ' Dense drizzle ', \n 61: ' Slight rain ', \n 63: ' Moderate rain ', \n 65: ' Heavy rain ', \n 71: ' Slight snow fall ', \n 73: ' Moderate snow fall ', \n 75: ' Heavy snow fall ', \n 95: ' Thunderstorm ', \n } \n return conditions[code] || ' Unknown ' \n} \n \n const planIndoorActivities = createStep ({ \n id: ' plan-indoor-activities ', \n description: ' Suggests indoor activities based on weather conditions ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n console. log ( ' planIndoorActivities ', inputData) \n const forecast = inputData \n \n if (! forecast) { \n throw new Error ( ' Forecast data not found ') \n } \n \n const prompt = ` In case it rains, plan indoor activities for ${ forecast.location} on ${ forecast.date}` \n \n const agent = mastra?. getAgent ( ' planningAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n activitiesText += chunk \n } \n \n console. log ( ' planIndoorActivities ', activitiesText) \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n const sythesizeStep = createStep ({ \n id: ' sythesize-step ', \n description: ' Synthesizes the results of the indoor and outdoor activities ', \n inputSchema: z. object ({ \n ' plan-activities ': z. object ({ \n activities: z. string (), \n }), \n ' plan-indoor-activities ': z. object ({ \n activities: z. string (), \n }), \n }), \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n console. log ( ' sythesizeStep ', inputData) \n const indoorActivities = inputData?.[ ' plan-indoor-activities '] \n const outdoorActivities = inputData?.[ ' plan-activities '] \n \n const prompt = ` Indoor activtities: \n ${ indoorActivities?.activities} \n \n Outdoor activities: \n ${ outdoorActivities?.activities} \n \n There is a chance of rain so be prepared to do indoor activities if needed. ` \n \n const agent = mastra?. getAgent ( ' synthesizeAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk) \n activitiesText += chunk \n } \n \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n const weatherWorkflow = createWorkflow ({ \n id: ' plan-both-workflow ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n steps: [planActivities, planIndoorActivities, sythesizeStep], \n}) \n // run `planActivities` and `planIndoorActivities` in parallel \n // `synthesizeStep` waits for both steps to be completed before executing. \n . parallel ([planActivities, planIndoorActivities]) \n . then (sythesizeStep) \n . commit () \n \n export { weatherWorkflow } Register the agents and workflow with the mastra instance.\nThis is critical for enabling access to the agents within the workflow. import { Mastra } from ' @mastra/core/mastra ' \n import { createLogger } from ' @mastra/core/logger ' \n import { weatherWorkflow } from './workflows ' \n import { planningAgent, synthesizeAgent } from './agents ' \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n weatherWorkflow, \n }, \n agents: { \n planningAgent, \n synthesizeAgent \n }, \n logger: createLogger ({ \n name: ' Mastra ', \n level: ' info ', \n }), \n}) \n \n export { mastra } Here, we’ll get the weather workflow from the mastra instance, then create a run and execute the created run with the required inputData. import { mastra } from \"./ \" \n \n const workflow = mastra. vnext_getWorkflow ( ' weatherWorkflow ') \n const run = workflow. createRun () \n \n const result = await run. start ({ inputData: { city: ' Ibiza ' } }) \n console. dir (result, { depth: null })",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Parallel%20Execution%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20execute%20multiple%20independent%20tasks%20in%20parallel%20within%20a%20workflow.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows_vNext/parallel-steps",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows_vNext/calling-agent",
            "title": "Example: Calling an Agent from a Workflow | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows_vNext/calling-agent",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "This example demonstrates how to create a workflow that calls an AI agent to sugest activities for the provided weather conditions, and execute it within a workflow step. Define a planning agent which leverages an LLM call to plan activities given a location and corresponding weather conditions. import { Agent } from ' @mastra/core/agent ' \n import { openai } from ' @ai-sdk/openai ' \n \n const llm = openai ( ' gpt-4o ') \n \n const planningAgent = new Agent ({ \n name: ' planningAgent ', \n model: llm, \n instructions: ` \n You are a local activities and travel expert who excels at weather-based planning. Analyze the weather data and provide practical activity recommendations. \n \n 📅 [Day, Month Date, Year] \n ═══════════════════════════ \n \n 🌡️ WEATHER SUMMARY \n • Conditions: [brief description] \n • Temperature: [X°C/Y°F to A°C/B°F] \n • Precipitation: [X% chance] \n \n 🌅 MORNING ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🌞 AFTERNOON ACTIVITIES \n Outdoor: \n • [Activity Name] - [Brief description including specific location/route] \n Best timing: [specific time range] \n Note: [relevant weather consideration] \n \n 🏠 INDOOR ALTERNATIVES \n • [Activity Name] - [Brief description including specific venue] \n Ideal for: [weather condition that would trigger this alternative] \n \n ⚠️ SPECIAL CONSIDERATIONS \n • [Any relevant weather warnings, UV index, wind conditions, etc.] \n \n Guidelines: \n - Suggest 2-3 time-specific outdoor activities per day \n - Include 1-2 indoor backup options \n - For precipitation &gt;50%, lead with indoor activities \n - All activities must be specific to the location \n - Include specific venues, trails, or locations \n - Consider activity intensity based on temperature \n - Keep descriptions concise but informative \n \n Maintain this exact formatting for consistency, using the emoji and section headers as shown. \n `, \n}) \n \n export { planningAgent } Define the weather workflow with 2 steps: one to fetch the weather via a network call, and another to plan activities using the planning agent. workflows/agent-workflow.ts import { createWorkflow, createStep } from ' @mastra/core/workflows/vNext ' \n import { z } from ' zod ' \n \n function getWeatherCondition (code: number): string { \n const conditions: Record &lt; number, string &gt; = { \n 0: ' Clear sky ', \n 1: ' Mainly clear ', \n 2: ' Partly cloudy ', \n 3: ' Overcast ', \n 45: ' Foggy ', \n 48: ' Depositing rime fog ', \n 51: ' Light drizzle ', \n 53: ' Moderate drizzle ', \n 55: ' Dense drizzle ', \n 61: ' Slight rain ', \n 63: ' Moderate rain ', \n 65: ' Heavy rain ', \n 71: ' Slight snow fall ', \n 73: ' Moderate snow fall ', \n 75: ' Heavy snow fall ', \n 95: ' Thunderstorm ', \n } \n return conditions[code] || ' Unknown ' \n} \n \n const forecastSchema = z. object ({ \n date: z. string (), \n maxTemp: z. number (), \n minTemp: z. number (), \n precipitationChance: z. number (), \n condition: z. string (), \n location: z. string (), \n}) \n \n const fetchWeather = createStep ({ \n id: ' fetch-weather ', \n description: ' Fetches weather forecast for a given city ', \n inputSchema: z. object ({ \n city: z. string (), \n }), \n outputSchema: forecastSchema, \n execute: async ({ inputData }) =&gt; { \n if (! inputData) { \n throw new Error ( ' Trigger data not found ') \n } \n \n const geocodingUrl = ` https://geocoding-api.open-meteo.com/v1/search?name= ${ encodeURIComponent (inputData.city)} &amp;count=1 ` \n const geocodingResponse = await fetch (geocodingUrl) \n const geocodingData = ( await geocodingResponse. json ()) as { \n results: { latitude: number; longitude: number; name: string }[] \n } \n \n if (! geocodingData.results?.[ 0]) { \n throw new Error ( ` Location ' ${ inputData.city} ' not found `) \n } \n \n const { latitude, longitude, name } = geocodingData.results[ 0] \n \n const weatherUrl = ` https://api.open-meteo.com/v1/forecast?latitude= ${ latitude} &amp;longitude= ${ longitude} &amp;current=precipitation,weathercode&amp;timezone=auto,&amp;hourly=precipitation_probability,temperature_2m ` \n const response = await fetch (weatherUrl) \n const data = ( await response. json ()) as { \n current: { \n time: string \n precipitation: number \n weathercode: number \n } \n hourly: { \n precipitation_probability: number [] \n temperature_2m: number [] \n } \n } \n \n const forecast = { \n date: new Date (). toISOString (), \n maxTemp: Math. max (... data.hourly.temperature_2m), \n minTemp: Math. min (... data.hourly.temperature_2m), \n condition: getWeatherCondition (data.current.weathercode), \n location: name, \n precipitationChance: data.hourly.precipitation_probability. reduce ( \n (acc, curr) =&gt; Math. max (acc, curr), \n 0 \n ), \n } \n \n return forecast \n }, \n}) \n \n const planActivities = createStep ({ \n id: ' plan-activities ', \n description: ' Suggests activities based on weather conditions ', \n inputSchema: forecastSchema, \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n const forecast = inputData \n \n if (! forecast) { \n throw new Error ( ' Forecast data not found ') \n } \n \n const prompt = ` Based on the following weather forecast for ${ forecast.location}, suggest appropriate activities: \n ${ JSON. stringify (forecast, null, 2)} \n ` \n \n const agent = mastra?. getAgent ( ' planningAgent ') \n if (! agent) { \n throw new Error ( ' Planning agent not found ') \n } \n \n const response = await agent. stream ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n \n let activitiesText = '' \n \n for await ( const chunk of response.textStream) { \n process.stdout. write (chunk) \n activitiesText += chunk \n } \n \n return { \n activities: activitiesText, \n } \n }, \n}) \n \n const weatherWorkflow = createWorkflow ({ \n steps: [fetchWeather, planActivities], \n id: ' weather-workflow-step1-single-day ', \n inputSchema: z. object ({ \n city: z. string (). describe ( ' The city to get the weather for '), \n }), \n outputSchema: z. object ({ \n activities: z. string (), \n }), \n}) \n . then (fetchWeather) \n . then (planActivities) \n \n weatherWorkflow. commit () \n \n export { weatherWorkflow } Register the planning agent and weather workflow with the mastra instance.\nThis is critical for enabling access to the planning agent within the weather workflow. import { Mastra } from ' @mastra/core/mastra ' \n import { createLogger } from ' @mastra/core/logger ' \n import { weatherWorkflow } from './workflows ' \n import { planningAgent } from './agents ' \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n weatherWorkflow, \n }, \n agents: { \n planningAgent, \n }, \n logger: createLogger ({ \n name: ' Mastra ', \n level: ' info ', \n }), \n}) \n \n export { mastra } Here, we’ll get the weather workflow from the mastra instance, then create a run and execute the created run with the required inputData. Here, we’ll get the weather workflow from the mastra instance, then create a run and execute the created run with the required inputData. import { mastra } from \"./ \" \n \n const workflow = mastra. vnext_getWorkflow ( ' weatherWorkflow ') \n const run = workflow. createRun () \n \n const result = await run. start ({ inputData: { city: ' New York ' } }) \n console. dir (result, { depth: null })",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Calling%20an%20Agent%20from%20a%20Workflow%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20call%20an%20AI%20agent%20from%20within%20a%20workflow%20step.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows_vNext/calling-agent",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          },
          {
            "id": "https://mastra.ai/examples/workflows_vNext/agent-and-tool-interop",
            "title": "Example: Using a Tool/Agent as a Step | Workflows | Mastra Docs",
            "url": "https://mastra.ai/examples/workflows_vNext/agent-and-tool-interop",
            "publishedDate": "1999-10-01T00:00:00.000Z",
            "author": "",
            "text": "Tool/Agent as a Workflow step \n This example demonstrates how to create and integrate a tool or an agent as a workflow step.\nMastra provides a createStep helper function which accepts either a step or agent and returns an object which satisfies the Step interface. \n Define Interop Workflow \n Defines a workflow which takes an agent and tool as a step. \n workflows/interop-workflow.ts import { createWorkflow, createStep } from ' @mastra/core/workflows/vNext ' \n import { weatherTool } from '../tools ' \n import { weatherReporterAgent } from '../agents ' \n import { z } from ' zod ' \n \n const fetchWeather = createStep (weatherTool) \n const reportWeather = createStep (weatherReporterAgent) \n \n const weatherWorkflow = createWorkflow ({ \n steps: [fetchWeather, reportWeather], \n id: ' weather-workflow ', \n inputSchema: z. object ({ \n location: z. string (). describe ( ' The city to get the weather for '), \n }), \n outputSchema: z. object ({ \n text: z. string (), \n }), \n}) \n . then (fetchWeather) \n . then ( \n createStep ({ \n id: ' report-weather ', \n inputSchema: fetchWeather.outputSchema, \n outputSchema: z. object ({ \n text: z. string (), \n }), \n execute: async ({ inputData, mastra }) =&gt; { \n const prompt = ' Forecast data: ' + JSON. stringify (inputData) \n const agent = mastra. getAgent ( ' weatherReporterAgent ') \n const result = await agent. generate ([ \n { \n role: ' user ', \n content: prompt, \n }, \n ]) \n return { text: result.text } \n }, \n }) \n ) \n \n weatherWorkflow. commit () \n \n export { weatherWorkflow } \n Register Workflow instance with Mastra class \n Register the workflow with the mastra instance. \n import { Mastra } from ' @mastra/core/mastra ' \n import { createLogger } from ' @mastra/core/logger ' \n import { weatherWorkflow } from './workflows ' \n \n const mastra = new Mastra ({ \n vnext_workflows: { \n weatherWorkflow, \n }, \n logger: createLogger ({ \n name: ' Mastra ', \n level: ' info ', \n }), \n}) \n \n export { mastra } \n Execute the workflow \n Here, we’ll get the weather workflow from the mastra instance, then create a run and execute the created run with the required inputData. \n import { mastra } from \"./ \" \n \n const workflow = mastra. vnext_getWorkflow ( ' weatherWorkflow ') \n const run = workflow. createRun () \n \n const result = await run. start ({ inputData: { location: \" Lagos \" } }) \n console. dir (result, { depth: null }) Calling an Agent Human in the Loop",
            "image": "https://mastra.ai/api/og/docs?title=Example:%20Using%20a%20Tool/Agent%20as%20a%20Step%20|%20Workflows%20|%20Mastra%20Docs&description=Example%20of%20using%20Mastra%20to%20integrate%20a%20tool%20or%20an%20agent%20as%20a%20step%20in%20a%20workflow.",
            "favicon": "https://mastra.ai/favicon.ico",
            "extras": {
              "links": [
                "https://mastra.ai/en/examples/workflows_vNext/agent-and-tool-interop",
                "https://mastra.ai/",
                "https://mastra.ai/docs",
                "https://mastra.ai/examples"
              ]
            }
          }
        ]
      }
    ],
    "requestId": "20e67d8417ce35857c693296b598e7bc",
    "costDollars": {
      "total": 0.091,
      "contents": {
        "text": 0.091
      }
    }
  }
}
