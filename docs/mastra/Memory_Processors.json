{
    "id": "https://mastra.ai/docs/memory/memory-processors",
    "title": "Memory Processors",
    "url": "https://mastra.ai/docs/memory/memory-processors",
    "author": "",
    "text": "\n Memory Processors allow you to modify the list of messages retrieved from memory before they are added to the agent’s context window and sent to the LLM. This is useful for managing context size, filtering content, and optimizing performance. \n Processors operate on the messages retrieved based on your memory configuration (e.g., lastMessages, semanticRecall). They do not affect the new incoming user message. \n Built-in Processors \n Mastra provides built-in processors: \n TokenLimiter \n This processor is used to prevent errors caused by exceeding the LLM’s context window limit. It counts the tokens in the retrieved memory messages and removes the oldest messages until the total count is below the specified limit. \n import { Memory } from \" @mastra/memory \"; \n import { TokenLimiter } from \" @mastra/memory/processors \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n model: openai ( \" gpt-4o \"), \n memory: new Memory ({ \n processors: [ \n // Ensure the total tokens from memory don't exceed ~127k \n new TokenLimiter ( 127000), \n ], \n }), \n}); \n The TokenLimiter uses the o200k_base encoding by default (suitable for GPT-4o). You can specify other encodings if needed for different models: \n // Import the encoding you need (e.g., for older OpenAI models) \n import cl100k_base from \" js-tiktoken/ranks/cl100k_base \"; \n \n const memoryForOlderModel = new Memory ({ \n processors: [ \n new TokenLimiter ({ \n limit: 16000, // Example limit for a 16k context model \n encoding: cl100k_base, \n }), \n ], \n}); \n See the OpenAI cookbook  or js-tiktoken repo for more on encodings. \n ToolCallFilter \n This processor removes tool calls from the memory messages sent to the LLM. It saves tokens by excluding potentially verbose tool interactions from the context, which is useful if the details aren’t needed for future interactions. It’s also useful if you always want your agent to call a specific tool again and not rely on previous tool results in memory. \n import { Memory } from \" @mastra/memory \"; \n import { ToolCallFilter, TokenLimiter } from \" @mastra/memory/processors \"; \n \n const memoryFilteringTools = new Memory ({ \n processors: [ \n // Example 1: Remove all tool calls/results \n new ToolCallFilter (), \n \n // Example 2: Remove only noisy image generation tool calls/results \n new ToolCallFilter ({ exclude: [ \" generateImageTool \"] }), \n \n // Always place TokenLimiter last \n new TokenLimiter ( 127000), \n ], \n}); \n Applying Multiple Processors \n You can chain multiple processors. They execute in the order they appear in the processors array. The output of one processor becomes the input for the next. \n Order matters! It’s generally best practice to place TokenLimiter last in the chain. This ensures it operates on the final set of messages after other filtering has occurred, providing the most accurate token limit enforcement. \n import { Memory } from \" @mastra/memory \"; \n import { ToolCallFilter, TokenLimiter } from \" @mastra/memory/processors \"; \n // Assume a hypothetical 'PIIFilter' custom processor exists \n // import { PIIFilter } from './custom-processors'; \n \n const memoryWithMultipleProcessors = new Memory ({ \n processors: [ \n // 1. Filter specific tool calls first \n new ToolCallFilter ({ exclude: [ \" verboseDebugTool \"] }), \n // 2. Apply custom filtering (e.g., remove hypothetical PII - use with caution) \n // new PIIFilter(), \n // 3. Apply token limiting as the final step \n new TokenLimiter ( 127000), \n ], \n}); \n Creating Custom Processors \n You can create custom logic by extending the base MemoryProcessor class. \n import { Memory, CoreMessage } from \" @mastra/memory \"; \n import { MemoryProcessor, MemoryProcessorOpts } from \" @mastra/core/memory \"; \n \n class ConversationOnlyFilter extends MemoryProcessor { \n constructor () { \n // Provide a name for easier debugging if needed \n super({ name: \" ConversationOnlyFilter \" }); \n } \n \n process ( \n messages: CoreMessage [], \n _opts: MemoryProcessorOpts = {}, // Options passed during memory retrieval, rarely needed here \n ): CoreMessage [] { \n // Filter messages based on role \n return messages. filter ( \n (msg) =&gt; msg.role === \" user \" || msg.role === \" assistant \", \n ); \n } \n} \n \n // Use the custom processor \n const memoryWithCustomFilter = new Memory ({ \n processors: [ \n new ConversationOnlyFilter (), \n new TokenLimiter ( 127000), // Still apply token limiting \n ], \n}); \n When creating custom processors avoid mutating the input messages array or its objects directly.",
    "image": "https://mastra.ai/api/og/docs?title=Memory%20Processors&description=undefined",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/memory/memory-processors",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}