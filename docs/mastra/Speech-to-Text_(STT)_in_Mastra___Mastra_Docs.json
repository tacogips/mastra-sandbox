{
    "id": "https://mastra.ai/docs/voice/speech-to-text",
    "title": "Speech-to-Text (STT) in Mastra | Mastra Docs",
    "url": "https://mastra.ai/docs/voice/speech-to-text",
    "publishedDate": "",
    "author": "",
    "text": "\n Speech-to-Text (STT) in Mastra provides a standardized interface for converting audio input into text across multiple service providers.\nSTT helps create voice-enabled applications that can respond to human speech, enabling hands-free interaction, accessibility for users with disabilities, and more natural human-computer interfaces. \n Configuration \n To use STT in Mastra, you need to provide a listeningModel when initializing the voice provider. This includes parameters such as: \n \n name: The specific STT model to use. \n apiKey: Your API key for authentication. \n Provider-specific options: Additional options that may be required or supported by the specific voice provider. \n \n Note: All of these parameters are optional. You can use the default settings provided by the voice provider, which will depend on the specific provider you are using. \n const voice = new OpenAIVoice ({ \n listeningModel: { \n name: \" whisper-1 \", \n apiKey: process.env.OPENAI_API_KEY, \n }, \n}); \n \n // If using default settings the configuration can be simplified to: \n const voice = new OpenAIVoice (); \n Available Providers \n Mastra supports several Speech-to-Text providers, each with their own capabilities and strengths: \n \n OpenAI - High-accuracy transcription with Whisper models \n Azure - Microsoft’s speech recognition with enterprise-grade reliability \n ElevenLabs - Advanced speech recognition with support for multiple languages \n Google - Google’s speech recognition with extensive language support \n Cloudflare - Edge-optimized speech recognition for low-latency applications \n Deepgram - AI-powered speech recognition with high accuracy for various accents \n Sarvam - Specialized in Indic languages and accents \n \n Each provider is implemented as a separate package that you can install as needed: \n pnpm add @mastra/voice-openai # Example for OpenAI \n Using the Listen Method \n The primary method for STT is the listen() method, which converts spoken audio into text. Here’s how to use it: \n import { Agent } from ' @mastra/core/agent '; \n import { openai } from ' @ai-sdk/openai '; \n import { OpenAIVoice } from ' @mastra/voice-openai '; \n import { getMicrophoneStream } from \" @mastra/node-audio \"; \n \n const voice = new OpenAIVoice (); \n \n const agent = new Agent ({ \n name: \" Voice Agent \", \n instructions: \" You are a voice assistant that provides recommendations based on user input. \", \n model: openai ( \" gpt-4o \"), \n voice, \n}); \n \n const audioStream = getMicrophoneStream (); // Assume this function gets audio input \n \n const transcript = await agent.voice. listen (audioStream, { \n filetype: \" m4a \", // Optional: specify the audio file type \n}); \n \n console. log ( ` User said: ${ transcript}`); \n \n const { text } = await agent. generate ( ` Based on what the user said, provide them a recommendation: ${ transcript}`); \n \n console. log ( ` Recommendation: ${ text}`); \n Check out the Adding Voice to Agents documentation to learn how to use STT in an agent. Text to Speech Speech to Speech new",
    "image": "https://mastra.ai/api/og/docs?title=Speech-to-Text%20(STT)%20in%20Mastra%20|%20Mastra%20Docs&description=Overview%20of%20Speech-to-Text%20capabilities%20in%20Mastra,%20including%20configuration,%20usage,%20and%20integration%20with%20voice%20providers.",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/voice/speech-to-text",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}