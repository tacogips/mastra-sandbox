{
    "id": "https://mastra.ai/docs/frameworks/ai-sdk",
    "title": "Using with Vercel AI SDK",
    "url": "https://mastra.ai/docs/frameworks/ai-sdk",
    "author": "",
    "text": "\n Mastra leverages AI SDK’s model routing (a unified interface on top of OpenAI, Anthropic, etc), structured output, and tool calling. \n We explain this in greater detail in this blog post  \n Mastra + AI SDK \n Mastra acts as a layer on top of AI SDK to help teams productionize their proof-of-concepts quickly and easily. \n Model routing \n When creating agents in Mastra, you can specify any AI SDK-supported model: \n import { openai } from \" @ai-sdk/openai \"; \n import { Agent } from \" @mastra/core/agent \"; \n \n const agent = new Agent ({ \n name: \" WeatherAgent \", \n instructions: \" Instructions for the agent... \", \n model: openai ( \" gpt-4-turbo \"), // Model comes directly from AI SDK \n}); \n \n const result = await agent. generate ( \" What is the weather like? \"); \n AI SDK Hooks \n Mastra is compatible with AI SDK’s hooks for seamless frontend integration: \n useChat \n The useChat hook enables real-time chat interactions in your frontend application \n \n Works with agent data streams i.e..toDataStreamResponse() \n The useChat api defaults to /api/chat \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for data streams,\ni.e. no structured output is defined. \n \n import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages); \n \n return stream. toDataStreamResponse (); \n} \n import { useChat } from ' @ai-sdk/react '; \n \n export function ChatComponent () { \n const { messages, input, handleInputChange, handleSubmit } = useChat ({ \n api: ' /path-to-your-agent-stream-api-endpoint ' \n }); \n \n return ( \n &lt; div &gt; \n {messages. map ( m =&gt; ( \n &lt; div key = { m. id} &gt; \n {m. role}: { m.content} \n &lt;/ div &gt; \n ))} \n &lt; form onSubmit = {handleSubmit} &gt; \n &lt; input \n value = {input} \n onChange = {handleInputChange} \n placeholder = \" Say something... \" \n /&gt; \n &lt;/ form &gt; \n &lt;/ div &gt; \n ); \n} \n \n Gotcha: When using useChat with agent memory functionality, make sure to check out the Agent Memory section for important implementation details. \n \n useCompletion \n For single-turn completions, use the useCompletion hook: \n \n Works with agent data streams i.e..toDataStreamResponse() \n The useCompletion api defaults to /api/completion \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for data streams,\ni.e. no structured output is defined. \n \n app/api/completion/route.ts import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages); \n \n return stream. toDataStreamResponse (); \n} \n import { useCompletion } from \" @ai-sdk/react \"; \n \n export function CompletionComponent () { \n const { \n completion, \n input, \n handleInputChange, \n handleSubmit, \n } = useCompletion ({ \n api: ' /path-to-your-agent-stream-api-endpoint ' \n }); \n \n return ( \n &lt; div &gt; \n &lt; form onSubmit = {handleSubmit} &gt; \n &lt; input \n value = {input} \n onChange = {handleInputChange} \n placeholder = \" Enter a prompt... \" \n /&gt; \n &lt;/ form &gt; \n &lt; p &gt;Completion result: {completion}&lt; / p &gt; \n &lt;/ div &gt; \n ); \n} \n useObject \n For consuming text streams that represent JSON objects and parsing them into a complete object based on a schema. \n \n Works with agent text streams i.e..toTextStreamResponse() \n Works with the Mastra REST API agent stream endpoint {MASTRA_BASE_URL}/agents/:agentId/stream for text streams,\ni.e. structured output is defined. \n \n app/api/use-object/route.ts import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n const stream = await myAgent. stream (messages, { \n output: z. object ({ \n weather: z. string (), \n }), \n }); \n \n return stream. toTextStreamResponse (); \n} \n import { experimental_useObject as useObject } from ' @ai-sdk/react '; \n \n export default function Page () { \n const { object, submit } = useObject ({ \n api: ' /api/use-object ', \n schema: z. object ({ \n weather: z. string (), \n }), \n }); \n \n return ( \n &lt; div &gt; \n &lt; button onClick = { () =&gt; submit ( ' example input ')} &gt; Generate &lt;/ button &gt; \n {object?. content &amp;&amp; &lt; p &gt; {object. content} &lt;/ p &gt;} \n &lt;/ div &gt; \n ); \n} \n Tool Calling \n AI SDK Tool Format \n Mastra supports tools created using the AI SDK format, so you can use\nthem directly with Mastra agents. See our tools doc on Vercel AI SDK Tool Format\n for more details. \n Client-side tool calling \n Mastra leverages AI SDK’s tool calling, so what applies in AI SDK applies here still.\n Agent Tools in Mastra are 100% percent compatible with AI SDK tools. \n Mastra tools also expose an optional execute async function. It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process. \n One way to then leverage client-side tool calling is to use the @ai-sdk/react useChat hook’s onToolCall property for\nclient-side tool execution \n Custom DataStream \n In certain scenarios you need to write custom data, message annotations to an agent’s dataStream.\nThis can be useful for: \n \n Streaming additional data to the client \n Passing progress info back to the client in real time \n \n Mastra integrates well with AI SDK to make this possible \n CreateDataStream \n The createDataStream function allows you to stream additional data to the client \n import { createDataStream } from \" ai \" \n import { Agent } from ' @mastra/core/agent '; \n \n export const weatherAgent = new Agent ({ \n name: ' Weather Agent ', \n instructions: ` \n You are a helpful weather assistant that provides accurate weather information. \n \n Your primary function is to help users get weather details for specific locations. When responding: \n - Always ask for a location if none is provided \n - If the location name isn't in English, please translate it \n - If giving a location with multiple parts (e.g. \"New York, NY\"), use the most relevant part (e.g. \"New York\") \n - Include relevant details like humidity, wind conditions, and precipitation \n - Keep responses concise but informative \n \n Use the weatherTool to fetch current weather data. \n `, \n model: openai ( ' gpt-4o '), \n tools: { weatherTool }, \n }); \n \n const stream = createDataStream ({ \n async execute (dataStream) { \n // Write data \n dataStream. writeData ({ value: ' Hello ' }); \n \n // Write annotation \n dataStream. writeMessageAnnotation ({ type: ' status ', value: ' processing ' }); \n \n //mastra agent stream \n const agentStream = await weatherAgent. stream ( ' What is the weather ') \n \n // Merge agent stream \n agentStream. mergeIntoDataStream (dataStream); \n }, \n onError: error =&gt; ` Custom error: ${ error.message}`, \n }); \n \n CreateDataStreamResponse \n The createDataStreamResponse function creates a Response object that streams data to the client \n import { mastra } from \" @/src/mastra \"; \n \n export async function POST (req: Request) { \n const { messages } = await req. json (); \n const myAgent = mastra. getAgent ( \" weatherAgent \"); \n //mastra agent stream \n const agentStream = await myAgent. stream (messages); \n \n const response = createDataStreamResponse ({ \n status: 200, \n statusText: ' OK ', \n headers: { \n ' Custom-Header ': ' value ', \n }, \n async execute (dataStream) { \n // Write data \n dataStream. writeData ({ value: ' Hello ' }); \n \n // Write annotation \n dataStream. writeMessageAnnotation ({ type: ' status ', value: ' processing ' }); \n \n // Merge agent stream \n agentStream. mergeIntoDataStream (dataStream); \n }, \n onError: error =&gt; ` Custom error: ${ error.message}`, \n }); \n \n return response \n}",
    "image": "https://mastra.ai/api/og/docs?title=Using%20with%20Vercel%20AI%20SDK&description=Learn%20how%20Mastra%20leverages%20the%20Vercel%20AI%20SDK%20library%20and%20how%20you%20can%20leverage%20it%20further%20with%20Mastra",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/frameworks/ai-sdk",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}