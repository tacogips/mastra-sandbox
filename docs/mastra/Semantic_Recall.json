{
    "id": "https://mastra.ai/docs/memory/semantic-recall",
    "title": "Semantic Recall",
    "url": "https://mastra.ai/docs/memory/semantic-recall",
    "author": "",
    "text": "\n If you ask your friend what they did last weekend, they will search in their memory for events associated with “last weekend” and then tell you what they did. That’s sort of like how semantic recall works in Mastra. \n How Semantic Recall Works \n Semantic recall is RAG-based search that helps agents maintain context across longer interactions when messages are no longer within recent conversation history. \n It uses vector embeddings of messages for similarity search, integrates with various vector stores, and has configurable context windows around retrieved messages. \n \n When it’s enabled, new messages are used to query a vector DB for semantically similar messages. \n After getting a response from the LLM, all new messages (user, assistant, and tool calls/results) are inserted into the vector DB to be recalled in later interactions. \n Quick Start \n Semantic recall is enabled by default, so if you give your agent memory it will be included: \n import { Agent } from \" @mastra/core/agent \"; \n import { Memory } from \" @mastra/memory \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n name: \" SupportAgent \", \n instructions: \" You are a helpful support agent. \", \n model: openai ( \" gpt-4o \"), \n memory: new Memory (), \n}); \n Recall configuration \n The two main parameters that control semantic recall behavior are: \n \n topK: How many semantically similar messages to retrieve \n messageRange: How much surrounding context to include with each match \n \n const agent = new Agent ({ \n memory: new Memory ({ \n options: { \n semanticRecall: { \n topK: 3, // Retrieve 3 most similar messages \n messageRange: 2, // Include 2 messages before and after each match \n }, \n }, \n }), \n}); \n Storage configuration \n Semantic recall relies on a storage and vector db to store messages and their embeddings. \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { LibSQLStore, LibSQLVector } from \" @mastra/libsql \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // this is the default storage db if omitted \n storage: new LibSQLStore ({ \n url: \" file:./local.db \", \n }), \n // this is the default vector db if omitted \n vector: new LibSQLVector ({ \n connectionUrl: \" file:./local.db \", \n }), \n }), \n}); \n Storage/vector code Examples: \n \n LibSQL \n Postgres \n Upstash \n \n Embedder configuration \n Semantic recall relies on an embedding model to convert messages into embeddings. You can specify any embedding model  compatible with the AI SDK. \n To use FastEmbed (a local embedding model), install @mastra/fastembed: \n npm install @mastra/fastembed \n Then configure it in your memory: \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { fastembed } from \" @mastra/fastembed \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // ... other memory options \n embedder: fastembed, \n }), \n}); \n Alternatively, use a different provider like OpenAI: \n import { Memory } from \" @mastra/memory \"; \n import { Agent } from \" @mastra/core/agent \"; \n import { openai } from \" @ai-sdk/openai \"; \n \n const agent = new Agent ({ \n memory: new Memory ({ \n // ... other memory options \n embedder: openai. embedding ( \" text-embedding-3-small \"), \n }), \n}); \n Disabling \n There is a performance impact to using semantic recall. New messages are converted into embeddings and used to query a vector database before new messages are sent to the LLM. \n Semantic recall is enabled by default but can be disabled when not needed: \n const agent = new Agent ({ \n memory: new Memory ({ \n options: { \n semanticRecall: false, \n }, \n }), \n}); \n You might want to disable semantic recall in scenarios like: \n \n When conversation history provide sufficient context for the current conversation. \n In performance-sensitive applications, like realtime two-way audio, where the added latency of creating embeddings and running vector queries is noticeable. \n",
    "image": "https://mastra.ai/api/og/docs?title=Semantic%20Recall&description=undefined",
    "favicon": "https://mastra.ai/favicon.ico",
    "extras": {
        "links": [
            "https://mastra.ai/en/docs/memory/semantic-recall",
            "https://mastra.ai/",
            "https://mastra.ai/docs",
            "https://mastra.ai/examples"
        ]
    }
}